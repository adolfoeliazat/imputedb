\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{qtree} % for diagram
\usepackage{listings} % for code excerpt
\usepackage{color} % for code excerpt
\lstset{
  basicstyle=\ttfamily\footnotesize,
  language=SQL,
  keywordstyle=\color{blue},
  otherkeywords={IMPUTE, impute}
}
\usepackage{booktabs} % for table

\title{6.830 Project Proposal \\
  \large Data Imputation as a Query Optimization}
\author{Jose Cambronero \& John Feser \& Micah Smith}

\begin{document}
\maketitle

\section{Problem Statement}
Data imputation is a well studied problem in the machine learning literature, but some aspects related to the implementation of imputation in database systems have not been sufficiently examined. Previous work has viewed imputation as a pre-processing step on a table, rather than as an active step in each query. Particularly, existing work has not incorporated imputation into query optimization, nor has it investigated the relative impact of early versus late stage imputation on the quality of query results.

\section{Approach}
In order to study the placement of an imputation step, we propose to create a logical imputation operator (along with respective physical instances) and incorporate its placement as part of the query plan optimization process in SimpleDB.  We will introduce measures of uncertainty and completeness of query results as well as the cost of imputation, which outline the main trade-offs in the imputation placement. We add these measures into our cost estimation, allowing us to intelligently place the data imputation step during query planning. Finally, we will produce empirical results that show the trade-offs between efficiency and accuracy for simple data imputation models. We will focus on queries that perform aggregate operations or return a significant number of tuples (in contrast to simple point-queries).  These types of queries are likely to provide opportunities for imputation of missing values and represent an increasingly large portion of workloads in modern OLAP warehouses. Finally, we will compare our results to existing approaches.

\section{Example}
In this section, we use a simple example to show that placing the imputation operator at different points in the query plan has a significant impact on the query results.

We start with the relation \lstinline{Employees(id, salary, dept)} shown in Table~\ref{example-table}. We impute  by averaging over \lstinline{salary}, and we compare two plans for the query \lstinline{SELECT AVG(salary) FROM Employees GROUP BY dept}.

\begin{table}[h]
\centering
\caption{Example table with missing data}
\label{example-table}
\begin{tabular}{@{}lll@{}}
\toprule
id & salary & dept \\ \midrule
1  & 10     & 1    \\
2  & 15     & 1    \\
3  & NA     & 1    \\
4  & 5      & 2    \\ \bottomrule
\end{tabular}
\end{table}

\begin{figure}
 \begin{minipage}[b]{0.5\textwidth}
	\Tree [.\lstinline{AVG(salary)} [.\lstinline{GROUP BY (dept)} [.\lstinline{IMPUTE(Salary)} \lstinline{Employees} ] ] ]
    \caption{Early placement of impute operator}
    \label{early-fig}
  \end{minipage}
\hfill
 \begin{minipage}[b]{0.5\textwidth}
	\Tree [.\lstinline{AVG(salary)} [.\lstinline{IMPUTE(Salary)} [.\lstinline{GROUP BY (dept)} \lstinline{Employees} ] ] ]
    \caption{Late placement of impute operator}
    \label{late-fig}
  \end{minipage}
\end{figure}

In the early case (Figure~\ref{early-fig}), we impute over all salaries, and obtain as a final result \lstinline{[(1, 11.67), (2, 5)]}. The late case (Figure~\ref{late-fig}) averages over salaries within a department and results in \lstinline{[(1, 12.5), (2, 5)]}.

This example shows that the imputation operator, even in its simplest form, can have a significant impact on query results. Furthermore, this example fails to capture other important dimensions such as cost of imputation.

\section{Evaluation}
We are considering several data sources for our evaluation. Existing literature uses the American Community Survey, so we think this is a good choice for our experiments as well. The ACS data is a good test dataset because of its scale and relative simplicity. Additionally, we may be able to compare directly to existing work that uses ACS data.

In order to measure the accuracy of our imputation algorithm, we plan to compute results with and without missing values where possible, and use standard error measures to compare to our results. The best-case scenario for imputation is a dataset where data is removed uniformly at random. It should be easy to construct a test dataset  simply by removing data from an existing dataset; artificially degrading an existing dataset also means that we will have ground truth with which to compare our results.

In contrast to pre-processing-based imputation, we plan to make a case for imputation on intermediate query results. This approach is similar to a view, as neither alter the original table and both provide logical independence. Many modern database servers have a variety of clients with different needs, so one imputation strategy is unlikely to satisfy everyone. Consider a financial data warehouse with a business strategy client and an actuarial client. It is unlikely that the queries issued by these two groups will require or benefit from the same imputation strategy. If a salary value is missing for a customer, the business strategy query may not require a costly regression model estimate and a simple heuristic could be enough to return a reasonable answer. It may be cheaper to do imputation on the fly than to maintain a separate copy of the data for each imputation strategy.

Furthermore, we believe that there is an opportunity for more efficient imputation on (smaller) intermediate results. In very large databases it may not be feasible to perform imputation on the base tables, particularly if the imputation method of choice is expensive. However, queries often return results that are smaller than the base tables, so it may be faster to perform imputation on intermediate results than on the base tables. The intermediate results could then be stored as a materialized view if they are expected to be reused or they could be recomputed as necessary.

\section{Project Milestones}

We outline a series of milestones to be completed:
\begin{enumerate}
\item Initial data collection and query workload for evaluation.
\item Consolidate set of imputation strategies.
\item Formalize a measure of uncertainty, completeness and cost in imputation as related to 2.
\item Formalize the possible query rewrites when using an imputation operator.
\item Incorporate imputation-related measures into cost estimation for SimpleDB.
\item Incorporate imputation-related rewrites for SimpleDB.
\item Evaluate performance.
\end{enumerate}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
