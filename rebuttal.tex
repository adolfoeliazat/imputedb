\documentclass{vldb}
\usepackage[activate={true,nocompatibility},final,kerning=true,spacing=true,factor=1100,stretch=10,shrink=10]{microtype}
\usepackage[subtle]{savetrees}
\usepackage{xcolor}
\newcommand{\resp}[1]{{\color{blue}{#1}}}
\newcommand{\todo}[1]{{\color{red}{\textit{#1}}}}
\newcommand{\ProjectName}[0]{ImputeDB}

\begin{document}
\section{Introduction}
The authors would like to thank the reviewers for their helpful and insightful comments.
We have taken steps to address each comment and we believe that the paper is significantly improved
as a result. In this document, we give a point-by-point response to the comments from each reviewer.

In the rebuttal, the reviewer's original comments are in black and our responses are in \resp{blue}.
In our draft, new or significantly changed text is highlighted in {\color{blue}blue}.

\section{Overall changes}
\begin{itemize}
\item Redundant figures, excessive whitespace, formatting.
  
  \resp{Several reviewers commented on the amount of whitespace, so we've compressed many of the figures and removed whitespace to make room for additional material.}
  
\item \resp{
    We have improved our system since the paper was submitted by adding imputation for categorical variables.
    This has a minor impact on the experimental results.
    It improves the ability of the regression tree model to perform imputation by making these variables available to the model.
  }
\end{itemize}

  \section{Reviewer 1}
  \begin{itemize}
  \item W2. The paper completely obscures the ML technique being used: (1) It would be nice to have some discussion of the complete API for implementing an ML technique, as well as how one might implement a cost-estimator for one or two examples. (2) It would be helpful to see the impact of a few different approaches (e.g., Tree-based, Interpolation, Regression, FD-based) on performance and accuracy. (3) Given that the ML time estimator is being treated as a black box, it would be useful to see how sensitive the plan selector is to inaccurate estimates.
  
  \resp{
    We extended our evaluation to include two commonly used imputation methods: mean value imputation and hot-deck imputation.
    These methods are in addition to the original imputation method (CART), which is based on regression trees.
    The regression tree model incorporates correlations between attributes, so should be able to take advantage of any functional dependencies to make better predictions.
    Interpolation based imputation is useful for data where the ordering of tuples is significant, like time series, but is not well suited for survey data where the ordering of tuples is arbitrary.
    
    We also added details on how to pick the cost functions that define the API for impute operators.
    We provide details on how these functions are defined for all the methods used in our evaluation.

    Our optimizer (like any query optimizer) is sensitive to the quality of its estimates.
    We believe that the cost functions we have chosen lead to good results overall, but there may be cases where our estimates lead to bad outcomes.
    It is an interesting and difficult problem to accurately estimate the quality of a regression model before running the model.
    Any work in this area could be used immediately to improve our algorithm.
  }

  \item W3. Reducing runtime to ~10s is excellent, but comes at a very serious accuracy cost (3-5x worse SMAPE). Worse still, there's no way for the user to differentiate between the cases of Q4 and Q8 at runtime. 24\% error relative to direct imputation is huge.
    
    \resp{
      The outsized error in Q8 is due to a drop operator placed very early on in the plan (a result of optimizing for performance). This produced significantly smaller sample sizes for various
      groups, which skewed the average calculation for each group significantly. For example, the average student debt for subjects with no high school degree was almost 83\% off from the estimate
      using the base table imputation, but if we look at the underlying population the former was calculated with 6 people, while the latter was with 258 people.
      
      We can trivially extend \ProjectName{} to warn users when the query chosen for execution has a high $\textsc{Penalty}$ estimate, along with the number of tuples that have
      been dropped (after execution), so that situations such as those observed in Q8 can be identified by the user.
    }

  \item Although it doesn't consider cost or the drop operator, a similar technique appears in Yang et. al. "Lenses: An On-Demand Approach to ETL." A comparison would be nice.
    
   \resp{
     Yang's work is definitely related, and we've added it to our related work section.
   Their system helps an analyst prioritize cleaning tasks for a particular query.
   It uses pretrained regression models as one of a number of repair strategies for missing data rather than creating a query plan which incorporates the training and running of such models.
   }

 \item S1: "Furthermore, the results obtained with this approach are similar in quality to those obtained with the traditional imputation approach between 0 and 24 percent" 24 percent error is pretty bad.

   \resp{See the response to W3 for discussion of Q8.}

  \item S2: The drop operator appears to be a weaker form of SQL's null-value / 3-valued Boolean semantics. The latter might actually be more useful. For example, consider the case of a predicate (A $>$ 5 OR B $<$ 10). If A $>$ 5, then the condition is independent of B, and the tuple can still be salvaged. If A $\leq$ 5, then the expression evaluates to UNKNOWN and the tuple is dropped anyway. For aggregates there is a greater need, as one might have different aggregates computed over different subsets of the data, but even here it's not clear that this is any less informative than just throwing data away.

    \resp{We agree that it would be interesting to allow drops to be omitted before filters.
      This would increase the plan space somewhat and might lead to better plans in some cases.
      This optimization would only change the output of the filter when the predicate is disjunctive, so would not have an impact on the queries we used in our experiments.
      Filters with conjunctive predicates behave identically with and without the drop, although removing the drop will give a minor performance improvement.}

  \item S2: Why no unions?
    
    \resp{We skipped unions for brevity. There is nothing fundamental about our algorithm which makes unions difficult to implement.}

  \item S3.4.1, Definition 2: On my first read-through, I was puzzled about why there was no plan with zero-loss. It would help to clarify that Loss is not about tuple loss, but rather a heuristic penalty function.

    \resp{We have clarified this in the paper by emphasizing that \textit{Loss} is not a loss in the statistical sense or in the tuple loss sense and by renaming it to $\textsc{Penalty}$.}

  \item S3.4.3: An extremely nice emergent property from the L-value function for the imputation operator seems to be that the operator will be moved to a position in the plan where it has access to the maximum training data. I am slightly concerned that by focusing entirely on size, this might lead to skew in the training set. For example, consider a one-to-many join \verb/µ(R) |X| S/. Your optimization algorithm would likely wish to move imputation up: \verb/µ(R |X| S)/. Yes, there is more training data, but if the fanout for each tuple is skewed, some tuples in R will become over-represented in the join result. A similar observation holds for selection. For example take your Q8, where the conditions "studentdebtowe $>$ 0 AND schooldegree $\geq$ 0" presumably indicate invalid data that should not be used for training. (is this perhaps why you have so much error for full imputation on Q8?)

    \resp{This is a reasonable concern. Though often an appropriate proxy, optimizing for the maximum training set is flawed when the query requires the construction a large, skewed data set. The problem seems to be particular to queries which generate large intermediate tables. In some cases, as when joining two datasets, the intermediate table is a good target for training because it exposes correlations which would be hidden in the separate tables. In other cases, this intermediate table is large and skewed, which causes problems for imputation. 
    
    Avoiding this case requires us to estimate the skew in these intermediate tables, which would be in itself an interesting and challenging research problem. If we had improved intermediate
    estimates, we could incorporate those to improve our system.
      }
    %\todo{Need a better explanation here.}

  \item S3.4.4: It appears that Time() assumes that plans are IO-bound. This is a perfectly reasonable assumption, but should be stated explicitly, as datasets that fit in memory will require a different cost model.

    \resp{Agreed. Our database stores tables in files, so our plans are I/O bound, but a database which uses an in-memory representation will need a different Time() function. We have clarified this in the paper.}

  \item S3.4.4: "Drop is a special case of a sequential scan, and its time complexity can be expressed as a function of the number of heap pages read and the I/O cost per page." seems to suggest that drop is implemented as a blocking operator. Why not just take the same approach as selection and treat it as having negligible cost?
    
    \resp{We mistakenly suggest in S3 that we treat \textit{Drop} as a blocking operator. In
      our implementation, it has $0$ cost, and we have updated the text to reflect that.}

  \item S4: All of the workload queries are (data-size-independent) AVG queries. What about other types of aggregates (e.g., SUM, COUNT)?

    \resp{Though we do not show this explicitly, we repeat all of the queries using COUNT
      aggregates in order to compute the ``Count Fraction''. The timing results are
      identical in this case and the number of tuples returned can be seen
      directly in Table 4. We would expect benchmark queries that use aggregates such as SUM 
      to work well in our system, with errors proportional to the fraction of relevant tuples retrieved
      and the AVG value estimated.}

  \item S4.4.1: Imputation over the raw data is being used as ground truth. Is there any reason to suspect that this is the correct thing to do? Perhaps joining multiple tables together exposes new attributes that makes imputation *more* reliable? At the very least, it would be useful to understand why there is so much error in $\alpha=0$ on Q2 and Q8.

    \resp{For our purposes, imputation over the raw data is a reasonable ground truth. This
      is because the analyst, in practice, should be considering the tradeoffs on quality
      and performance of using ImputeDB versus a production imputation system on the raw
      data. Unfortunately, they would never have access to a real ground truth in such a
      scenario, and moreover, finding ground truth for use even in an empirical evaluation
      of imputation methods can be challenging because artificially dirtying the data
      often does not capture the complex reasons for missingness in real world datasets.
      We updated the discussion in S4.4.1 to emphasize this point.}

  \item There are a number of opportunities for trimming in the figures, tables, and algorithms: Algorithms 1-3 could be compacted a bunch, Table 1 is full of whitespace (e.g., make it a 3-column {figure*}), Tables 1-2 could also be summarized as CDFs, Figure 6 is full of whitespace, Table 3 could be put queries on a single row (e.g., Q1 would easily fit on 1 line), and the references have a lot of redundant information (URLs, for example).

    \resp{We have compacted the algorithms and tables. Note that Tables 1-2 show the percentage of missing values for \emph{each attribute}, not as an overall percentage, so cannot be summarized as CDFs. We think that the URLs add value to the references, particularly for data sets.}

  \item Additional space recovered from these sections could be used to discuss the choice of techniques for imputation, and the confidence estimation idea suggested in the future work section. 

  \item Another interesting use of the space, or at least an idea for future work might be the role of aggregation on inference accuracy. In my experience aggregate queries tend to be *much* more forgiving of approximations (e.g., Online Aggregation) and data errors.
  \end{itemize}

  \section{Reviewer 2}
  \begin{itemize}
  \item 1) No new imputation method is presented. The model relies on external imputation techniques, and given it's done on-the-fly, presumably imputed values depend on qualifying query records.

    \resp{Our contribution is the planning algorithm which allows imputation to be interleaved into the query plan.
      We do not present a new imputation method, but our technique is agnostic to the particular imputation method used.
      We reduce the overhead of imputation by performing it only on the tuples which are relevant to a particular query, so the imputed results do depend on the records which are relevant to particular query.}
    
  \item 2) No comparative evaluation against existing imputation/data cleaning techniques.

    \resp{
      We compare our planning technique for on-the-fly imputation to imputation performed on the full dataset, using the same imputation algorithm in both cases.
      We are not aware of another database technology which offers on-the-fly imputation optimization, so there is no direct comparison for the planning algorithm.
%      However, we ran a simple experiment using BayesDB to compare possible imputation times. In order to make the comparison as fair as possible in terms of running
%      time, we reduced the number of models that BayesDB instantiates to a single model per base table. We fit each model with 100 iterations, in an optimized setting.
%      The fitting times on the CDC tables ranged between 119 and 320 seconds (per table). The FCC model fitting took 237 seconds. The user would then still need to use these models to
%      perform imputation.  The underlying modeling technique for BayesDB is quite sophisticated, so the high running time is not a surprise, but should provide some context
%      for the challenges faced by users trying to explore new datasets. In a real world scenario, the user would instantiate more than a single model, increasing these running
%      times further.
      % Our evaluation compares our technique to an imputation of the full dataset. We use the same imputation algorithm for our on-the-fly imputation as for the full dataset imputation. Cleaning the full dataset is the standard method of handling missing values.
    }

  \item D1) The model relies on external imputation techniques, and given it's done on-the-fly, the imputed values will differ based on the qualifying query records?

    \resp{Yes, as discussed in the response to 1.}

  \item D2) The paper does not propose new imputation techniques but rely on existing statistical imputation algorithms. There is no comparative evaluation, nor samples of imputed values to show the goodness of the imputed values. It would be nice to see a comparative evaluation of different statistical imputation methods, and suggestions for extensions that can fit within the proposed imputation model.

    \resp{
      We have added two new imputation methods to the experiments, to show that the planning algorithm can be used successfully with different imputation methods.
      We have also added a subsection to the algorithm section explaining how we chose the cost functions for each imputation method that we test and giving guidelines for choosing cost functions for a new imputation method.
    }
  \end{itemize}

  \section{Reviewer 3}
  \begin{itemize}
  \item W2. More work is required in the presentation: important parts are not explained in depth, while there are many details in other sections that can be omitted.

  \item W3. More experiments needed.
  
  \resp{
  We've extended the evaluation section to include more impute operators and an evaluation of planning times.
  }

  \item D1. Section 3, which is the meat of the paper, is very overloaded, without getting in depth in various parts of the system. It definitely should be split in more sections, and more details should be given. In particular:
    -- Give some more information about cardinality estimation. Also, Algorithm 3 is too far from the text, and needs some explanation too.
    -- Some more details are also required in the Imputation quality section. Why are you choosing these specific formulas for Loss, especially for the impute operator. Some examples would help.
    -- Algorithms 1 and 2 could probably be presented in an easier to grasp way.
    
    \resp{
    We've broken up this section and consolidated algorithm notation. The cardinality estimation section provides more detail about the algorithm. We also
    added intuition for how the heuristic Penalty (a renaming of Loss, for clarity) and Time should be designed. We walk through these definitions for three different
    impute operators.
    }

  \item D2. It would be nice to show how exactly your approach can be implemented in an existing optimizer. This comes up in various places in the paper:
    -- When defining the search space, why not first define the transformation operators that inject the imputation operators. Then any optimizer can pick these rules up and add them in its plan enumeration. Of course, the used approach that follows System-R's plan enumeration is useful, but why lose generality when presenting the search space. Moreover, showing the transformation rules can help you show the size of the search space that will better justify the restrictions you apply.
    -- Why use simplified cost formulas for the traditional relational operators and not existing systems' formulas?
    -- It would be great (but not necessary of course) to add your techniques in an existing DBMS, such as Postgres to see the impact on a real system in your experiments.

   \resp{
   We've added a section that describes in detail the components that need to be added to an existing database system to implement our technique. 
   The details come directly from our implementation of \ProjectName{} on top of SimpleDB, a popular teaching database used at MIT, University of Washington, and Northwestern University, 
   amongst others. These extensions could further exploit any existing sophistication in the underlying system for tasks such as execution cost estimates.
   
   Extending a rule-based optimizer to perform dynamic imputation would certainly be an interesting idea. However, the planning algorithm presented benefits from the tight
   interaction between traditional query planning and imputation operator placement. The latter can influence aspects such as join ordering and cardinality estimates for subplans.
   Incorporating these into a rule-based system would be possible, but we believe the plan selected through this approach will be suboptimal, compared to that chosen through
   our integrated planning approach.
   }


  \item D3. Is there any way for the user to know what error to expect based on the $\alpha$ value? Moreover, is its value somehow proportional to the error (e.g., $\alpha=0.5$ would give roughly double the error of $\alpha=0.25$)? Or should the user simply try multiple execution with different values of $\alpha$ until the desired results are given?

    \resp{In our intended usage, the user starts by running with $\alpha = 0$ and increases $\alpha$ if the query takes too long to complete.
     
     We agree that having a way of providing an estimate on error, based on $\alpha$, would be both useful and interesting to develop.
     Providing error estimates further requires a reasonable notion of ground-truth. For evaluation, we've used the values imputed on the 
     tables when applying whole-table imputation. Based on our evaluation, higher values of $\alpha$ correctly identify increased error
     within a query, however, extending that to be predictive is not something we implemented in the current system.
     }

  \item D4. How would a two-step approach that first finds the optimal plan (w/o imputation) and then adds the imputation operators perform?

    \resp{This approach would not be able to improve on the plans produced by our approach, as it would produce a subset of the plans that we consider. It's unclear how much worse the plans would be, but a major issue is that the Drop and Impute operators have an impact on the cardinality of their outputs. Our optimizer can use this information to improve the plan, but adding imputation after the fact would not be able to rearrange the plan to capitalize on changes in cardinality.}

  \item D5. Can you have plans with a mix of drop and impute operators?
    
    \resp{Yes, we have now highlighted this for clarity in the paper as well.}

  \item D6. During plan enumeration, can you safely prune plans at some point, similar to what a traditional optimizer would do?

    \resp{We maintain the set of Pareto optimal plans at each point in the optimization. This set is significantly smaller than the set of all plans, so it represents some pruning. We found that the size of these Pareto sets is small enough that we did not need to prune further.

      If necessary, it is possible to sample from the Pareto optimal plans during the plan enumeration. The final selected plan may not be Pareto optimal, but this is a potential way to control the number of plans.
      To perform this approximation, we only add plans to the frontier that are non-dominated and are sufficiently different from existing plans in the frontier. The plan already in the frontier should
      provide similar tradeoffs and reduces the search space explored. \ProjectName{} only employs this optimization when a large number of tables requiring imputation are involved in the query.
      }

    \item D7. When defining the cost of impute operator, how do you know a priori the number of iterations for the algorithm (given it has to converge)?
      
  \resp{For the iterative regression impute operator, we set a predefined number of iterations as an upper bound on its execution. If the imputation converges,
  we can return immediately, otherwise we return after this upper bound is reached. This is a common approach used in iterative optimization techniques, such
  as gradient descent.
}

  \item D8. Are there impute implementation that are non-blocking? Or do they always have to perform multiple passes over the data?

    \resp{Yes. For example, in time series data, it is reasonable to fill the most recent non-null value forward. This kind of imputation isn't useful for survey data, so we didn't consider it for our experiments. We have extended the experiments section to consider a simple non-blocking imputation method relevant to our dataset: mean value imputation. This is a common technique, which fills in  missing values
    with the average observed value. In our case, we implement a non-blocking variant that estimates the mean from the plan's histograms, avoiding the need to perform a full pass over
    the data before imputing.}

  \item D9. Additional experiments I suggest to include:
    -- Consider a scenario where the user submits multiple similar queries. Show after how many such queries your approach has no longer a competitive advantage compared to performing imputation on the data.
    -- Try different values for $\alpha$ (not just 0 and 1).
    -- Show runtime of enumeration algorithm as queries get more complex.

    \resp{
      The sensitivity of different values of $\alpha$ depends on the query itself. Performing a scan of values of $\alpha$ with small step sizes can return
      a lot of the same plans for execution. In lieu of this, we extended the results section to include a subsection that presents the final frontiers for
      two queries, one from each dataset used during evaluation. This frontier can be exposed to the user to help in their workflow.
      \\
      We explored the planning times for queries with increasing number of joins by running additional experiments with up to 8 joins. 
      The standard algorithm presented in the original version submitted handles
      up to 5 joins, with columns requiring imputation in all tables, with planning time of approximately one second or less. We added a small optimization to help reduce
      planning times for a larger number of joins. We maintain an approximation of the Pareto sets, rather than the exact set, by only adding non-dominated plans that
      are sufficiently different. This optimization reduces planning time linearly with respect to the reduction in the size of the final frontier. Users can increase
      the level of approximation to reduce planning times further.}

  \item D10. The scenario you are considering, with multiple queries during exploration, begs for reuse of previous query results, as you also mention in the future work section.

    \resp{We agree.
      Reusing previous query results is a tricky problem because it requires the database to speculatively store some of the intermediate results of its query plans.
      A simple heuristic which should work well is to store the results of full table imputation whenever a query forces this to occur. These results are guaranteed to be useful later.
      Storing other intermediate results is problematic, particularly those which are computed after filtering or joining, as we would need to have a mapping back to the tuples in the base tables.}

  \item D11. Less important points:
    -- Sections 4.2 and 4.3 in the experiments are too verbose.
    -- In Section 3.2, you mention that joins are performed after filtering. When filters are on results of joins, this is not possible.

    \resp{The subset of SQL that we consider does not allow filtering on the results of joins (see Section 3.2). Extending the planner to cover this case is not complex. Before adding filters, it must add imputations on the required attributes.}

  \item Given you optimize for both performance and quality, Tummer's and Koch's work on multi-objective QO is related.

    \resp{Yes, Tummer \& Koch is definitely related.
      However, much of the complexity of their method is in dealing with cost functions which are not well behaved.
      Our cost functions are monotonic, which allows us to use a simpler optimizer than what Tummer \& Koch propose.
    We have added a discussion of their work to the related works section.}

\item Could be interesting for the optimizer to pick imputation strategy based on the type of the query.
  \end{itemize}
\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
