\documentclass{vldb}
\usepackage{xcolor}
\newcommand{\resp}[1]{{\color{blue}{#1}}}
\newcommand{\todo}[1]{{\color{red}{\textit{#1}}}}
\newcommand{\ProjectName}[0]{ImputeDB}

\begin{document}
\section{Reviewer 1}
\begin{itemize}
\item W1. A number of the algorithms/figures are a bit redundant (e.g., Histogram estimation for classical join operators in Alg. 3) Overall, the paper in its current form feels padded -- it could be trimmed down by 1-2 pages. See below for a few ideas of things to use this space for. I will note that I'd be perfectly happy accepting this as a shorter, unpadded paper.

\item W2. The paper completely obscures the ML technique being used: (1) It would be nice to have some discussion of the complete API for implementing an ML technique, as well as how one might implement a cost-estimator for one or two examples. (2) It would be helpful to see the impact of a few different approaches (e.g., Tree-based, Interpolation, Regression, FD-based) on performance and accuracy. (3) Given that the ML time estimator is being treated as a black box, it would be useful to see how sensitive the plan selector is to inaccurate estimates.

\item W3. Reducing runtime to ~10s is excellent, but comes at a very serious accuracy cost (3-5x worse SMAPE). Worse still, there's no way for the user to differentiate between the cases of Q4 and Q8 at runtime. 24\% error relative to direct imputation is huge.
\resp{
The outsized error in Q8 is a result of a drop operator placed very early on in the plan (a result of optimizing for performance). This produced significantly smaller sample sizes for various
groups, which skewed the average calculation for each group significantly. For example, the average student debt for subjects with no high school degree was almost 83\% off from the estimate
using the base table imputation, but if we look at the underlying population the former was calculated with 6 people, while the latter was with 258 people.
\todo{We have extended ImputeDB to provide users with a estimate of the number of tuples dropped in a calculation due to nulls, which may be indicative of the underlying error or skew
in the results presented. This is a simple heuristic but we find that it would have provided appropriate warning in cases such as Q8.}
}


\item - Although it doesn't consider cost or the drop operator, a similar technique appears in Yang et. al. "Lenses: An On-Demand Approach to ETL." A comparison would be nice.

\item - S1: "Furthermore, the results obtained with this approach are similar in quality to those obtained with the traditional imputation approach — between 0 and 24 percent" 24 percent error is pretty bad. 

\item - S2: The drop operator appears to be a weaker form of SQL's null-value / 3-valued Boolean semantics. The latter might actually be more useful. For example, consider the case of a predicate (A $>$ 5 OR B $<$ 10). If A $>$ 5, then the condition is independent of B, and the tuple can still be salvaged. If A $\leq$ 5, then the expression evaluates to UNKNOWN and the tuple is dropped anyway. For aggregates there is a greater need, as one might have different aggregates computed over different subsets of the data, but even here it's not clear that this is any less informative than just throwing data away. 

\item - S2: Why no unions?
  
\resp{We skipped unions for brevity. There is nothing fundamental about our algorithm which makes unions difficult to implement.}

\item - S3.4.1, Definition 2: On my first read-through, I was puzzled about why there was no plan with zero-loss. It would help to clarify that Loss is not about tuple loss, but rather a heuristic penalty function.

  \resp{We have clarified this in the paper.}

\resp{We emphasize that \textit{Loss} is not a loss in the statistical sense or in the tuple loss sense by renaming it to \textit{Penalty}.}

\item - S3.4.3: An extremely nice emergent property from the L-value function for the imputation operator seems to be that the operator will be moved to a position in the plan where it has access to the maximum training data. I am slightly concerned that by focusing entirely on size, this might lead to skew in the training set. For example, consider a one-to-many join \verb/µ(R) |X| S/. Your optimization algorithm would likely wish to move imputation up: \verb/µ(R |X| S)/. Yes, there is more training data, but if the fanout for each tuple is skewed, some tuples in R will become over-represented in the join result. A similar observation holds for selection. For example take your Q8, where the conditions "studentdebtowe $>$ 0 AND schooldegree $\geq$ 0" presumably indicate invalid data that should not be used for training. (is this perhaps why you have so much error for full imputation on Q8?)

  % \resp{This is a reasonable concern. Though often an appropriate proxy, optimizing for the maximum training set is flawed when the training data is skewed. We think that this problem is avoidable with a minor tweak to the loss function. Instead of optimizing for a large training set, we can optimize for a \emph{diverse} training set by tracking the 
% TODO also add an explanation of the coding of schooldegree -- and why it is filter >= 0,
    % which seems unreasonable to an observer.

\item - S3.4.4: It appears that Time() assumes that plans are IO-bound. This is a perfectly reasonable assumption, but should be stated explicitly, as datasets that fit in memory will require a different cost model.

  \resp{Agreed. Our database stores tables in files, so our plans are I/O bound, but a database which uses an in-memory representation will need a different Time() function. We have clarified this in the paper.}

\item - S3.4.4: "Drop is a special case of a sequential scan, and its time complexity can be expressed as a function of the number of heap pages read and the I/O cost per page." seems to suggest that drop is implemented as a blocking operator. Why not just take the same approach as selection and treat it as having negligible cost?
    \resp{We mistakenly suggest in S3 that we treat \textit{Drop} as a blocking operator. In
        our implementation, we do endow it with $0$ cost, and we have updated the text to
        reflect that.}

  \resp{This was an error in the paper. In the implementation, we do treat Drop as taking negligible time.}

\item - S4: All of the workload queries are (data-size-independent) AVG queries. What about other types of aggregates (e.g., SUM, COUNT)?

    \resp{Though we do not show this explicitly, we repeat all of the queries using COUNT
        aggregates in order to compute the ``Count Fraction''. The timing results are
        clearly identical in this case and the number of tuples returned can be seen
        directly in Table 4.}
    \todo{For SUM queries, that may depend more directly on the number of rows returned than the
        quality of imputations for any individual row, the effect of different COUNT results
        dominates the computation AVG*COUNT. How do we handle this?}

\item - S4.4.1: Imputation over the raw data is being used as ground truth. Is there any reason to suspect that this is the correct thing to do? Perhaps joining multiple tables together exposes new attributes that makes imputation *more* reliable? At the very least, it would be useful to understand why there is so much error in $\alpha=0$ on Q2 and Q8.

    \resp{For our purposes, imputation over the raw data is a reasonable ground truth. This
        is because the analyst, in practice, should be considering the tradeoffs on quality
        and performance of using ImputeDB versus a production imputation system on the raw
        data. Unfortunately, they would never have access to a real ground truth in such a
        scenario, and moreover, finding ground truth for use even in an empirical evaluation
        of imputation methods can be challenging because artificially dirtying the data
        often does not capture the complex reasons for missingness in real world datasets.
        We update the discussion in S4.4.1 to emphasize this point.}

\item - There are a number of opportunities for trimming in the figures, tables, and algorithms: Algorithms 1-3 could be compacted a bunch, Table 1 is full of whitespace (e.g., make it a 3-column {figure*}), Tables 1-2 could also be summarized as CDFs, Figure 6 is full of whitespace, Table 3 could be put queries on a single row (e.g., Q1 would easily fit on 1 line), and the references have a lot of redundant information (URLs, for example). 

\item - Additional space recovered from these sections could be used to discuss the choice of techniques for imputation, and the confidence estimation idea suggested in the future work section. 

\item - Another interesting use of the space, or at least an idea for future work might be the role of aggregation on inference accuracy. In my experience aggregate queries tend to be *much* more forgiving of approximations (e.g., Online Aggregation) and data errors.
\end{itemize}

\section{Reviewer 2}
\begin{itemize}
\item 1) No new imputation method is presented. The model relies on external imputation techniques, and given it's done on-the-fly, presumably imputed values depend on qualifying query records.

  \resp{Yes, the imputed results depend on the records which are relevant to particular query. We do not present a new imputation method, but our technique is agnostic to the particular imputation method used. We reduce the overhead of imputation by performing it only on the tuples which are relevant to a particular query.}
  
\item 2) No comparative evaluation against existing imputation/data cleaning techniques.

  \resp{Our evaluation compares our technique to an imputation of the full dataset. We use the same imputation algorithm for our on-the-fly imputation as for the full dataset imputation. Cleaning the full dataset is the standard method of handling missing values.}

\item D1) The model relies on external imputation techniques, and given it's done on-the-fly, the imputed values will differ based on the qualifying query records?

  \resp{Yes, as discussed above.}

\item D2) The paper does not propose new imputation techniques but rely on existing statistical imputation algorithms. There is no comparative evaluation, nor samples of imputed values to show the goodness of the imputed values. It would be nice to see a comparative evaluation of different statistical imputation methods, and suggestions for extensions that can fit within the proposed imputation model.

  \resp{The error measures that we provide compare our on-the-fly application of imputation to imputation over the full dataset.}

\item D3) Table 3 uses a lot of space, and contains simple queries that are already described in the text. The table can be removed to include additional experiments as suggested above.
\end{itemize}

\section{Reviewer 3}
\begin{itemize}
\item W1. The paper feels like an early submission that is not ready for publication at its current state.

\item W2. More work is required in the presentation: important parts are not explained in depth, while there are many details in other sections that can be omitted.

\item W3. More experiments needed.

\item D1. Section 3, which is the meat of the paper, is very overloaded, without getting in depth in various parts of the system. It definitely should be split in more sections, and more details should be given. In particular:
-- Give some more information about cardinality estimation. Also, Algorithm 3 is too far from the text, and needs some explanation too.
-- Some more details are also required in the Imputation quality section. Why are you choosing these specific formulas for Loss, especially for the impute operator. Some examples would help.
-- Algorithms 1 and 2 could probably be presented in an easier to grasp way.

\item D2. It would be nice to show how exactly your approach can be implemented in an existing optimizer. This comes up in various places in the paper:
-- When defining the search space, why not first define the transformation operators that inject the imputation operators. Then any optimizer can pick these rules up and add them in its plan enumeration. Of course, the used approach that follows System-R's plan enumeration is useful, but why lose generality when presenting the search space. Moreover, showing the transformation rules can help you show the size of the search space that will better justify the restrictions you apply.
-- Why use simplified cost formulas for the traditional relational operators and not existing systems' formulas?
-- It would be great (but not necessary of course) to add your techniques in an existing DBMS, such as Postgres to see the impact on a real system in your experiments.

\resp{
Existing systems have very complex formulas for estimating query costs. We felt that the overhead of reimplementing an existing cost model would not be worth the benefit.
However, we added a new section that discusses in detail the main components for \ProjectName{}, so that developers of existing systems can use this as a roadmap
for implementation.
}

\item D3. Is there any way for the user to know what error to expect based on the $\alpha$ value? Moreover, is its value somehow proportional to the error (e.g., $\alpha=0.5$ would give roughly double the error of $\alpha=0.25$)? Or should the user simply try multiple execution with different values of $\alpha$ until the desired results are given?

  \resp{In our intended usage, the user starts by running with $\alpha = 0$ and increases $\alpha$ if the query takes too long to complete.}

\item D4. How would a two-step approach that first finds the optimal plan (w/o imputation) and then adds the imputation operators perform?

  \resp{This approach would not be able to improve on the plans produced by our approach, as it would produce a subset of the plans that we consider. It's unclear how much worse the plans would be, but a major issue is that the Drop and Impute operators have an impact on the cardinality of their outputs. Our optimizer can use this information to improve the plan, but adding imputation after the fact would not be able to rearrange the plan to capitalize on changes in cardinality.}

\item D5. Can you have plans with a mix of drop and impute operators?
  
  \resp{Yes.}

\item D6. During plan enumeration, can you safely prune plans at some point, similar to what a traditional optimizer would do?

  \resp{We maintain the set of pareto optimal plans at each point in the optimization. This set is significantly smaller than the set of all plans, so it represents some pruning. We found that the size of these pareto sets is small enough that we did not need to prune further.

    If necessary, it should be possible to sample from the pareto optimal plans during the plan enumeration. The final selected plan may not be pareto optimal, but this is a potential way to control the number of plans.}

\item D7. When defining the cost of impute operator, how do you know a priori the number of iterations for the algorithm (given it has to converge)?

\item D8. Are there impute implementation that are non-blocking? Or do they always have to perform multiple passes over the data?

\item D9. Additional experiments I suggest to include:
-- Consider a scenario where the user submits multiple similar queries. Show after how many such queries your approach has no longer a competitive advantage compared to performing imputation on the data.
-- Try different values for $\alpha$ (not just 0 and 1).
-- Show runtime of enumeration algorithm as queries get more complex.

\item D10. The scenario you are considering, with multiple queries during exploration, begs for reuse of previous query results, as you also mention in the future work section.

\item D11. Less important points:
-- Sections 4.2 and 4.3 in the experiments are too verbose.
-- In Section 3.2, you mention that joins are performed after filtering. When filters are on results of joins, this is not possible.

\item -- Given you optimize for both performance and quality, Tummer's and Koch's work on multi-objective QO is related.

  \resp{Yes, Tummer \& Koch is closely related. However, much of the complexity of their paper is in dealing with cost functions which are not well behaved. Our cost functions are both monotonic, which allows us to use a simpler optimizer than what Tummer \& Koch propose.}

\item -- Could be interesting for the optimizer to pick imputation strategy based on the type of the query.
\end{itemize}
\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
