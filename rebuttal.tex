\documentclass{vldb}
\usepackage{xcolor}
\newcommand{\resp}[1]{{\color{blue}{#1}}}
\newcommand{\todo}[1]{{\color{red}{\textit{#1}}}}
\newcommand{\ProjectName}[0]{ImputeDB}

\begin{document}
\section{Multiple reviewers}
\begin{itemize}
\item Redundant figures, excessive whitespace, formatting.
  
  \resp{We've compressed many of the figures and removed whitespace to make room for additional material.}
\end{itemize}

  \section{Reviewer 1}
  \begin{itemize}
  \item W2. The paper completely obscures the ML technique being used: (1) It would be nice to have some discussion of the complete API for implementing an ML technique, as well as how one might implement a cost-estimator for one or two examples. (2) It would be helpful to see the impact of a few different approaches (e.g., Tree-based, Interpolation, Regression, FD-based) on performance and accuracy. (3) Given that the ML time estimator is being treated as a black box, it would be useful to see how sensitive the plan selector is to inaccurate estimates.
    \todo{}

  \item W3. Reducing runtime to ~10s is excellent, but comes at a very serious accuracy cost (3-5x worse SMAPE). Worse still, there's no way for the user to differentiate between the cases of Q4 and Q8 at runtime. 24\% error relative to direct imputation is huge.
    
    \resp{
      The outsized error in Q8 is a result of a drop operator placed very early on in the plan (a result of optimizing for performance). This produced significantly smaller sample sizes for various
      groups, which skewed the average calculation for each group significantly. For example, the average student debt for subjects with no high school degree was almost 83\% off from the estimate
      using the base table imputation, but if we look at the underlying population the former was calculated with 6 people, while the latter was with 258 people.
      \todo{We have extended ImputeDB to provide users with a estimate of the number of tuples dropped in a calculation due to nulls, which may be indicative of the underlying error or skew
        in the results presented. This is a simple heuristic but we find that it would have provided appropriate warning in cases such as Q8.}
    }

  \item Although it doesn't consider cost or the drop operator, a similar technique appears in Yang et. al. "Lenses: An On-Demand Approach to ETL." A comparison would be nice.
    \todo{}

  \item S1: "Furthermore, the results obtained with this approach are similar in quality to those obtained with the traditional imputation approach — between 0 and 24 percent" 24 percent error is pretty bad.
    \todo{}

  \item S2: The drop operator appears to be a weaker form of SQL's null-value / 3-valued Boolean semantics. The latter might actually be more useful. For example, consider the case of a predicate (A $>$ 5 OR B $<$ 10). If A $>$ 5, then the condition is independent of B, and the tuple can still be salvaged. If A $\leq$ 5, then the expression evaluates to UNKNOWN and the tuple is dropped anyway. For aggregates there is a greater need, as one might have different aggregates computed over different subsets of the data, but even here it's not clear that this is any less informative than just throwing data away.

    \resp{We agree that it would be interesting to allow drops to be omitted before filters.
      This would increase the plan space somewhat and might lead to better plans in some cases.
      This optimization would only change the output of the filter when the predicate is disjunctive, so would not have an impact on the queries we used in our experiments.
      Filters with conjunctive predicates behave identically with and without the drop, although removing the drop will give a minor performance improvement.}

  \item S2: Why no unions?
    
    \resp{We skipped unions for brevity. There is nothing fundamental about our algorithm which makes unions difficult to implement.}

  \item S3.4.1, Definition 2: On my first read-through, I was puzzled about why there was no plan with zero-loss. It would help to clarify that Loss is not about tuple loss, but rather a heuristic penalty function.

    \resp{We have clarified this in the paper by emphasizing that \textit{Loss} is not a loss in the statistical sense or in the tuple loss sense and by renaming it to \textit{Penalty}.}

  \item S3.4.3: An extremely nice emergent property from the L-value function for the imputation operator seems to be that the operator will be moved to a position in the plan where it has access to the maximum training data. I am slightly concerned that by focusing entirely on size, this might lead to skew in the training set. For example, consider a one-to-many join \verb/µ(R) |X| S/. Your optimization algorithm would likely wish to move imputation up: \verb/µ(R |X| S)/. Yes, there is more training data, but if the fanout for each tuple is skewed, some tuples in R will become over-represented in the join result. A similar observation holds for selection. For example take your Q8, where the conditions "studentdebtowe $>$ 0 AND schooldegree $\geq$ 0" presumably indicate invalid data that should not be used for training. (is this perhaps why you have so much error for full imputation on Q8?)

    \resp{This is a reasonable concern. Though often an appropriate proxy, optimizing for the maximum training set is flawed when the query requires the construction a large, skewed data set. The problem seems to be particular to queries which generate large intermediate tables. In some cases, as when joining two datasets, the intermediate table is a good target for training because it exposes correlations which would be hidden in the separate tables. In other cases, this intermediate table is large and skewed, which causes problems for imputation. Avoiding this case requires us to estimate the skew in these intermediate tables.

      To clarify the point about Q8, we've removed the predicate condition $schooldegree \geq 0$. This was in fact a trivially satisfied predicate given the way our dataset was set up.}
    \todo{Need a better explanation here.}

  \item S3.4.4: It appears that Time() assumes that plans are IO-bound. This is a perfectly reasonable assumption, but should be stated explicitly, as datasets that fit in memory will require a different cost model.

    \resp{Agreed. Our database stores tables in files, so our plans are I/O bound, but a database which uses an in-memory representation will need a different Time() function. We have clarified this in the paper.}

  \item S3.4.4: "Drop is a special case of a sequential scan, and its time complexity can be expressed as a function of the number of heap pages read and the I/O cost per page." seems to suggest that drop is implemented as a blocking operator. Why not just take the same approach as selection and treat it as having negligible cost?
    \resp{We mistakenly suggest in S3 that we treat \textit{Drop} as a blocking operator. In
      our implementation, it has $0$ cost, and we have updated the text to reflect that.}

  \item S4: All of the workload queries are (data-size-independent) AVG queries. What about other types of aggregates (e.g., SUM, COUNT)?

    \resp{Though we do not show this explicitly, we repeat all of the queries using COUNT
      aggregates in order to compute the ``Count Fraction''. The timing results are
      clearly identical in this case and the number of tuples returned can be seen
      directly in Table 4.}

    \todo{For SUM queries, that may depend more directly on the number of rows returned than the
      quality of imputations for any individual row, the effect of different COUNT results
      dominates the computation AVG*COUNT. How do we handle this?}

  \item S4.4.1: Imputation over the raw data is being used as ground truth. Is there any reason to suspect that this is the correct thing to do? Perhaps joining multiple tables together exposes new attributes that makes imputation *more* reliable? At the very least, it would be useful to understand why there is so much error in $\alpha=0$ on Q2 and Q8.

    \resp{For our purposes, imputation over the raw data is a reasonable ground truth. This
      is because the analyst, in practice, should be considering the tradeoffs on quality
      and performance of using ImputeDB versus a production imputation system on the raw
      data. Unfortunately, they would never have access to a real ground truth in such a
      scenario, and moreover, finding ground truth for use even in an empirical evaluation
      of imputation methods can be challenging because artificially dirtying the data
      often does not capture the complex reasons for missingness in real world datasets.
      We update the discussion in S4.4.1 to emphasize this point.}

  \item There are a number of opportunities for trimming in the figures, tables, and algorithms: Algorithms 1-3 could be compacted a bunch, Table 1 is full of whitespace (e.g., make it a 3-column {figure*}), Tables 1-2 could also be summarized as CDFs, Figure 6 is full of whitespace, Table 3 could be put queries on a single row (e.g., Q1 would easily fit on 1 line), and the references have a lot of redundant information (URLs, for example).

    \resp{We have compacted the algorithms and tables. Note that Tables 1-2 show the percentage of missing values for \emph{each attribute}, not as an overall percentage, so cannot be summarized as CDFs. We think that the URLs add value to the references, particularly for data sets.}

  \item Additional space recovered from these sections could be used to discuss the choice of techniques for imputation, and the confidence estimation idea suggested in the future work section. 

  \item Another interesting use of the space, or at least an idea for future work might be the role of aggregation on inference accuracy. In my experience aggregate queries tend to be *much* more forgiving of approximations (e.g., Online Aggregation) and data errors.
  \end{itemize}

  \section{Reviewer 2}
  \begin{itemize}
  \item 1) No new imputation method is presented. The model relies on external imputation techniques, and given it's done on-the-fly, presumably imputed values depend on qualifying query records.

    \resp{Our contribution is the planning algorithm which allows imputation to be interleaved into the query plan.
      We do not present a new imputation method, but our technique is agnostic to the particular imputation method used.
      We reduce the overhead of imputation by performing it only on the tuples which are relevant to a particular query, so the imputed results do depend on the records which are relevant to particular query.}
    
  \item 2) No comparative evaluation against existing imputation/data cleaning techniques.

    \resp{
      We compare our planning technique for on-the-fly imputation to imputation performed on the full dataset, using the same imputation algorithm in both cases.
      We are not aware of another database technology which offers on-the-fly imputation, so there is no direct comparison for the planning algorithm.
      % Our evaluation compares our technique to an imputation of the full dataset. We use the same imputation algorithm for our on-the-fly imputation as for the full dataset imputation. Cleaning the full dataset is the standard method of handling missing values.
    }

  \item D1) The model relies on external imputation techniques, and given it's done on-the-fly, the imputed values will differ based on the qualifying query records?

    \resp{Yes, as discussed in the response to 1.}

  \item D2) The paper does not propose new imputation techniques but rely on existing statistical imputation algorithms. There is no comparative evaluation, nor samples of imputed values to show the goodness of the imputed values. It would be nice to see a comparative evaluation of different statistical imputation methods, and suggestions for extensions that can fit within the proposed imputation model.

    \resp{
      We have added two new imputation methods to the experiments, to show that the planning algorithm can be used successfully with different imputation methods.
      We have also added a subsection to the algorithm section explaining how we chose the cost functions for each imputation method that we test and giving rules for choosing cost functions for a new imputation method.
    }
  \end{itemize}

  \section{Reviewer 3}
  \begin{itemize}
  \item W2. More work is required in the presentation: important parts are not explained in depth, while there are many details in other sections that can be omitted.

  \item W3. More experiments needed.

  \item D1. Section 3, which is the meat of the paper, is very overloaded, without getting in depth in various parts of the system. It definitely should be split in more sections, and more details should be given. In particular:
    -- Give some more information about cardinality estimation. Also, Algorithm 3 is too far from the text, and needs some explanation too.
    -- Some more details are also required in the Imputation quality section. Why are you choosing these specific formulas for Loss, especially for the impute operator. Some examples would help.
    -- Algorithms 1 and 2 could probably be presented in an easier to grasp way.

  \item D2. It would be nice to show how exactly your approach can be implemented in an existing optimizer. This comes up in various places in the paper:
    -- When defining the search space, why not first define the transformation operators that inject the imputation operators. Then any optimizer can pick these rules up and add them in its plan enumeration. Of course, the used approach that follows System-R's plan enumeration is useful, but why lose generality when presenting the search space. Moreover, showing the transformation rules can help you show the size of the search space that will better justify the restrictions you apply.
    -- Why use simplified cost formulas for the traditional relational operators and not existing systems' formulas?
    -- It would be great (but not necessary of course) to add your techniques in an existing DBMS, such as Postgres to see the impact on a real system in your experiments.

    \resp{
      Existing systems have very complex formulas for estimating query costs. We felt that the overhead of reimplementing an existing cost model would not be worth the benefit.
      However, we added a new section that discusses in detail the main components for \ProjectName{}, so that developers of existing systems can use this as a roadmap
      for implementation.

      \todo{More sophisticated cost formulas from exiting systems could further improve our time estimates for traditional operators. In this case, we focused on developing the heuristics
        relevant for the new operators, at the expense of simplifying implementation of common formulas, as we felt that was core to our contribution.}
    }

  \item D3. Is there any way for the user to know what error to expect based on the $\alpha$ value? Moreover, is its value somehow proportional to the error (e.g., $\alpha=0.5$ would give roughly double the error of $\alpha=0.25$)? Or should the user simply try multiple execution with different values of $\alpha$ until the desired results are given?

    \resp{In our intended usage, the user starts by running with $\alpha = 0$ and increases $\alpha$ if the query takes too long to complete.
      \todo{
        We agree that having a way of providing an estimate on error would be interesting. Since error in this case relates to the imputation values
        generated by the whole-table imputation (as there is no ground truth), this would likely need to be on a case-by-case basis for distinct
        instantiations of the impute operator. 
      }
    }

  \item D4. How would a two-step approach that first finds the optimal plan (w/o imputation) and then adds the imputation operators perform?

    \resp{This approach would not be able to improve on the plans produced by our approach, as it would produce a subset of the plans that we consider. It's unclear how much worse the plans would be, but a major issue is that the Drop and Impute operators have an impact on the cardinality of their outputs. Our optimizer can use this information to improve the plan, but adding imputation after the fact would not be able to rearrange the plan to capitalize on changes in cardinality.}

  \item D5. Can you have plans with a mix of drop and impute operators?
    
    \resp{Yes.}

  \item D6. During plan enumeration, can you safely prune plans at some point, similar to what a traditional optimizer would do?

    \resp{We maintain the set of pareto optimal plans at each point in the optimization. This set is significantly smaller than the set of all plans, so it represents some pruning. We found that the size of these pareto sets is small enough that we did not need to prune further.

      If necessary, it is possible to sample from the pareto optimal plans during the plan enumeration. The final selected plan may not be pareto optimal, but this is a potential way to control the number of plans.}
    \todo{Talk about pareto set approximation here.}

  \item D7. When defining the cost of impute operator, how do you know a priori the number of iterations for the algorithm (given it has to converge)?
    \todo{Micah: what do we actually do here?}

  \item D8. Are there impute implementation that are non-blocking? Or do they always have to perform multiple passes over the data?

    \resp{Yes. For example, in time series data, it is reasonable to fill the most recent non-null value forward. This kind of imputation isn't useful for survey data, so we didn't consider it for our experiments. We have also extended the experiments section to consider a simple imputation method which fills missing values with the mean of the column, which only needs one pass over the data.}

  \item D9. Additional experiments I suggest to include:
    -- Consider a scenario where the user submits multiple similar queries. Show after how many such queries your approach has no longer a competitive advantage compared to performing imputation on the data.
    -- Try different values for $\alpha$ (not just 0 and 1).
    -- Show runtime of enumeration algorithm as queries get more complex.

    \resp{
      The sensitivity of different values of $\alpha$ depends on the query itself. Performing a scan of values of $\alpha$ with small step sizes can return
      a lot of the same plans for execution. In lieu of this, we extended the results section to include a subsection that presents the final frontiers for
      two queries, one from each dataset used during evaluation. This frontier can be exposed to the user to help in their workflow.
      \\
      We explored the planning times for queries with increasing number of joins. The standard algorithm presented in the original version submitted handles
      up to 5 joins, with columns requiring imputation in all tables, with planning time of approximately one second. We added a small optimization to help reduce
      planning times for a larger number of joins. We maintain an approximation of the Pareto sets, rather than the exact set, by only adding non-dominated plans that
      are sufficiently different. This helps reduce some of the increases in planning time for a larger number of joins.}

  \item D10. The scenario you are considering, with multiple queries during exploration, begs for reuse of previous query results, as you also mention in the future work section.

    \resp{We agree.
      Reusing previous query results is a tricky problem because it requires the database to speculatively store some of the intermediate results of its query plans.
      A simple heuristic which should work well is to store the results of full table imputation whenever a query forces this to occur. These results are guaranteed to be useful later.
      Storing other intermediate results is problematic, particularly those which are computed after filtering or joining, as we would need to have a mapping back to the tuples in the base tables.}

  \item D11. Less important points:
    -- Sections 4.2 and 4.3 in the experiments are too verbose.
    -- In Section 3.2, you mention that joins are performed after filtering. When filters are on results of joins, this is not possible.

    \resp{The subset of SQL that we consider does not allow filtering on the results of joins (see Section 3.2). Extending the planner to cover this case is not complex. Before adding filters, it must add imputations on the required attributes.}

  \item -- Given you optimize for both performance and quality, Tummer's and Koch's work on multi-objective QO is related.

    \resp{Yes, Tummer \& Koch is definitely related.
      However, much of the complexity of their method is in dealing with cost functions which are not well behaved.
      Our cost functions are monotonic, which allows us to use a simpler optimizer than what Tummer \& Koch propose.
    We have added a discussion of their work to the related works section.}

\item -- Could be interesting for the optimizer to pick imputation strategy based on the type of the query.
  \end{itemize}
\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
