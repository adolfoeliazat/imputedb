\section{Algorithm}

Our optimizer searches a restricted space (Sec.~\ref{sec:search-space}) of query plans for a plan that minimizes a metric (Sec.~\ref{sec:cost-model}) which combines the runtime cost of the query and the quality of the results.
This plan must not emit any tuples which contain null values, regardless of the state of the base tables.
Additionally, the traditional relational operators (selection $\sigma$, projection $\pi$, join $\bowtie$, and group-by/aggregate) must never observe a null value.

\subsection{Imputation operators}
We introduce two new relational operators to perform imputation: impute ($\mu$) and drop ($\delta$). Each operator takes arguments $(C, R)$ where $C$ is a set of attributes and $R$ is a relation. Impute uses a machine learning method to replace all null values with non-null values for attributes in $C$ in the relation $R$. Drop simply removes from $R$ all tuples which have a null value for some attribute in $C$. Both operators guarantee that the resulting relation will contain no null values for attributes in $C$. 

\subsection{Cost model}
\label{sec:cost-model}
We rank query plans $Q$ using a cost model $\text{Cost}(Q) = \alpha \times \text{Time}(Q) + (1 - \alpha) \times \text{Loss}(Q)$. $\text{Time}(Q)$ is an estimate of the runtime of the query which is derived from table statistics and selectivity estimation of the query predicates. $\text{Loss}(Q)$ is an estimate of the amount of error introduced by the imputation procedure. $\alpha$ is a parameter to the query optimizer that controls the emphasis on quality versus performance. $\alpha = 1.0$ means that the query should be as fast as possible, $\alpha=0.0$ means that the query should be as accurate as possible.

In order to correctly estimate $\text{Time}(Q)$ and $\text{Loss}(Q)$, the system must have cardinality estimates
for each sub-query in each query plan.
These cardinality estimates are impacted not just by filtering or joining, as in the traditional relational calculus, but also by the imputation operators.
For example, a drop operator will reduce the cardinality of the result while an impute operator will maintain the same cardinality as the input.
For simplicity, each of the logical nodes in a query plan refers to a set of histograms.
When the optimizer creates a new query plan, it copies the histograms of the sub-plans and modifies them as necessary to account for new operation in the plan.
Algorithm~\ref{algo:histogram-transformation} describes the process of generating new histograms from sub-plans.

The computation of $\text{Time}(Q)$ is standard and is comparable to the cost model used in any modern RDBMS.\@ Our runtime cost model is perhaps less sophisticated than most, as it is identical to the cost model in SimpleDB.

\todo{Check loss calculation w/ Micah}
$\text{Loss}(Q)$ is computed as follows.
For each sub-query, in addition to a histogram, we keep track of the estimated number of null values in each column.
\begin{align*}
  \text{Loss}(Q) = \begin{cases}
    \sum_{c \in C} \text{Null}(c, Q') & Q = \delta_C(Q') \\
    \frac{1}{\sqrt{|Q'|}} \sum_{c \in C} \text{Null}(c, Q') & Q = \mu_C(Q') \\
    \text{Loss}(Q_1') + \text{Loss}(Q_2') & Q = Q_1' \bowtie_\psi Q_2' \\
    \text{Loss}(Q') & Q = \sigma_\phi(Q'), Q = \pi_C(Q') \\
  \end{cases}
\end{align*}
Dropping tuples which contain a null value incurs a loss penalty of 1 for each null field dropped.
Imputing null fields incurs a loss penalty $0 < p \leq 1$ which decreases as the number of tuples available to train the imputation algorithm increases.
The intuition behind this formula is that imputation performance should increase if there is more training data available, but there is a saturation point beyond which more data provides little additional benefit.

\subsection{Search space}
\label{sec:search-space}
To reduce the size of the query plan search space, only plans that fit the following template are considered. First, all filters are pushed to the leaves of the query tree, immediately after the scans. Joins are performed after filtering, and only left-deep plans are considered. Any group-by/aggregate will be performed after the final join. Finally, projections are placed at the root of the query tree. The space of query plans is similar to that considered by System R~\cite{blasgen1981system}, with the addition of imputation operators appearing before/after traditional operators.

\subsection{Imputation placement}
\label{sec:placement}
Imputation operators must be placed so that no relational operator receives a tuple containing null in an attribute that the operator examines, regardless of the state of the data in the base tables.

Imputation operators can be placed at any point in the query plan, but to meet the guarantee that no non-imputation operator sees a null value, there are cases where an imputation operator is required. To track these cases, each query plan is associated with a set of dirty attributes $D$. An attribute $c$ is \emph{dirty} in some relation if the values for $c$ may contain null. We compute a dirty set for each base table using the table statistics, which track the number of null values in each column. If we apply an imputation operator to a relation $R$ with a dirty set $D$, $\mu_C (R)$ or $\delta_C (R)$, the resulting query has a dirty set $D' = D \setminus C$.  Applying a projection $\pi_C(R)$ produces a dirty set $D' = D \cap C$. A join $R_1 \Join_\psi R_2$ with dirty sets $D_1$ and $D_2$  produces a dirty set $D' = D_1 \cup D_2$.  Filters do not change the dirty set.

The dirty set over-approximates the set of attributes that contain null. For example, a filter might remove all tuples which contain null without changing the dirty set, forcing an unnecessary imputation. We choose to over-approximate the dirty set to avoid the possibility of dropping a tuple that contains a null value without explicitly imputing the value or applying a drop operator.

\subsection{Query planning}
The input to our query planner is a tuple $(T, \Phi, \Psi, P, G, A)$: a set of tables $T$, a set of filter predicates $\phi_t, t \in T$, a set of join predicates $\psi_(t_1, t_2), t_1, t_2 \in T$, a set of attributes $P$, and an optional set of attributes $G$ and aggregation function $A \in \{\text{Max}, \text{Min}, \text{Sum}, \text{Avg}, \text{Count}\}$.

The query planner must select a join ordering in addition to placing imputation operators as described in Section~\ref{sec:placement}.

To reduce the search space, we only consider the minimal imputation, the maximal imputation, and the minimal drop. The minimal imputation (resp. drop) only imputes (resp. drops) the columns required by the relational operator immediately following the imputation. The maximal imputation imputes all columns in the relation, regardless of which are required.  Algorithm~\ref{algo:plan-helpers} presents
a series of helper functions used by the top level planner, shown in Algorithm~\ref{algo:top-level-planner}.

\begin{algorithm}
\input{algorithms/plan-helpers.tex}
\end{algorithm}

\begin{algorithm}
\input{algorithms/plan-toplevel.tex}
\end{algorithm}

\begin{algorithm}
\input{algorithms/histogram-transform.tex}
\end{algorithm}

\subsection{Imputation Strategies}
The current version of the system uses an imputation strategy based on chained-equations regression trees (CERT)\cite{burgette2010multiple}. Note that plugging in a new
imputation strategy takes minimal effort, and doesn't require changes to the optimizer, so the system could in principle be tuned to a specific domain.
The imputation algorithm is iterative in nature, and proceeds by repeatedly fitting regression trees to a subset of the data and the target imputation columns.
In each iteration, the missing values of a column are replaced with newly imputed values. As the iterations proceed, the quality of the imputation
should improve as values progressively reflect more accurate relationships amongst attributes. Algorithm~\ref{algo:imputation-strategy}
provides details on the implementation.

It is worth noting that given the nature of the imputation strategy used here, imputation operators become blocking in our system.

\begin{algorithm}
\input{algorithms/cert.tex}
\end{algorithm}


\todo{Jose:Micah, take a look at the explanation above and see if it's missing anything (or is incorrect)}

\subsection{Complexity}
Our optimization algorithm builds off the approach taken by System R\cite{blasgen1981system}, therefore our algorithm still operates in exponential time. Indeed, 
note that if we remove our restriction on types of imputation($\delta_{min}, \mu_{min}, \mu_{max}$), and allow any arbitrary subset of attributes to be imputed,
then every single operator in the query plan has a number of imputations exponential in the number of dirty columns. Our restriction, instead, increases the number
of plans (in the worst case) at each operator by a factor of 3. Of course, this implies that in the worst case (where all dirty sets tracked are distinct throughout the query plan),
we explore a number of plans that further scales the number of plans in the original algorithm by an exponential factor.

However, we note that in all cases we have considered, this exponential blowup does not affect the practical performance of our optimizer.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
