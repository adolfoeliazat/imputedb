\section{Algorithm}
In order to correctly plan around missing values, we first extend the relational algebra operators (selection $\sigma$, projection $\pi$, join $\bowtie$, and group-by/aggregate $g$) 
with two new operators (\Cref{sec:operators}). We then define the search space (\Cref{sec:search-space}) of plans to encompass non-trivial operations, such as joins and 
aggregations. The decision to place imputation operators is driven by our cost model (\Cref{sec:cost-model}), which characterizes a plan based on estimates for the 
quality of the imputations performed and the runtime performance. Soundness of query execution is provided by our main design invariant, which guarantees traditional relational
algebra operators must never observe a missing value in any attribute that they operate on directly (\Cref{sec:placement}). Our imputation planning algorithm is
agnostic to the type of imputation used (\Cref{sec:imputation}). Finally, we show
that while our algorithm is exponential in the number of joins(\Cref{sec:complexity}), in practice, planning times are not a concern.


\subsection{Imputation operators}
\label{sec:operators}
We introduce two new relational operators to perform imputation: \textit{Impute} ($\mu$) and
\textit{Drop} ($\delta$). Each operator takes arguments $(C, R)$ where $C$ is a set of
attributes and $R$ is a relation. \textit{Impute} uses a machine learning algorithm to
replace all null values with non-null values for attributes in $C$ in the relation $R$ (discussed in detail in~\Cref{sec:imputation}).
In our experiments, we instantiate \textit{Impute} with the CE-CART\cite{burgette2010multiple} algorithm, but we highlight that
our system is agnostic to this choice.
\textit{Drop} simply removes from $R$ all tuples which have a null value for some attribute in $C$.
Both operators guarantee that the resulting relation will contain no null values for
attributes in $C$.  

\subsection{Search space}
\label{sec:search-space}
To keep the query plan search space tractable, only plans that fit the following template are considered.

\begin{itemize}
\item All filters are pushed to the leaves of the query tree, immediately after scanning a table. We use a pre-processing step which conjoins together filters which operate on the same table, so we can assume that each table has at most one relevant filter.

\item Joins are performed after filtering, and only left-deep plans are considered.

\item A grouping and aggregation operator is optional and if present will be performed after the final join.

\item Projections are placed at the root of the query tree.
\end{itemize}
  
The space of query plans is similar to that considered by System R~\cite{blasgen1981system}, with the addition of imputation operators appearing before and after traditional operators.
\Cref{fig:query-schematic} shows a plan schematic for a query involving three tables, absent any imputation operators.

\begin{figure}
\Tree [.$\pi$ [.$g$ [.$\bowtie$ [.$\bowtie$ [.$\sigma$ $t_1$ ] [.$\sigma$ $t_2$ ] ] [.$\sigma$ $t_3$ ] ] ] ]
\caption{Query plan schematic for the type of traditional plans explored (absent imputation operators).}
\label{fig:query-schematic}
\end{figure}

\subsection{Cost model}
\label{sec:cost-model}
The cost of a plan $q$ is expressed as a tuple $\langle \textsc{Loss}(q), \textsc{Time}(q) \rangle$.
$\textsc{Loss}(q) \in [0,1]$ is an estimate of the amount of information lost by the imputation procedures used.
$\textsc{Time}(q) \in (0, \infty)$ is an estimate of the runtime of a query $q$ derived from table statistics and selectivity estimation of the query predicates. 

\begin{definition}
We say that a plan $p$ with cost $\langle l, t \rangle$ dominates a plan $p'$ with cost $\langle l', t' \rangle$ if $(l \leq l' \land t < t') \lor (l < l' \land t \leq t')$. We denote this as $\doms{p}{p'}$. 
\end{definition}

Given a query, \ProjectName{} produces a final set of plans $P$ by only keeping plans that are not dominated by others in the search space. So for a search space $S$, \[P= \{ p ~|~ \not\exists p' \in S.\ p \neq p' \land \doms{p'}{p} \}.\]
 
This set $P$ will be the Pareto frontier of $S$~\cite{pareto1964cours}, and it contains the best option for all possible trade-offs of $\textsc{Time}$ and $\textsc{Loss}$ for the current query.

In order to pick a final plan from the frontier, our model introduces a tradeoff parameter $\alpha$ which encodes an upper bound on the $\textsc{Loss}$ value that the user is willing
to tolerate relative to the minimum possible amongst frontier plans.

\begin{definition}
Let $P$ be a set of plans. For $p \in P$, we say $p$ is $\alpha$-bound, if $(\textsc{Loss}(p) - \min_{p' \in P}(\textsc{Loss}(p')) \leq \alpha$. We denote this as $p^\alpha$.
We say $p$ is $\alpha$-bound optimal if $\forall p'^\alpha \in P: \textsc{Time}(p) \leq \textsc{Time}(p')$. 
\end{definition}

Given $\alpha$ and $P$, \ProjectName{} returns the $\alpha$-bound optimal plan in $P$. Note that the $\alpha$-bound optimal plan is unique in $P$, since $P$ contains only non-dominated plans.

So in essence, $\alpha$ is a tunable parameter that determines whether the optimizer focuses on quality or on performance, as plans that
lose a lot of information through dropping are also the fastest. 

$\alpha = 1.0$ means that the optimal query should have the fastest runtime (at the expense of quality), while $\alpha=0.0$ means that the optimal query should lose
as little information as possible (at the expense of performance).

The computation of $\textsc{Time}(q)$ and $\textsc{Loss}(q)$ relies on having an accurate cardinality estimate of a query plan $q$ and its sub-plans. 
These cardinality estimates are impacted not just by filtering or joining, as in the traditional relational calculus, but also by the imputation operators.
For example, a drop operator will reduce the cardinality of the result while an impute operator will maintain the same cardinality as the input.
For simplicity, each of the logical nodes in a query plan points to a set of histograms.
When the optimizer creates a new query plan, it copies the histograms of the sub-plans and modifies them as necessary to account for the new operation in the plan.
\Cref{algo:histogram-transformation} describes the process of generating new histograms from sub-plans.

For each sub-plan, we keep track of the estimated cardinality by using the histograms associated with
that plan. We use this estimate to compute $\text{Loss}(Q)$, a measure averaged over imputation operations, as follows.

\begin{align*}
  \textsc{L}(q) &= \begin{cases}
     \{1\} \cup \textsc{L}(q') & q = \delta_C(q') \\
    \left\{\frac{1}{\sqrt{ \textsc{Attr}(q') * \textsc{Card}(q')}}\right\} \cup \textsc{L}(q')  & q = \mu_C(q') \\
    \textsc{L}(q_1') \cup \textsc{L}(q_2') & q = q_1' \bowtie_\psi q_2' \\
    \textsc{L}(q') & q = \sigma_\phi(q'), \\ & q = \pi_C(q') \\
    \emptyset\ multiset & q = t\\
  \end{cases}\\
  \textsc{Loss}(q) &= \frac{\Sigma \textsc{L}(q)}{|\textsc{L}(q)|}
\end{align*}

\srm{I find this table hard to follow -- I think it'd be good to explain one or two of the rows in detail, and to explain what the set L contains.}
Using the drop operator incurs a loss of information of 1 (i.e. we lose all information that
the dropped tuples might have encoded). Imputing null fields incurs a loss penalty $p \in (0, 1]$,
which is inversely proportional to the number of total cells available to train the imputation algorithm. 
Total cells are defined as the product of the number of attributes and the number of tuples.
Intuitively, imputation
accuracy \textit{increases} when more complete attributes and complete tuples are available;
correspondingly, the information lost \textit{decreases} with more values to work
with. Since we conceive that
alternate imputation strategies could be used, the analyst may provide a corresponding
loss with a reasonable parameterization.

To compute $\textsc{Time}(Q)$, an additive measure of runtime cost, we retain a simple set of heuristics used by a standard 
database. 

%\begin{align*}
%  \textsc{T}(q) &= \begin{cases}
%     \{ f_1(pg_{fetch}, io_{cost}) \} \cup \textsc{T}(q') & q = \delta_C(q') \\
%      \{ f_2(\textsc{Attr}(q'), C, \textsc{Card}(q'), iters) \} \cup \textsc{T}(q')  & q = \mu_C(q') \\  
%     \{ f_3(\textsc{T}(q_1'), \textsc{T}(q_2')) \} & q = q_1' \bowtie_\psi q_2' \\
%    \{ \hat{t} \} \cup \textsc{T}(q') & q = \sigma_\phi(q'), \\ & q = \pi_C(q') \\
%    \{ f_4(\textsc{Card}(t), pg_{size}, io_{cost}) \} & q = t\\
%  \end{cases}
%  \intertext{ \textit{for some set of functions  $f_{1,2,3,4}$, constant $\hat{t}$, IO Cost/Page $io_{cost}$, page fetches $pg_{fetch}$, page size $pg_{size}$, iterations $iters$} } 
%  \textsc{Time}(q) &= \Sigma \textsc{T}(q)
%\end{align*}

Scan costs are estimated based on the number of tuples, 
page size, and an unit-less I/O cost per page. Join costs are estimated as a function
of the two relations joined, any blocking operations, and repeated fetching of tuples (if required).
Filtering, projecting, and aggregating are all done in memory and have low computational overhead
so we assume negligible time costs for those operations. We extend these heuristics to 
account for the new operators: \textit{Drop} and \textit{Impute}. \textit{Drop} is a special case of a sequential scan, and its time
complexity can be expressed as a function of the number of heap pages read and the I/O cost
per page.

The time computation for \textit{Impute} depends on the properties of the underlying
algorithm. \ProjectName{} treats the actual machine learning algorithm as a black box
and simply queries it for a time estimate. In the base implementation of our system,
we use an iterative algorithm (see~\Cref{sec:imputation}), where the runtime cost
is a function of the number of attributes with no missing data, number of attributes with
missing data being imputed, number of tuples, and the number of iterations for the algorithm.

In general, evaluating the time complexity for different \textit{Impute} operators is
complicated, as the properties of
different algorithms can vary significantly, the underlying I/O cost
is not easily extracted (especially in the case that the entire set of tuples does not
fit in memory), and the CPU cost dominates (in contrast to the other operators, which may
not even consider CPU cost explicitly). For example, \cite{martin1995time} find that the
time complexity of the \textit{build tree} algorithm for one commonly-used class of
decision trees is a function of the number of classes, several overhead constants, the
parameterization of the partition and heuristic functions, and the \textit{arity} (the
number of subsets considered for each split). Indeed, the \textit{build tree} phase often
does not even dominate computation, as post-processing steps like pruning, which are vital
in achieving good performance, can have cost exponential in the height of the tree. In
practice, we find that a simple parameterization can be acceptable (i.e. yield intelligent query
plans).


\subsection{Imputation placement}
\label{sec:placement}
Imputation operators must be placed so that no relational operator receives a tuple containing null in an attribute that the operator examines, regardless of the state of the data in the base tables.

Imputation operators can be placed at any point in the query plan, but to meet the guarantee that no non-imputation operator sees a null value, there are cases where an imputation operator is required. To track these cases, each query plan $q$ is associated with a set of dirty attributes $\textsc{Dirty}(q)$. An attribute $c$ is \emph{dirty} in some relation if the values for $c$ contain null. We compute a dirty set for each table using the table histograms, which track the number of null values in each column. The dirty set for a query plan can be computed recursively as follows:

\begin{align*}
  \textsc{Dirty}(q) = \begin{cases}
    \textsc{Dirty}(q') \setminus C & q = \delta_C(q')\ \text{or}\ q = \mu_C(q') \\
    \textsc{Dirty}(q') \cap C & q = \pi_C(q') \\
    \textsc{Dirty}(q_l) \cup \textsc{Dirty}(q_r) & q = q_l \bowtie_\psi q_r \\
    \textsc{Dirty}(q') & q = \sigma_\phi(q') \\
    \textsc{Dirty}(t) & q = \text{some table}\ t \\
  \end{cases}
\end{align*}

Note that \textsc{Dirty} over-approximates the set of attributes that contain null. For example, a filter might remove all tuples which contain null, but the dirty set would be unchanged. We choose to over-approximate to ensure that all null values are explicitly imputed or dropped.

\subsection{Query planning}
\label{sec:planning}
The input to our query planner is a tuple $(T, F, J, P, G, A)$:
\begin{itemize}
\item A set of tables $T$.
\item A relation $F: T \times \Phi$ between tables and filter predicates.
\item A relation $J : T \times \Psi \times T$ between tables and join predicates.
\item A set of projection attributes $P$.
\item An optional set of grouping attributes $G$ and aggregator function $A$.
\end{itemize}

The query planner must select a join ordering in addition to placing imputation operators as described in~\Cref{sec:placement}.

Before describing the operation of the query planner, we describe the semantics of a specialized data structure for holding sub-plans, called a \emph{plan cache}.
At a high level, a plan cache is a mapping from a set of tables to a set of dirty-set specific dominating plans that perform the required joins and filters over these tables.
We store multiple plans rather than a single plan because each plan has a distinct set of dirty attributes, each of which maintains its own Pareto frontier. 
Details of the plan-cache semantics are shown in~\Cref{fig:semantics-plan-cache}.


\begin{figure}
  \begin{align*}
    \llbracket Q[T] \rrbracket &= \begin{cases}
      P & (T, P) \in Q \\
      \emptyset & \text{otherwise} \\
    \end{cases} \\
    \llbracket Q[T] \lhd p \rrbracket &= \begin{aligned} Q \cup \{(T,\ P') \} \end{aligned}\\
                                            \text{ where } \\
                                           P' &= \left\{q ~|~ \begin{aligned}
                                                                     q \in \llbracket Q[T] \rrbracket \cup \{p\}, \\
                                                                     \textsc{Dirty}(q) = \textsc{Dirty}(p),\\
                                                                     \forall q' \in \llbracket Q[T] \rrbracket, \textsc{Dirty}(q')=\textsc{Dirty}(q).  \doms{q} {q'} 
                                                                   \end{aligned}
                                           \right\}
  \end{align*}
  \caption{Semantics of a plan cache $Q$}
  \label{fig:semantics-plan-cache}
\end{figure}

\todobox{jose rewrite this clearly}{
While $\textsc{Loss}$ is not monotonically increasing in our model, we maintain a collection of sound and complete Pareto frontiers by 
computing these specific to each dirty set. That is to say, that the combination of $\textsc{Loss}(p) \times \textsc{Dirty}(p)$ for some
plan $p$ behaves monotonically, as the dirty-set is strictly decreasing through the application of $\delta, \mu$ operations. 
The final Pareto frontier corresponds to the collection of plans associated with the empty dirty set.}

To reduce the search space, we only consider the minimal imputation, the maximal imputation, and the minimal drop. The minimal imputation (resp.\ drop) only imputes (resp.\ drops) the columns required by the relational operator immediately following the imputation. The maximal imputation imputes all of the attributes in the relation which are used by other operators in the query plan or are visible in the query output.

\Cref{algo:plan-helpers} presents a series of helper functions used by the top-level planner, shown in~\Cref{algo:top-level-planner}.

The planner operates as follows.
First, it collects the set of attributes which are used by the operators in the plan or which are visible in the output of the plan (\Cref{lst:line:attr-start}--\Cref{lst:line:attr-end}).
This set will be used to determine which attributes are imputed.
Then, it constructs a plan cache. For each table used in the query, any available selections are pushed to the leaves and
a search over imputation operations produces various possible plans, which are added to the cache (\Cref{lst:line:sel}). If no selections are available, a 
simple scan is added to the plan cache (\Cref{lst:line:scan}). Join order is jointly optimized with imputation placements and a set of
plans encompassing all necessary tables is produced (\Cref{lst:line:join}). This set is now extended for any necessary grouping and aggregation
operations (\Cref{lst:line:group}). After a similar step has been taken for projections, the planner now contains the plan frontier (i.e.\ the
best possible set from which to pick our final plan). The final step in the planner is to find all plans that are
$\alpha$-bound and return the one that has the lowest runtime: the $\alpha$-bound optimal plan.

The join optimization is based on the algorithm used in System R~\cite{blasgen1981system},
but rather than select the best plan at every point, we use our plan cache, which keeps track of the tables joined and the dirty-set specific Pareto frontiers.
The output of the join optimizer is the set of plans which satisfy all join predicates and lie along their respective frontier.

\begin{algorithm}
\input{algorithms/plan-helpers.tex}
\end{algorithm}

\begin{algorithm}
\input{algorithms/plan-toplevel.tex}
\end{algorithm}

%\begin{algorithm}
%\input{algorithms/histogram-helpers.tex}
%\end{algorithm}

\begin{algorithm}
\input{algorithms/histogram-transform.tex}
\end{algorithm}



\subsection{Imputation Strategies}
\label{sec:imputation}

\ProjectName{} uses a general-purpose imputation strategy based on chained-equation classification and regression trees (CE-CART).
Chained equation imputation methods~\cite{vanbuuren2011mice} (sometimes called \textit{iterative regression}~\cite{gelman2006data}) impute multiple missing attributes by iteratively fitting a predictive model of one missing attribute which is conditional on all complete attributes and the other missing attributes.
Decision tree algorithms like CART are found to be effective~\cite{akande2015empirical} and are widely used in epidemiological domains~\cite{burgette2010multiple}.

\ProjectName{} is designed so that any imputation strategy can be plugged in with minimal effort and without changes to the optimizer, simply by adding time and loss functions (\Cref{sec:cost}) which describe the new algorithm.
In principle, new imputation algorithms can be added which are targeted to a specific domain.

These imputation algorithms are rarely used in isolation --- rather, the ultimate goal is to
perform statistical analysis of the complete data. To this end, the technique of
\textit{multiple imputation} can be used, in which multiple distinct copies of the complete
data are generated and estimators are computed by averaging over the multiple datasets. In
ImputeDB, we cannot assume the user desires multiple copies of the query result, though we
note that in further work, multiple imputation could be used within ImputeDB to reduce the
variance of estimates of some aggregates, like averages.

Our algorithm proceeds by iteratively fitting regression trees to a subset of the data and
the target imputation columns. In each iteration, the missing values of a column are
replaced with newly imputed values. With each epoch, the quality of the imputation improves
as values progressively reflect more accurate relationships amongst attributes. The
algorithm terminates when convergence is achieved (i.e.\ the imputed values do not change across
epochs) or a fixed number of epochs is reached. 

The flexibility of this non-parametric imputation approach aligns well with the spirit of
query optimization, which aims to reconcile performance with a declarative interface to data
manipulation. 

It is worth noting that given the nature of the imputation strategy used here, imputation
operators become blocking in our system. An extension of ImputeDB could consider regression
algorithms that learn in an online nature; this would reduce the cost of the imputation at
the possible expense of quality. Again, given the imputation strategy-agnostic design of
ImputeDB, this would not pose a problem.

%\begin{algorithm}
%    \input{algorithms/cart.tex}
%\end{algorithm}

%\subsection{Optimality}\label{sec:optimal}
%We prove the optimality of the final plan selected by showing that our cost-model and algorithm
%construct a complete Pareto frontier.
%
%\begin{definition}
%$I: \mathcal{P}(D) \rightarrow \mathcal{P}(\{ \delta, \mu \})$, where
%$D$ is the dirty set for a relation, is an abstract imputation operator mapping a set of
%attributes with missing values to a set of imputation operations.
%
%For convenience we denote applying the set of imputation operations to a plan $p$ as $I(p)$ and understand that this
%is shorthand for $\{ op(p)   | op \in I(\textsc{Dirty}(p)) \}$.
%\end{definition}
%
%We can succinctly describe possible imputation transformations to a plan at a given point in the planning algorithm solely through $I$.
%
%\begin{theorem}\label{theorem:no-bad-sit}
%By computing dirty-set specific Pareto frontiers, it is never the case that we discard a plan $p_1$ s.t. $\doms{p_2}{p_1}$, but 
%$\exists p_1' \in I(p_1), p_2' \in I(p_2). \doms{p_1'}{p_2'} \land \textsc{Dirty}(p_1') = \textsc{Dirty}(p_2') \land p_1' \not \in I(p_2)$
%\end{theorem}
%
%\Cref{theorem:no-bad-sit} states that there is no \textit{bad situation} such as that depicted in~\Cref{fig:bad-sit-diagram}
%
%\begin{figure}
%\centering
%\begin{tikzpicture}
%% before I
%\node(p1) {$p_1$};
%\node[right=of p1](dom1){$\prec$};
%\node[right=of dom1](p2) {$p_2$};
%
%% after I
%\node[below=of p1](p1'){$p_1'$};
%\node[right=of p1'](dom2) {$\succ$};
%\node[right=of dom2](p2'){$p_2'$};
%
%% arrows
%\draw[->] (p1) -- node[right] {$\scriptstyle I$} (p1');
%\draw[->] (p2) -- node[right] {$\scriptstyle I$} (p2');
%\end{tikzpicture}
%\caption{A bad Pareto frontier selection would choose $p_2$ over $p_1$, as it is locally optimal, but with the available transformations ($I$) $p_2$  can only reach $p_2'$, which is dominated by the inaccessible $p_1'$.~\Cref{theorem:no-bad-sit} says such a situation cannot arise when constructing Pareto frontiers on a dirty-set basis.}
%\label{fig:bad-sit-diagram}
%\end{figure}
%
%\begin{proof}
%For contradiction, let $p_1$ and $p_2$ be plans such that $\doms{p_2}{p_1}$, $p_1$ is removed from the plan cache
%in favor of $p_2$, but there  $\exists p_1' \in I(p_1), p_2' \in I(p_2). \doms{p_1'}{p_2'} \land \textsc{Dirty}(p_1') = \textsc{Dirty}(p_2')$
%and $p_1'$ is not reachable from $p_2$ by $I$.
%
%There are two possible cases :
%
%\begin{case}
%$\textsc{Dirty}(p_1) = \textsc{Dirty}(p_2)$, in which case $I(p_1) = I(p_2)$, as $I$ uniquely determines the possible transformations. This makes $p_2$ and $p_1$ substitutable, so $p_1'$ is reachable from $p_2$. This is a contradiction.
%\end{case}
%
%\begin{case}
%$\textsc{Dirty}(p_1) \not= \textsc{Dirty}(p_2)$, in which case $p_1$ is not removed from the plan cache as the $\doms{p_2}{p_1}$ relation does not occur within the same dirty set. This is a contradiction
%\end{case}
%
%So it must be the case that no such situation arises.
%\end{proof}
%
%
%Given that no desirable plans are discarded by computing Pareto frontiers for each dirty set, the final Pareto frontier (for dirty set $\emptyset$) is optimal.

\subsection{Complexity}\label{sec:complexity}
Our optimization algorithm builds off the approach taken by System R~\cite{blasgen1981system}.
If imputation operators are ignored, we search the same plan space.
Therefore, our algorithm has a complexity of at least $O(2^J)$, where $J$ is the number of joins.

The addition of imputation operators increases the number of plans exponentially, as an imputation may be placed before any of the relational operators.
We restrict imputations to two classes: those that impute only attributes used in the operator and those that impute all the attributes that are needed downstream in the query.
By doing so we limit the number of imputations at any point to four: \textit{Drop} or \textit{Impute} over the attributes used in the operator or over all the downstream attributes.
This modification increases the complexity of our planning algorithm to $O(2^{4J})$.

To motivate the restriction of imputation types, we consider the implications of allowing arbitrary imputations.
If we allow any arbitrary subset of attributes to be imputed, then we would need to consider $O(2^{|D|+1})$ different imputation operators before each relational operator where $D$ is the set of dirty attributes in all tables.
This would increase the overall complexity of the algorithm drastically.

Finally, we note that for the queries we have examined, this exponential blowup does not affect the practical performance of our optimizer.
Recall that the planner maintains different plans for different dirty sets, keeping only those that are not dominated by other plans with the same dirty set.
So in many cases we can drop some of these intermediate plans generated at each operator.
The worst case complexity only occurs if the dirty sets tracked are distinct through the entire planning phase.
Planning times are discussed in \Cref{sec:results}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
