\section{Algorithm}
In order to correctly plan around missing values, we first extend the set of relational algebra operators (selection $\sigma$, projection $\pi$, join $\bowtie$, and group-by/aggregate $g$) 
with two new operators (\Cref{sec:operators}). We then define the search space (\Cref{sec:search-space}) of plans to encompass non-trivial operations, such as joins and 
aggregations. Soundness of query execution is provided by our main design invariant, which guarantees traditional relational
algebra operators must never observe a missing value in any attribute that they operate on directly (\Cref{sec:placement}). 
The decision to place imputation operators is driven by our cost model (\Cref{sec:cost-model}), which characterizes a plan based on estimates for the 
quality of the imputations performed and the runtime performance. Our imputation planning algorithm is
agnostic to the type of imputation used (\Cref{sec:imputation}). Finally, we show
that while our algorithm is exponential in the number of joins (\Cref{sec:complexity}), in practice, planning times are not a concern.


\subsection{Imputation operators}
\label{sec:operators}
We introduce two new relational operators to perform imputation: \textit{Impute} ($\mu$) and
\textit{Drop} ($\delta$). Each operator takes arguments $(C, R)$ where $C$ is a set of
attributes and $R$ is a relation. \textit{Impute} uses a machine learning algorithm to
replace all \nullv{} values with non-\nullv{} values for attributes $C$ in the relation $R$ (discussed in detail in~\Cref{sec:imputation}).
In our experiments, we implement \textit{Impute} using a chained-equation decision trees algorithm, but our system is agnostic to this choice.
\textit{Drop} simply removes from $R$ all tuples which have a \nullv{} value for some attribute in $C$.
Both operators guarantee that the resulting relation will contain no \nullv{} values for
attributes in $C$.  

\subsection{Search space}
\label{sec:search-space}
To keep the query plan search space tractable, only plans that fit the following template are considered.

\begin{itemize}
\item All selections are pushed to the leaves of the query tree, immediately after scanning a table. We use a pre-processing step which conjoins together filters which operate on the same table, so we can assume that each table has at most one relevant filter.

\item Joins are performed after filtering, and only left-deep plans are considered.
  This significantly reduces the plan space while still allowing plans with interesting join patterns.

\item A grouping and aggregation operator is optional and if present will be performed after the final join.

\item Projections are placed at the root of the query tree.
\end{itemize}
  
The space of query plans is similar to that considered in a canonical
cost-based optimizer~\cite{blasgen1981system}, with the addition of imputation operators appearing before and after traditional operators.
\Cref{fig:query-schematic} shows a plan schematic for a query involving three tables, absent any imputation operators.

\begin{figure}
  \begin{minipage}[c]{0.5\columnwidth}
    \Tree [.$\pi$ [.$g$ [.$\bowtie$ [.$\bowtie$ [.$\sigma$ $t_1$ ] [.$\sigma$ $t_2$ ] ] [.$\sigma$ $t_3$ ] ] ] ]
  \end{minipage}\begin{minipage}[c]{0.5\columnwidth}
    \caption{
       Query plan schematic for the type of traditional plans explored (absent imputation operators).
    } \label{fig:query-schematic}
  \end{minipage}
\end{figure}

\subsection{Imputation placement}
\label{sec:placement}
We place imputation operators into the query plan so that no relational operator encounters a tuple containing \nullv{} in an attribute that the operator examines, regardless of the state of the data in the base tables.

Imputation operators can be placed at any point, but there are cases where an imputation operator is required to meet the guarantee that no non-imputation operator sees a \nullv{} value. To track these cases, we associate each query plan $q$ with a set of dirty attributes $\textsc{Dirty}(q)$. An attribute $c$ is \emph{dirty} in some relation if the values for $c$ contain \nullv{}. We compute a dirty set for each table using the table histograms, which track the number of \nullv{} values in each column. The dirty set for a query plan can be computed recursively as follows:

\begin{align*}
  \textsc{Dirty}(q) = \begin{cases}
    \textsc{Dirty}(q') \setminus C & q = \delta_C(q')\ \text{or}\ q = \mu_C(q') \\
    \textsc{Dirty}(q') \cap C & q = \pi_C(q') \\
    \textsc{Dirty}(q_l) \cup \textsc{Dirty}(q_r) & q = q_l \bowtie_\psi q_r \\
    \textsc{Dirty}(q') & q = \sigma_\phi(q') \\
    \textsc{Dirty}(t) & q = \text{some table}\ t \\
  \end{cases}
\end{align*}

Note that \textsc{Dirty} over-approximates the set of attributes that contain \nullv{}. For example, a filter might remove all tuples which contain \nullv{}, but the dirty set would be unchanged. We choose to over-approximate to ensure that all \nullv{} values are explicitly imputed or dropped.


\subsection{Cost model}
\label{sec:cost-model}
The cost of a plan $q$ is expressed as a tuple $\langle \textsc{Penalty}(q), \textsc{Time}(q) \rangle$.
$\textsc{Penalty}(q) \in [0,1]$ is an estimate of the amount of information lost by the imputation procedures used.
$\textsc{Time}(q) \in (0, \infty)$ is an estimate of the runtime of a query $q$ derived from table statistics and selectivity estimation of the query predicates. 

\subsubsection{Pareto-optimal plans}
Given a query, \ProjectName{} produces a final set of plans $P$ by only keeping plans that are not dominated by others in the search space. So for a search space $S$, \[P= \{ p ~|~ \not\exists p' \in S.\ p \neq p' \land \doms{p'}{p} \}.\]

\begin{definition}
We say that a plan $p$ with cost $\langle l, t \rangle$ \emph{dominates} a plan $p'$ with cost $\langle l', t' \rangle$ if $\textsc{Dirty}(p) = \textsc{Dirty}(p') \land ((l \leq l' \land t < t') \lor (l < l' \land t \leq t'))$. We denote this as $\doms{p}{p'}$. 
\end{definition}

This set $P$ will be the Pareto frontier of $S$~\cite{pareto1964cours}, and it contains the best option for all possible trade-offs of $\textsc{Time}$ and $\textsc{Penalty}$ for the current query.

In order to pick a final plan from the frontier, our model introduces a trade-off parameter $\alpha$, which is an upper bound on the $\textsc{Penalty}$ value that the user is willing
to tolerate relative to the minimum possible amongst frontier plans.

\begin{definition}
Let $P$ be a set of plans. For $p \in P$, we say $p$ is $\alpha$-bound, if $(\textsc{Penalty}(p) - \min_{p' \in P}(\textsc{Penalty}(p')) \leq \alpha$. We denote this as $p^\alpha$.
We define the set of $\alpha$-bound plans in $P$ as $P^\alpha$.
We say a plan $p^\alpha$ is $\alpha$-bound optimal if $\forall p' \in P^\alpha. \textsc{Time}(p) \leq \textsc{Time}(p')$. 
\end{definition}

Given $\alpha$ and $P$, \ProjectName{} returns an $\alpha$-bound optimal plan in $P$.

So in essence, $\alpha$ is a tunable parameter that determines whether the optimizer focuses on quality or on performance, as plans that
lose a lot of information through dropping are also the fastest. 

If $\alpha=0.0$, then the optimal query should lose as little information as possible (at
the expense of performance). If $\alpha = 1.0$, then the optimal query should have the
fastest runtime (at the expense of quality).

\subsubsection{Cardinality estimation}
The computation of $\textsc{Time}(q)$ and $\textsc{Penalty}(q)$ rely on having an accurate cardinality estimate of a query plan $q$ and its sub-plans.
These cardinality estimates are impacted not just by filtering or joining, as in the traditional relational calculus, but also by the imputation operators.
For example, a drop operator will reduce the cardinality of the result while an impute operator will maintain the same cardinality as the input.

To compute cardinality, we maintain histograms for each column in the database.
During query planning, each of the logical nodes in a query plan points to a set of histograms which describe the distribution of values in the output of the node.
When the optimizer creates a new query plan, it copies the histograms of the sub-plans and modifies them as necessary to account for the new operation in the plan.
For example it will redistribute the count of tuples with \nullv{} in an imputed field.
\Cref{algo:histogram-transformation} describes the process of generating new histograms from sub-plans.

\subsubsection{Imputation quality}
We estimate the quality of the imputations in a plan $Q$ using a heuristic measure $\text{Penalty}(Q)$, which is computed as follows.

\begin{align*}
  \textsc{L}(q) &= \begin{cases}
     1 + \textsc{L}(q') & q = \delta_C(q') \\
    \frac{1}{\sqrt{ \textsc{Attr}(q') * \textsc{Card}(q')}} + \textsc{L}(q')  & q = \mu_C(q') \\
    \textsc{L}(q_l') + \textsc{L}(q_r') & q = q_l' \bowtie_\psi q_r' \\
    \textsc{L}(q') & q = \sigma_\phi(q'), q = \pi_C(q') \\
    0 & q = \text{some table } t\\
  \end{cases}\\
    \textsc{N}(q) &= \begin{cases}
     1 + \textsc{N}(q') & q = \delta_C(q') \\
     1 + \textsc{N}(q')  & q = \mu_C(q') \\
    \textsc{N}(q_l') + \textsc{N}(q_r') & q = q_l' \bowtie_\psi q_r' \\
    \textsc{N}(q') & q = \sigma_\phi(q'), q = \pi_C(q') \\
    0 & q = \text{some table } t\\
  \end{cases}\\
  \textsc{Penalty}(q) &= \frac{\textsc{L}(q)}{\textsc{N}(q)}
\end{align*}

Each imputation operator incurs an individual penalty.
We define \textsc{Penalty} as the average over the penalties incurred by the imputation operators. Our recursive definition of $L$ accumulates these penalties and $N$ counts the number of imputation operations.
For example, $\delta$ contributes a penalty of 1 (i.e. we lose all information  that dropped tuples might have encoded).
Imputing \nullv{} fields with $\mu$ incurs a penalty $p \in (0, 1]$.
In this case, the penalty is inversely proportional to the square root of the number of cells available to train the imputation algorithm, which is defined as the product of the number of attributes and the number of tuples.
Intuitively, imputation accuracy \textit{increases} when more complete attributes and complete tuples are available; correspondingly, the penalty \textit{decreases} with more values to work with.
Scaling the cell count by applying square root incorporates the intuition that the number of training observations has diminishing returns for imputation quality and we find that it works well in practice.

\subsubsection{Query runtime}
To compute $\textsc{Time}(Q)$, an additive measure of runtime cost, we retain a simple set of heuristics used by a standard 
database. 

%\begin{align*}
%  \textsc{T}(q) &= \begin{cases}
%     \{ f_1(pg_{fetch}, io_{cost}) \} \cup \textsc{T}(q') & q = \delta_C(q') \\
%      \{ f_2(\textsc{Attr}(q'), C, \textsc{Card}(q'), iters) \} \cup \textsc{T}(q')  & q = \mu_C(q') \\  
%     \{ f_3(\textsc{T}(q_1'), \textsc{T}(q_2')) \} & q = q_1' \bowtie_\psi q_2' \\
%    \{ \hat{t} \} \cup \textsc{T}(q') & q = \sigma_\phi(q'), \\ & q = \pi_C(q') \\
%    \{ f_4(\textsc{Card}(t), pg_{size}, io_{cost}) \} & q = t\\
%  \end{cases}
%  \intertext{ \textit{for some set of functions  $f_{1,2,3,4}$, constant $\hat{t}$, IO Cost/Page $io_{cost}$, page fetches $pg_{fetch}$, page size $pg_{size}$, iterations $iters$} } 
%  \textsc{Time}(q) &= \Sigma \textsc{T}(q)
%\end{align*}

Scan costs are estimated based on the number of tuples, 
page size, and a unit-less I/O cost per page. 
We assume that I/O costs dominate CPU costs for relational operators. 
A database implementation for which this assumption is not true, i.e. an in memory database, will need a different cost model.
Join costs are estimated as a function
of the two relations joined, any blocking operations, and repeated fetching of tuples (if required).
Filtering, projecting, and aggregating are all done in memory and have low computational overhead
so we assume negligible time costs for those operations. 

We extend these heuristics to 
account for the new operators: \textit{Drop} and \textit{Impute}. \textit{Drop} is a special case of a filter, so its time complexity is correspondingly negligible. The time computation for \textit{Impute} depends on the properties of the underlying
algorithm. \ProjectName{} treats the actual machine learning algorithm as a black box
and simply queries it for a time estimate. In the implementation of our system,
we use an iterative algorithm (see~\Cref{sec:imputation}), where the runtime cost
is a function of the number of attributes with no missing data, number of attributes with
missing data being imputed, number of tuples, and the number of iterations for the algorithm.

In general, evaluating the time complexity for different \textit{Impute} operators is
complicated, as the properties of
different algorithms can vary significantly, the underlying I/O cost
is not easily extracted (especially in the case that the entire set of tuples does not
fit in memory), and the CPU cost dominates (in contrast to the other operators, which do
not consider CPU cost explicitly). For example,~\cite{martin1995time} finds that the
time complexity of the \textit{build tree} algorithm for one commonly-used class of
decision trees is a function of the number of classes, several overhead constants, the
parameterization of the partition and heuristic functions, and the \textit{arity} (the
number of subsets considered for each split). The \textit{build tree} phase often
does not even dominate computation, as post-processing steps like pruning, which are vital
in achieving good performance, can have cost exponential in the height of the tree.
Nonetheless, in practice, we find that a simple parameterization can be acceptable (i.e.
yields intelligent query plans).

\subsection{Query planning}
\label{sec:planning}
The query planner must select a join ordering in addition to placing imputation operators as described in~\Cref{sec:placement}.

\subsubsection{Plan cache}
Before describing the operation of the query planner, we describe the semantics of a specialized data structure for holding sub-plans, called a \emph{plan cache}.
At a high level, a plan cache is a mapping from a set of tables to a set of dirty set-specific dominating plans over these tables.
We store multiple plans rather than a single plan per dirty set, because maintaining a Pareto frontier as a plan progresses through the optimizer's pipeline
allows us to obtain the final set of plans that best trade-off computation
cost and imputation quality. Details of the plan-cache semantics are shown in~\Cref{fig:semantics-plan-cache}.


\begin{figure}
  \begin{align*}
    \llbracket Q[T] \rrbracket &= \begin{cases}
      P & (T, P) \in Q \\
      \emptyset & \text{otherwise} \\
    \end{cases} \\
    \llbracket Q[T] \lhd p \rrbracket &= \begin{aligned} Q \cup \{(T,\ P') \} \end{aligned}\\
                                            \text{ where } \\
                                           P' &= \left\{q ~|~ \begin{aligned}
                                                                     q \in \llbracket Q[T] \rrbracket \cup \{p\}, \\
                                                                     \textsc{Dirty}(q) = \textsc{Dirty}(p),\\
                                                                     \forall q' \in \llbracket Q[T] \rrbracket.  \doms{q} {q'} 
                                                                   \end{aligned}
                                           \right\}
  \end{align*}
  \caption{Semantics of a plan cache $Q$, where $T$ is a set of tables and $p$ is a query plan}
  \label{fig:semantics-plan-cache}
\end{figure}

The plan cache uses the partial order of plans defined by  $\langle \textsc{Dirty}, \textsc{Penalty}, \textsc{Time} \rangle$ to
collect sound and complete Pareto frontiers. For plans with the same dirty set, we can treat $\langle \textsc{Penalty}, \textsc{Time} \rangle$
as a total order, allowing us to appropriately select the frontier. Plans with different $\textsc{Dirty}$ sets cannot be compared. The final
Pareto frontier produced by the optimizer corresponds to the collection of plans associated with the empty dirty set.

\subsubsection{Planning algorithm}
The planner (\Cref{algo:top-level-planner}) operates as follows.
First, it collects the set of attributes which are used by the operators in the plan or which are visible in the output of the plan (\Cref{lst:line:attr-start}--\Cref{lst:line:attr-end}).
This set will be used to determine which attributes are imputed.
Then, it constructs a plan cache. For each table used in the query, any available selections are pushed to the leaves and
a search over imputation operations produces various possible plans, which are added to the cache (\Cref{lst:line:sel}). If no selections are available, a 
simple scan is added to the plan cache (\Cref{lst:line:scan}). Join order is jointly optimized with imputation placements and a set of
plans encompassing all necessary tables is produced (\Cref{lst:line:join}). This set is now extended for any necessary grouping and aggregation
operations (\Cref{lst:line:group}). After a similar step has been taken for projections, the planner now contains the plan frontier (i.e.\ the
best possible set from which to pick our final plan). The final step in the planner is to find all plans that are
$\alpha$-bound and return the one that has the lowest runtime: the $\alpha$-bound optimal plan.

The join order and imputation optimization is based on the algorithm used in System R~\cite{blasgen1981system},
but rather than select the best plan at every point, we use our plan cache, which keeps track of the tables joined and the Pareto frontiers.
The output of the join optimizer is the set of plans which satisfy all join predicates and lie along their respective frontier.

\begin{algorithm}
\input{algorithms/plan-toplevel.tex}
\end{algorithm}

%\begin{algorithm}
%\input{algorithms/histogram-helpers.tex}
%\end{algorithm}

\begin{algorithm}
\input{algorithms/histogram-transform.tex}
\end{algorithm}

\subsection{Imputation Strategies}
\label{sec:imputation}

\ProjectName{} is designed so that any imputation strategy
can be plugged in with minimal effort and without changes to the optimizer, simply by adding
time and penalty functions (\Cref{sec:cost-model}) which describe the new algorithm.
For our purposes, an imputation strategy is a function $I(R, C) \rightarrow R'$ which fills in the missing
attributes $C$ in a relation $R$, returning a new relation $R'$.
The flexibility of this approach aligns with the spirit of query optimization, which aims to
reconcile performance with a declarative interface to data manipulation.

New imputation algorithms can also be added. As a reference implementation, \ProjectName{} uses a general-purpose imputation strategy
based on chained-equation decision trees.  Chained-equation
imputation methods~\cite{vanbuuren2011mice} (sometimes called \textit{iterative
regression}~\cite{gelman2006data}) impute multiple missing attributes by iteratively fitting
a predictive model of one missing attribute, conditional on all complete attributes
and all other missing attributes. Chained-equation decision trees algorithms are found to be
effective~\cite{akande2015empirical} and are widely used in epidemiological
domains~\cite{burgette2010multiple}. For our experiments, we use Weka's {\tt REPTree}
implementation~\cite{witten2016data}.

The reference algorithm used by the \textit{Impute} operator proceeds by iteratively fitting
decision trees for the target imputation attribute on a subset of the data. In each
iteration, the missing values of a separate attribute are replaced with newly imputed values.
In one {\it epoch} of the algorithm, each of the target imputation
attributes is filled in with imputed values in a single iteration.
With each epoch, the quality of the imputation improves as values progressively
reflect more accurate relationships between attributes. The algorithm terminates when
convergence is achieved (i.e.\ the imputed values do not change across epochs) or a fixed
number of epochs is reached. 

These imputation algorithms are commonly used as part of a larger \textit{multiple imputation}
process, in which multiple distinct copies of the complete data are generated and estimators
are computed by averaging over the multiple datasets to form a single completed dataset~
\cite{gelman2006data}. In \ProjectName{}, we consider only single imputation, though
multiple imputation could incorporated as another avenue of assessing imputation confidence.

%\begin{algorithm}
%    \input{algorithms/cart.tex}
%\end{algorithm}

%\subsection{Optimality}\label{sec:optimal}
%We prove the optimality of the final plan selected by showing that our cost-model and algorithm
%construct a complete Pareto frontier.
%
%\begin{definition}
%$I: \mathcal{P}(D) \rightarrow \mathcal{P}(\{ \delta, \mu \})$, where
%$D$ is the dirty set for a relation, is an abstract imputation operator mapping a set of
%attributes with missing values to a set of imputation operations.
%
%For convenience we denote applying the set of imputation operations to a plan $p$ as $I(p)$ and understand that this
%is shorthand for $\{ op(p)   | op \in I(\textsc{Dirty}(p)) \}$.
%\end{definition}
%
%We can succinctly describe possible imputation transformations to a plan at a given point in the planning algorithm solely through $I$.
%
%\begin{theorem}\label{theorem:no-bad-sit}
%By computing dirty-set specific Pareto frontiers, it is never the case that we discard a plan $p_1$ s.t. $\doms{p_2}{p_1}$, but 
%$\exists p_1' \in I(p_1), p_2' \in I(p_2). \doms{p_1'}{p_2'} \land \textsc{Dirty}(p_1') = \textsc{Dirty}(p_2') \land p_1' \not \in I(p_2)$
%\end{theorem}
%
%\Cref{theorem:no-bad-sit} states that there is no \textit{bad situation} such as that depicted in~\Cref{fig:bad-sit-diagram}
%
%\begin{figure}
%\centering
%\begin{tikzpicture}
%% before I
%\node(p1) {$p_1$};
%\node[right=of p1](dom1){$\prec$};
%\node[right=of dom1](p2) {$p_2$};
%
%% after I
%\node[below=of p1](p1'){$p_1'$};
%\node[right=of p1'](dom2) {$\succ$};
%\node[right=of dom2](p2'){$p_2'$};
%
%% arrows
%\draw[->] (p1) -- node[right] {$\scriptstyle I$} (p1');
%\draw[->] (p2) -- node[right] {$\scriptstyle I$} (p2');
%\end{tikzpicture}
%\caption{A bad Pareto frontier selection would choose $p_2$ over $p_1$, as it is locally optimal, but with the available transformations ($I$) $p_2$  can only reach $p_2'$, which is dominated by the inaccessible $p_1'$.~\Cref{theorem:no-bad-sit} says such a situation cannot arise when constructing Pareto frontiers on a dirty-set basis.}
%\label{fig:bad-sit-diagram}
%\end{figure}
%
%\begin{proof}
%For contradiction, let $p_1$ and $p_2$ be plans such that $\doms{p_2}{p_1}$, $p_1$ is removed from the plan cache
%in favor of $p_2$, but there  $\exists p_1' \in I(p_1), p_2' \in I(p_2). \doms{p_1'}{p_2'} \land \textsc{Dirty}(p_1') = \textsc{Dirty}(p_2')$
%and $p_1'$ is not reachable from $p_2$ by $I$.
%
%There are two possible cases :
%
%\begin{case}
%$\textsc{Dirty}(p_1) = \textsc{Dirty}(p_2)$, in which case $I(p_1) = I(p_2)$, as $I$ uniquely determines the possible transformations. This makes $p_2$ and $p_1$ substitutable, so $p_1'$ is reachable from $p_2$. This is a contradiction.
%\end{case}
%
%\begin{case}
%$\textsc{Dirty}(p_1) \not= \textsc{Dirty}(p_2)$, in which case $p_1$ is not removed from the plan cache as the $\doms{p_2}{p_1}$ relation does not occur within the same dirty set. This is a contradiction
%\end{case}
%
%So it must be the case that no such situation arises.
%\end{proof}
%
%
%Given that no desirable plans are discarded by computing Pareto frontiers for each dirty set, the final Pareto frontier (for dirty set $\emptyset$) is optimal.

\subsection{Complexity}\label{sec:complexity}
Our optimization algorithm builds off the approach taken by 
a canonical cost-based optimizer~\cite{blasgen1981system}.
If imputation operators are ignored, we search the same plan space.
Therefore, our algorithm has a complexity of at least $O(2^J)$, where $J$ is the number of joins.

The addition of imputation operators increases the number of plans exponentially, as an imputation may be placed before any of the relational operators.
We restrict imputations to two classes: those that impute only attributes used in the operator and those that impute all the attributes that are needed downstream in the query.
By doing so we limit the number of imputations at any point to four: \textit{Drop} or \textit{Impute} over the attributes used in the operator or over all the downstream attributes.
This modification increases the complexity of our planning algorithm to $O(2^{4J})$.

To motivate the restriction of imputation types, we consider the implications of allowing arbitrary imputations.
If we allow any arbitrary subset of attributes to be imputed, then we would need to consider $O(2^{|D|+1})$ different imputation operators before each relational operator where $D$ is the set of dirty attributes in all tables.
This would increase the overall complexity of the algorithm drastically.

Finally, we note that for the queries we have examined, this exponential blowup does not affect the practical performance of our optimizer.
Recall that the planner maintains different plans for different dirty sets, keeping only those plans that are not dominated by others.
So in many cases we can drop some of the intermediate plans generated at each operator.
The worst case complexity only occurs if the dirty sets tracked are distinct through the entire planning phase.

In order to support a large number of joins, with reasonable planning times, \ProjectName{} can maintain approximations of the Pareto sets,
rather than exact sets. To do so, dominated plans are still removed based on new non-dominated plans, but new plans are only added
if they are significantly different from any existing plan in the set. The distance between plans is determined based on the cost penalty and time
values associated with a plan.  Planning times are discussed further in \Cref{sec:results}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
