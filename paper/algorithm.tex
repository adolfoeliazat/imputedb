\section{Algorithm}

At a high level, our goal is to select a query plan that minimizes a metric which combines the cost of the query and the quality of the results.
Precisely, we will find a query plan $Q$ that minimizes $\text{Cost}(Q) = \alpha \times \text{Time}(Q) + (1 - \alpha) \times \text{Loss}(Q)$. $\alpha$ is a parameter to the query optimizer that controls the emphasis on quality versus performance. $\alpha = 1.0$ means that the query should be as fast as possible, $\alpha=0.0$ means that the query should be as accurate as possible.

\subsection{Search space}
To reduce the size of the query plan search space, only plans that fit the following template are considered. First, all filters are pushed to the leaves of the query tree, immediately after the scans. Joins are performed after filtering, and only left-deep plans are considered. Any group-by/aggregate will be performed after the final join. Finally, projections are placed at the root of the query tree. The space of query plans is similar to that considered by Selinger \todo{Selinger cite}, with the addition of imputation operators.

\subsection{Imputation operators}
We introduce two new relational operators to perform imputation: impute ($\mu$) and drop ($\delta$). Each operator takes arguments $(C, R)$ where $C$ is a set of attributes and $R$ is a relation. Impute uses a machine learning method to replace all null values with non-null values for attributes in $C$ in the relation $R$. Drop simply removes all tuples which have a null value for some attribute in $C$ from $R$. Both operators guarantee that the resulting relation will contain no null values for attributes in $C$. 

\subsection{Imputation placement}
Imputation operators must be placed so that no relational operator receives a tuple containing null in an attribute that the operator examines, regardless of the state of the data in the base tables.

Imputation operators can be placed at any point in the query plan, but to meet the guarantee that no non-imputation operator sees a null value, there are cases where an imputation operator is required. To track these cases, each query plan is associated with a set of dirty attributes $D$. An attribute $c$ is \emph{dirty} in some relation if the values for $c$ contain null. We compute a dirty set for each base table using the table statistics, which track the number of null values in each column. If we apply an imputation operator to a relation $R$ with a dirty vector $D$, $\mu_C (R)$ or $\delta_C (R)$, the resulting query has a dirty vector $D' = D \ C$. Projections produce a new dirty vector by dropping each attribute that is not in the projection. A join R⨝_ψ▒R^′  ⁆ with dirty vectors D⁆ and D^′  produces a dirty vector D OR D^′⁆.  Filters do not change the dirty vector. [MS It's possible that a filter φ_t   removes all of the rows that had null-values in some column t` != t, thus removing t` from the dirty vector.] 

The dirty set over-approximates the set of attributes that contain null. For example, a filter might remove all tuples which contain null without changing the dirty set, forcing an unnecessary imputation. We choose to over-approximate the dirty set to avoid the possibility of dropping a tuple that contains a null value without explicitly imputing the value or applying a drop operator.

\subsection{Query planning}
The input to our query planner is a tuple $(T, \Phi, \Psi, P, G, A)$: a set of tables $T$, a set of filter predicates $\phi_t, t \in T$, a set of join predicates $\psi_(t_1, t_2), t_1, t_2 \in T$, a set of attributes $P$, and an optional set of attributes $G$ and aggregator function $A \in \{\text{Max}, \text{Min}, \text{Sum}, \text{Avg}, \text{Count}\}$.

To reduce the search space, we only consider the minimal imputation, the maximal imputation, and the minimal drop. The minimal imputation (resp. drop) only imputes (resp. drops) the columns required by the relational operator immediately following the imputation. The maximal imputation imputes all columns in the relation, regardless of which are required.

# For a plan and a set of necessary attributes, add the allowed amount of imputation.
AddImpute(q, attrs):
	Dattr = Dirty(q) intersect attrs
	if Dattr = empty:
		return {q}
	else:
		return {impute_all(q), impute_min(q, Dattr), drop_min(q, Dattr)}
		
# For a set of plans with different dirty sets, return the best plan for each dirty set.
OptRel(Q):
	D = {Dirty(q) | q in Q}
	return { argmin_q Cost(q) s.t. Dirty(q) = d | d in D}

# Given a table and a filter predicate, return the optimal plans.
OptFilter(t⁆,φ⁆):
	return OptRel(AddImpute(t, Attrs(φ)))
		
# For a set of tables (with filter predicates) and join predicates, return the optimal plans.
OptJoin(T⁆, join_preds):
	Q = empty
	for t, pred in T:
		S = T \ {t, pred}
		# Get the optimal plans for the left and right arguments to the join.
		for r_l, r_r in OptJoin(S) x OptFilter(t, pred):
			if r_l, r_r in join_preds:
				# Add imputation to the left and right arguments.
				Q = Q union OptRel(Join(r_l', r_r') for r_l' in AddImpute(r_l, Attrs(join_pred)), r_r' in AddImpute(r_r, Attrs(join_pred)))
	return OptRel(Q)
# Extended cost calculation to account for information loss/computation cost in imputations
Cost(q):
  case drop_min(q', d) =>  info_loss(drop, q', attrs(q') intersect d) + Cost(q')
  case impute_all(q') => info_loss(impute, q', attr(q')) + impute_cost(q') + Cost(q')
  case impute_min(q', d) => info_loss(impute, q', attrs(q') intersect d) + impute_cost(q'[d]) + Cost(q')
  case _ => normal cost calculation

  
info_loss(op, t, a):
    # beta: some scaling factor
    case info_loss(drop, t, a) => beta * sum(for ct_missing(t.a) in a)
    case info_loss(impute, t, a) => beta *sum(for ct_missing(t.a) / ct_complete(t) in a)

impute_cost(t):
  # alpha: some scaling factor
  alpha * (ntuples(t) + ct(attr(t)))

\subsection{Histogram computation}
Histogram adjustment algorithm:
Invariant: the distribution of non-null values is preserved
Drop(t, ix) => remove nulls from t table statistics for indices ix
Impute(t, ix) => redistribute nulls from t table statistics for indices ix to other buckets based on existing distribution
Filter(t, predicate) => scale all buckets of t table statistics and null counts by predicate selectivity
Join(t1, t2) => estimate cardinality of join result (following selinger, if non-equality, then 30% of product,
If equality and no primary/foreign key relationship then the maximum of the two cardinalities, otherwise
If equality and primary/foreign key relationship then the size ofthe foreign key relationship table). Scale t1's statistics and null counts to total the new cardinality estimate. Do same for t2. Union histograms.

\subsection{Complexity}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
