\section{Algorithm}

Our optimizer searches a restricted space (Sec.~\ref{sec:search-space}) of query plans containing imputation operators (Sec.~\ref{sec:operators}) for a plan that minimizes a metric (Sec.~\ref{sec:cost-model}) which combines the runtime cost of the query and the estimated quality of the results.
This plan is subject to two constraints (Sec.~\ref{sec:placement}).
First, it must not emit any tuples which contain null values, regardless of the state of the base tables.
Second, the traditional relational operators (selection $\sigma$, projection $\pi$, join $\bowtie$, and group-by/aggregate $g$) must never observe a null value in any attribute that they operate on directly.
Our planning algorithm is agnostic to the type of imputation used (Sec.~\ref{sec:imputation}), as it is treated as a black-box operation.

\subsection{Imputation operators}
\label{sec:operators}
We introduce two new relational operators to perform imputation: \textit{Impute} ($\mu$) and
\textit{Drop} ($\delta$). Each operator takes arguments $(C, R)$ where $C$ is a set of
attributes and $R$ is a relation. \textit{Impute} uses a machine learning algorithm to
replace all null values with non-null values for attributes in $C$ in the relation $R$.
The imputation algorithm is discussed in detail in Sec.~\ref{sec:imputation}.
\textit{Drop} simply removes from $R$ all tuples which have a null value for some attribute in $C$.
Both operators guarantee that the resulting relation will contain no null values for
attributes in $C$.  

\subsection{Search space}
\label{sec:search-space}
To keep the query plan search space tractable, only plans that fit the following template are considered.

\begin{itemize}
\item All filters are pushed to the leaves of the query tree, immediately after scanning a table. We use a pre-processing step which conjoins together filters which operate on the same table, so we can assume that each table has at most one relevant filter.

\item Joins are performed after filtering, and only left-deep plans are considered.

\item A grouping \& aggregation operator is optional, and if present will be performed after the final join.

\item Projections are placed at the root of the query tree.
\end{itemize}
  
The space of query plans is similar to that considered by System R~\cite{blasgen1981system}, with the addition of imputation operators appearing before and after traditional operators.
Figure~\ref{fig:query-schematic} shows a plan schematic for a query involving three tables, absent any imputation operators.

\begin{figure}
\Tree [.$\pi$ [.$g$ [.$\bowtie$ [.$\bowtie$ [.$\sigma$ $t_1$ ] [.$\sigma$ $t_2$ ] ] [.$\sigma$ $t_3$ ] ] ] ]
\caption{Query plan schematic for the type of traditional plans explored (absent imputation operators).}
\label{fig:query-schematic}
\end{figure}

\subsection{Cost model}
\label{sec:cost-model}
The cost for a plan $q$ is expressed as a tuple of $\langle \textsc{Loss}(q), \textsc{Time}(q) \rangle$.
$\textsc{Time}(q)$ is an estimate of the runtime of a query $q$ derived from table statistics and selectivity estimation of the query predicates. $\textsc{Loss}(q)$ is an estimate of the amount of error introduced by a sequence of imputation procedure. Our
cost model averages over the losses from each such operator. We note that our loss calculation constrains $\textsc{Loss}(q)$ to $[0, 1]$, while $\textsc{Time}(q)$ can range within $(0, \infty)$.

\begin{definition}
We say that a plan $p$ with cost $\langle l, t \rangle$ dominates a plan $p'$ with cost $\langle l', t' \rangle$, if $l < l' \land t < t'$, and denote this as $\doms{p}{p'}$. 
\end{definition}

Given a query, \ProjectName{} produces a final set of possible plans $P$ by only maintaining plans that are not dominated by others in the search space. So if
our search space is $S$

\begin{gather*}
 P= \{ p | \not\exists p' \in S. \doms{p'}{p} \}
 \end{gather*}
 
This set outlines the best possible options for trading performance and cost for the current query, building a Pareto frontier\cite{pareto1964cours}.

In order to pick a final plan from the frontier, our model introduces a parameter $\alpha$ which encodes an upper bound on the amount of information loss that a user is willing to tolerate
relative to the minimum information loss possible given the imputation operations.


\begin{definition}
Given a final set of plans $P$, and $p \in P$, we say $p$ is $\alpha$-bound, if $(\textsc{Loss}(p) - \min_{p' \in P}(\textsc{Loss}(p')) \leq \alpha$. We denote this as $p^\alpha$.
We say $p$ is $\alpha$-bound optimal if $\forall p'^\alpha \in P: \textsc{Time}(p) \leq \textsc{Time}(p')$. 
\end{definition}

Given $\alpha$ and $P$, \ProjectName{} returns the $\alpha$-bound optimal plan in $P$. Note that the $\alpha$-bound optimal plan is unique in $P$, given that
$P$ does not keep any plans dominated by others in the set.

So in essence, $\alpha$ is a tunable parameter that determines whether the optimizer focuses on quality or on performance, as plans that
lose a lot of information through dropping are also the fastest. 

$\alpha = 1.0$ means that the optimal query should have the highest performance (at the expense of quality), while $\alpha=0.0$ means that the optimal query should lose
as little information as possible (at the expense of performance).

The computation of $\textsc{Time}(q)$ and $\textsc{Loss}(q)$ relies on having an accurate cardinality estimate of $q$ and its sub-queries.
These cardinality estimates are impacted not just by filtering or joining, as in the traditional relational calculus, but also by the imputation operators.
For example, a drop operator will reduce the cardinality of the result while an impute operator will maintain the same cardinality as the input.
For simplicity, each of the logical nodes in a query plan points to a set of histograms.
When the optimizer creates a new query plan, it copies the histograms of the sub-plans and modifies them as necessary to account for new operation in the plan.
Algorithm~\ref{algo:histogram-transformation} describes the process of generating new histograms from sub-plans.

For each sub-query, we keep track of the estimated number of
null values in a column  ($\textsc{Missing}(c, Q)$) by using the histograms associated with
the sub-query. We use this estimate to compute the set $\text{Loss}(Q)$ as follows.

\begin{align*}
  \textsc{Loss}(q) = \begin{cases}
    \{1\} \cup \textsc{Loss}(q') & q = \delta_C(q') \\
    \{\frac{1}{\sqrt{ \textsc{Attr}(q') * \textsc{Cardinality}(q') }} \} \cup \textsc{Loss}(q')  & q = \mu_C(q') \\
    \textsc{Loss}(q_1') \cup \textsc{Loss}(q_2') & q = q_1' \bowtie_\psi q_2' \\
    \textsc{Loss}(q') & q = \sigma_\phi(q'), \\ & q = \pi_C(q') \\
  \end{cases}
\end{align*}

Using the drop operator incurs a loss of information of 1 (i.e. we lose all information that
the dropped tuples might have encoded). Imputing null fields incurs a loss penalty $p \in (0, 1]$,
which is inversely proportional to the number of total cells available to train the imputation algorithm. 
Total cells are defined as the product of the number of attributes and the number of tuples.
Intuitively, imputation
accuracy \textit{increases} when more complete attributes and complete tuples are available;
correspondingly, the information loss \textit{decreases} with more values to work
with. Since we conceive that
alternate imputation strategies could be used, the analyst may provide a corresponding
loss with a reasonable parameterization.

To compute $\text{Time}(Q)$, we retain a simple set of heuristics used by a standard 
database. Scan costs are estimated based on the number of tuples, 
page size, and an unit-less IO cost per page. Join costs are estimated as a function
of the two relations joined, any blocking operations, and repeated fetching of tuples (if required).
Filtering, projecting, and aggregating are all done in memory and have low computational overhead
so we assume negligible time costs for those operations. We extend these heuristics to 
account for the new operators: \textit{Drop} and \textit{Impute}. \textit{Drop} is a special case of a sequential scan, and its time
complexity can be expressed as a function of the number of heap pages read and the IO cost
per page.

The time computation for \textit{Impute} depends on the properties of the underlying
algorithm. \ProjectName{} treats the actual machine learning algorithm as a black box
and simply queries it for a time estimate. In the base implementation of our system,
we use an iterative algorithm (see Sec.~\ref{sec:imputation}), where the runtime cost
is a function of the number of attributes with no missing data, number of attributes with
missing data being imputed, number of tuples, and the number of iterations for the algorithm.

In order to avoid having the time component of our cost model dominate the information loss 
component, we scale the former by an arbitrary constant.

In general, evaluating the time complexity for different \textit{Impute} operators is
complicated, as the properties of
different algorithms can vary significantly, the underlying IO cost
is not easily extracted (especially in the case that the entire stream of tuples does not
fit in memory), and the CPU cost dominates (in contrast to the other operators, which may
not even consider CPU cost explicitly). For example, \cite{martin1995time} find that the
time complexity of the \textit{build tree} algorithm for one commonly-used class of
decision trees is a function of the number of classes, several overhead constants, the
parameterization of the partition and heuristic functions, and the \textit{arity} (the
number of subsets considered for each split). Indeed, the \textit{build tree} phase often
does not even dominate computation, as post-processing steps like pruning, which are vital
in achieving good performance, can have cost exponential in the height of the tree. In
practice, we find that a simple parameterization can be acceptable (i.e. yield intelligent query
plans).

\subsection{Imputation placement}
\label{sec:placement}
Imputation operators must be placed so that no relational operator receives a tuple containing null in an attribute that the operator examines, regardless of the state of the data in the base tables.

Imputation operators can be placed at any point in the query plan, but to meet the guarantee that no non-imputation operator sees a null value, there are cases where an imputation operator is required. To track these cases, each query plan $q$ is associated with a set of dirty attributes $\textsc{Dirty}(q)$. An attribute $c$ is \emph{dirty} in some relation if the values for $c$ contain null. We compute a dirty set for each table using the table statistics, which track the number of null values in each column. The dirty set for a query plan can be computed recursively as follows:

\begin{align*}
  \textsc{Dirty}(q) = \begin{cases}
    \textsc{Dirty}(q') \setminus C & q = \delta_C(q')\ \text{or}\ q = \mu_C(q') \\
    \textsc{Dirty}(q') \cap C & q = \pi_C(q') \\
    \textsc{Dirty}(q_l) \cup \textsc{Dirty}(q_r) & q = q_l \bowtie_\psi q_r \\
    \textsc{Dirty}(q') & q = \sigma_\phi(q') \\
    \textsc{Dirty}(t) & q = \text{some table}\ t \\
  \end{cases}
\end{align*}

Note that \textsc{Dirty} over-approximates the set of attributes that contain null. For example, a filter might remove all tuples which contain null, but the dirty set would be unchanged. We choose to over-approximate to ensure that all null values are explicitly imputed or dropped.

\subsection{Query planning}
The input to our query planner is a tuple $(T, F, J, P, G, A)$:
\begin{itemize}
\item A set of tables $T$.
\item A relation $F: T \times \Phi$ between tables and filter predicates.
\item A relation $J : T \times \Psi \times T$ between tables and join predicates.
\item A set of projection attributes $P$.
\item An optional set of grouping attributes $G$ and aggregator function $A$.
\end{itemize}

The query planner must select a join ordering in addition to placing imputation operators as described in Section~\ref{sec:placement}.

Before describing the operation of the query planner, we describe the semantics of a specialized data structure for holding sub-plans, called a \emph{plan cache}.
At a high level, a plan cache is a mapping from a set of tables to a set of dominating plans that perform the required joins and filters over these tables.
We store multiple plans rather than a single plan because each plan has a distinct set of dirty attributes and a choice over the plan frontier
can only be made once the set is final. Details are shown in Figure~\ref{fig:semantics-plan-cache}.

\begin{figure}
  \begin{align*}
    \llbracket Q[T] \rrbracket &= \begin{cases}
      P & (T, P) \in Q \\
      \emptyset & \text{otherwise} \\
    \end{cases} \\
    \llbracket Q[T] \lhd p \rrbracket &= \begin{aligned} Q \cup \{(T,\ P') \} \end{aligned}\\
                                            \text{ where } \\
                                           P' &= \left\{q ~|~ \begin{aligned}
                                                                     q \in \llbracket Q[T] \rrbracket \cup \{p\}, \\
                                                                     \textsc{Dirty}(q) = \textsc{Dirty}(p),\\
                                                                     \forall q' \in \llbracket Q[T] \rrbracket.  \doms{q} {q'} 
                                                                   \end{aligned}
                                           \right\}
  \end{align*}
  \caption{Semantics of the plan cache.}
  \label{fig:semantics-plan-cache}
\end{figure}

To reduce the search space, we only consider the minimal imputation, the maximal imputation, and the minimal drop. The minimal imputation (resp. drop) only imputes (resp. drops) the columns required by the relational operator immediately following the imputation. The maximal imputation imputes all of the attributes in the relation which are used by other operators in the query plan or are visible in the query output.

Algorithm~\ref{algo:plan-helpers} presents a series of helper functions used by the top-level planner, shown in Algorithm~\ref{algo:top-level-planner}.

The planner operates as follows.
First, it collects the set of attributes which are used by the operators in the plan or which are visible in the output of the plan.
This set will be used to determine which attributes are imputed.
Then, it constructs a plan cache. For each table used in the query, any available selections are pushed to the leaves and
a search over imputation operations produces various possible plans, which are added to the cache. If no selections are available, a 
simple scan is added to the plan cache. Join order is jointly optimized with imputation placements and a set of
plans encompassing all necessary tables is produced. This set is now extended for any necessary grouping and aggregation
operations. After a similar step has been taken for projections, the planner returns the cheapest plan from the cache.

\begin{algorithm}
\input{algorithms/plan-helpers.tex}
\end{algorithm}

\begin{algorithm}
\input{algorithms/plan-toplevel.tex}
\end{algorithm}

%\begin{algorithm}
%\input{algorithms/histogram-helpers.tex}
%\end{algorithm}

\begin{algorithm}
\input{algorithms/histogram-transform.tex}
\end{algorithm}

\subsection{Imputation Strategies}
\label{sec:imputation}

We design our system such that any imputation strategy can be plugged in with minimal effort
and without changes to the optimizer, so the strategy can in principle be targeted to a
specific domain. The current version of the system uses a general-purpose imputation
strategy based on chained-equation classification and regression trees (CE-CART). Chained
equation imputation methods \cite{vanbuuren2011mice} (sometimes called \textit{iterative
regression} \cite{gelman2006data}) impute multiple missing attributes by
iteratively estimating predictive models of one missing attribute conditional on all
complete attributes and the other missing attributes. The individual predictive models can
be customized by the analyst to the problem at hand. Decision tree algorithms, like CART,
are found to be effective \cite{akande2015empirical} in empirical studies in general-purpose
routines and are widely used in epidemiological domains \cite{burgette2010multiple}. Though
we provide quantitative results for the CE-CART implementation, ImputeDB is designed to be
agnostic to the choice of imputation algorithm, and additional algorithms --- with
associated time and loss functions --- could be inserted without any change in design.

These imputation algorithms are rarely used in isolation --- rather, the ultimate goal is to
perform statistical analysis of the complete data. To this end, the technique of
\textit{multiple imputation} can be used, in which multiple distinct copies of the complete
data are generated and estimators are computed by averaging over the multiple datasets. In
ImputeDB, we cannot assume the user desires multiple copies of the query result, though we
note that in further work, multiple imputation could be used within ImputeDB to reduce the
variance of estimates of some aggregates, like averages.

Our algorithm proceeds by iteratively fitting regression trees to a subset of the data and
the target imputation columns. In each iteration, the missing values of a column are
replaced with newly imputed values. With each epoch, the quality of the imputation improves
as values progressively reflect more accurate relationships amongst attributes. The
algorithm terminates when convergence is achieved (i.e. the imputed values do not change across
epochs) or a fixed number of epochs is reached.  Algorithm~\ref{algo:imputation-strategy}
provides details on the implementation.  

The flexibility of this non-parametric imputation approach aligns well with the spirit of
query optimization, which aims to reconcile performance with a declarative interface to data
manipulation. By removing concerns for imputation, users executing queries in \ProjectName{}
don't need to consider the potential nature of missing data.

It is worth noting that given the nature of the imputation strategy used here, imputation
operators become blocking in our system. An extension of ImputeDB could consider regression
algorithms that learn in an online nature; this would reduce the cost of the imputation at
the possible expense of quality. Again, given the imputation strategy-agnostic design of
ImputeDB, this would not pose a problem.

\begin{algorithm}
    \input{algorithms/cart.tex}
\end{algorithm}

\subsection{Complexity}
\todo{this section needs to be cleaned up/clarified}
Our optimization algorithm builds off the approach taken by System R \cite{blasgen1981system}, therefore our algorithm still operates in exponential time. Indeed, 
note that if we remove our restriction on types of imputation (i.e. minimal vs maximal), and allow any arbitrary subset of attributes to be imputed,
then every single operator in the query plan has a number of imputations exponential in the number of dirty columns. Our restriction, instead, increases the number
of plans (in the worst case) at each operator by a factor of 3. Of course, this implies that in the worst case (where all dirty sets tracked are distinct throughout the query plan),
we explore a number of plans that further scales the number of plans in the original algorithm by an exponential factor.

However, we note that in all cases we have considered, this exponential blowup does not affect the practical performance of our optimizer.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
