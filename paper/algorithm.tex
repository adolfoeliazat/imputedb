\section{Algorithm}
In order to correctly plan around missing values, we first extend the set of relational algebra operators (selection $\sigma$, projection $\pi$, join $\bowtie$, and group-by/aggregate $g$) 
with two new operators (\Cref{sec:operators}). We then define the search space (\Cref{sec:search-space}) of plans to encompass non-trivial operations, such as joins and 
aggregations. Soundness of query execution is provided by our main design invariant, which guarantees traditional relational
algebra operators must never observe a missing value in any attribute that they operate on directly (\Cref{sec:placement}). 
The decision to place imputation operators is driven by our cost model (\Cref{sec:cost-model}), which characterizes a plan based on estimates for the 
quality of the imputations performed and the runtime performance. Our imputation planning algorithm is
agnostic to the type of imputation used (\Cref{sec:imputation}). Finally, we show
that while our algorithm is exponential in the number of joins (\Cref{sec:complexity}), in practice, planning times are not a concern for most queries.


\subsection{Imputation Operators}
\label{sec:operators}
We introduce two new relational operators to perform imputation: \textit{Impute} ($\mu$) and
\textit{Drop} ($\delta$). Each operator takes arguments $(C, R)$ where $C$ is a set of
attributes and $R$ is a relation. \textit{Impute} uses a machine learning algorithm to
replace all \nullv{} values with non-\nullv{} values for attributes $C$ in the relation $R$ (discussed in detail in~\Cref{sec:imputation}).
% In our experiments, we implement \textit{Impute} using a general-purpose chained-equation decision trees algorithm, but our system is agnostic to this choice.
\textit{Drop} simply removes from $R$ all tuples which have a \nullv{} value for some attribute in $C$.
Both operators guarantee that the resulting relation will contain no \nullv{} values for
attributes in $C$.  

\subsection{Search Space}
\label{sec:search-space}
To keep the query plan search space tractable, only plans that fit the following template are considered.

\begin{itemize}
\item All selections are pushed to the leaves of the query tree, immediately after scanning a table. We use a pre-processing step which conjoins together filters which operate on the same table, so we can assume that each table has at most one relevant filter.

\item Joins are performed after filtering, and only left-deep plans are considered.
  This significantly reduces the plan space while still allowing plans with interesting join patterns.

\item A grouping and aggregation operator is optional and if present will be performed after the final join.

\item Projections are placed at the root of the query tree.
\end{itemize}
  
The space of query plans is similar to that considered in a canonical
cost-based optimizer~\cite{blasgen1981system}, with the addition of imputation operators appearing before and after traditional operators.
\Cref{fig:query-schematic} shows a plan schematic for a query involving three tables, absent any imputation operators.

\begin{figure}
  \begin{minipage}[c]{0.5\columnwidth}
    \Tree [.$\pi$ [.$g$ [.$\bowtie$ [.$\bowtie$ [.$\sigma$ $t_1$ ] [.$\sigma$ $t_2$ ] ] [.$\sigma$ $t_3$ ] ] ] ]
  \end{minipage}\begin{minipage}[c]{0.5\columnwidth}
    \caption{
       Query plan schematic for the type of traditional plans explored (absent imputation operators).
    } \label{fig:query-schematic}
  \end{minipage}
\end{figure}

\subsection{Imputation Placement}
\label{sec:placement}
We place imputation operators into the query plan so that no relational operator encounters a tuple containing \nullv{} in an attribute that the operator examines, regardless of the state of the data in the base tables.

Imputation operators can be placed at any point, but there are cases where an imputation operator is required to meet the guarantee that no non-imputation operator sees a \nullv{} value. To track these cases, we associate each query plan $q$ with a set of dirty attributes $\textsc{Dirty}(q)$. An attribute $c$ is \emph{dirty} in some relation if the values for $c$ contain \nullv{}. We compute a dirty set for each table using the table histograms, which track the number of \nullv{} values in each column. The dirty set for a query plan can be computed recursively as follows:

\begin{align*}
  \textsc{Dirty}(q) = \begin{cases}
    \textsc{Dirty}(q') \setminus C & q = \delta_C(q')\ \text{or}\ q = \mu_C(q') \\
    \textsc{Dirty}(q') \cap C & q = \pi_C(q') \\
    \textsc{Dirty}(q_l) \cup \textsc{Dirty}(q_r) & q = q_l \bowtie_\psi q_r \\
    \textsc{Dirty}(q') & q = \sigma_\phi(q') \\
    \textsc{Dirty}(t) & q = \text{some table}\ t \\
  \end{cases}
\end{align*}

Note that \textsc{Dirty} over-approximates the set of attributes that contain \nullv{}. For example, a filter might remove all tuples which contain \nullv{}, but the dirty set would be unchanged. We choose to over-approximate to ensure that all \nullv{} values are explicitly imputed or dropped.


\subsection{Cost Model}
\label{sec:cost-model}
The cost of a plan $q$ is a tuple $\langle \textsc{Penalty}(q), \textsc{Time}(q) \rangle$.
$\textsc{Penalty}(q) \in (0,1]$ is a heuristic that quantifies the amount of information lost by the imputation procedure used.
$\textsc{Time}(q) \in (0, \infty)$ is an estimate of the runtime of a query $q$ derived from table statistics, selectivity estimation of the query predicates, and the complexity of the
impute operation. 

\subsubsection{Pareto-optimal plans}
Given a query, \ProjectName{} produces a final set of query plans $Q$ by only keeping plans that are not dominated by others in the search space. So for a search space $S$, \[Q= \{ q ~|~ \not\exists q' \in S.\ q \neq q' \land \doms{q'}{q} \}.\]

\begin{definition}
We say that a plan $q$ with cost $\langle l, t \rangle$ \emph{dominates} a plan $q'$ with cost $\langle l', t' \rangle$ if $\textsc{Dirty}(q) = \textsc{Dirty}(q') \land ((l \leq l' \land t < t') \lor (l < l' \land t \leq t'))$. We denote this as $\doms{q}{q'}$. 
\end{definition}

This set $Q$ will be the Pareto frontier~\cite{pareto1964cours} of $S$, and it contains the best option for all feasible trade-offs of $\textsc{Time}$ and $\textsc{Penalty}$ for the current query. This
set of plans is significantly smaller than the set of all plans, which our algorithm prunes away.

In order to pick a final plan from the frontier, our model introduces a trade-off parameter $\alpha$, which is an upper bound on the $\textsc{Penalty}$ value that the user is willing
to tolerate relative to the minimum possible among the frontier plans.

\begin{definition}
Let $Q$ be a set of plans. For $q \in Q$, we say $q$ is $\alpha$-bound, if $(\textsc{Penalty}(q) - \min_{q' \in Q}(\textsc{Penalty}(q')) \leq \alpha$. We denote this as $q^\alpha$.
We define the set of $\alpha$-bound plans in $Q$ as $Q^\alpha$.
We say a plan $q^\alpha$ is $\alpha$-bound optimal if $\forall q' \in Q^\alpha. \textsc{Time}(q) \leq \textsc{Time}(q')$. 
\end{definition}

Given $\alpha$ and $Q$, \ProjectName{} returns an $\alpha$-bound optimal plan in $Q$. In essence, $\alpha$ is a tunable parameter 
that determines whether the optimizer focuses on quality or on performance, as plans that
lose a lot of information through dropping are also the fastest. 
If $\alpha=0.0$, then the optimal query should lose as little information as possible (at
the expense of performance). If $\alpha = 1.0$, then the optimal query should have the
lowest runtime (at the expense of quality).

\review{
\subsubsection{Cardinality estimation}
\label{sec:cardinal}
The computation of $\textsc{Penalty}(q)$ and $\textsc{Time}(q)$ relies on accurate cardinality estimates for the plan $q$ and its sub-plans.
These estimates are impacted not just by filtering or joining, as in the traditional relational algebra, but also by the imputation operators.

To compute cardinality, we maintain histograms for each column in the database.
These histograms track the distribution of values in the column as well as the number of missing values.
During query planning, each of the logical nodes in a query plan points to a set of histograms which describe the distribution of values in the output of the node.
When the optimizer creates a new query plan, it copies the histograms of the sub-plans and modifies them as described in~\Cref{algo:histogram-transformation} to account for the new operation in the plan.

Drop operators (\Crefrange{lst:drop-start}{lst:drop-end}) reduce the cardinality of their input relation by removing tuples which contain missing values.
For a drop $\delta_C$, for each column $c \in C$ we set its missing value count to zero, because all of these tuples will be dropped.
This can result in a set of histograms which have different cardinalities if some columns have more missing values than others.
To account for the discrepancy, we rescale all the histograms without changing the shape of their distributions to match the lowest cardinality column.

Impute operators (\Crefrange{lst:impute-start}{lst:impute-end}) maintain the cardinality of their inputs, but they also fill in missing values.
For an impute $\mu_C$, we set the missing value count to zero, and rescale each histogram so that its new cardinality, with no missing values, is the same as its old cardinality.
We assume that the imputed values will not change the shape of the distribution of values in each column.

Cardinality estimation for standard relational operators (\Crefrange{lst:std-start}{lst:std-end}) is the same as in a normal database.
\begin{algorithm}
\input{algorithms/histogram-transform.tex}
\end{algorithm}

\subsubsection{Imputation quality} 
\label{sec:quality}
Every instantiation of the impute-operator defines a custom penalty function $P$ for imputing \nullv{} fields.
\ProjectName{} estimates the aggregate quality of the imputations in a plan $q$ using the heuristic measure $\text{Penalty}_{P}(q)$, computed as follows.

\begin{align*}
    &\textsc{L}_{P}(q) = \begin{cases}
     1 + \textsc{L}(q') & q = \delta_C(q') \\
     P(q') + \textsc{L}(q') & q = \mu_C(q') \\
     \textsc{L}(q_l') + \textsc{L}(q_r') & q = q_l' \bowtie_\psi q_r' \\
     \textsc{L}(q') & q = \sigma_\phi(q'),\ q = \pi_C(q') \\
     0 & q = \text{some table } t\\
  \end{cases}\\
    &\textsc{Penalty}_{P}(q) = \frac{\textsc{L}_{P}(q)}{\textsc{L}_{P_\delta}(q)}
\end{align*}

$\textsc{Penalty}_{P}(q)$ aggregates penalties from individual imputations to produce a measure of the quality of imputation for the entire plan.
For imputation on a subplan $q'$, a penalty $P(q')$ in $(0,1]$ is assigned to $\mu(q')$
and a penalty of $1$ is assigned to $\delta(q')$.
The function $\textsc{L}_{P}(q')$ is then defined recursively to aggregate these penalties for the entire plan. 
The result is normalized by $\textsc{L}_{P_\delta}(q')$, which is the total penalty for always dropping tuples with \nullv{} values and is equal to the number of imputation operators in the plan.

\textbf{Constructing $P$}:
The form of $P$ is chosen to reflect the properties of the imputation method being used.
A well-chosen $P$ should provide a reasonable estimate of the quality of imputation from the set of plan histograms.
From these, $P$ can obtain the number of clean and dirty attributes in its input, their cardinalities, and approximations of distribution statistics such as mean and variance.
Intuitively, imputation quality \textit{increases} when more complete attributes and complete tuples are available; correspondingly, $P(q')$ \textit{decreases} as the number of values increases.
The penalty should also reflect the intuition that the number of training observations has diminishing returns by decreasing more slowly as the number of values increases.
Finally, the penalty may increase as amount of noise or spread in the data increases, as this makes it more difficult to obtain precise imputations.

Imputation quality $P(q')=0$ is unattainable, as this suggests the values of missing elements are known exactly.
On the other hand, $P(q')=1$ occurs when all tuples with missing values are dropped, as this represents the worst case in terms of recovering information from these tuples.
In Section~\ref{sec:imputation}, we discuss choices for $P$ for several common imputation methods.

\subsubsection{Query runtime}
\label{sec:query-runtime}

To compute $\textsc{Time}_{T}(q)$, an additive measure of runtime cost, we retain a simple set of heuristics used by a standard database.

%\begin{align*}
%  \textsc{T}(q) &= \begin{cases}
%     \{ f_1(pg_{fetch}, io_{cost}) \} \cup \textsc{T}(q') & q = \delta_C(q') \\
%      \{ f_2(\textsc{Attr}(q'), C, \textsc{Card}(q'), iters) \} \cup \textsc{T}(q')  & q = \mu_C(q') \\  
%     \{ f_3(\textsc{T}(q_1'), \textsc{T}(q_2')) \} & q = q_1' \bowtie_\psi q_2' \\
%    \{ \hat{t} \} \cup \textsc{T}(q') & q = \sigma_\phi(q'), \\ & q = \pi_C(q') \\
%    \{ f_4(\textsc{Card}(t), pg_{size}, io_{cost}) \} & q = t\\
%  \end{cases}
%  \intertext{ \textit{for some set of functions  $f_{1,2,3,4}$, constant $\hat{t}$, IO Cost/Page $io_{cost}$, page fetches $pg_{fetch}$, page size $pg_{size}$, iterations $iters$} } 
%  \textsc{Time}(q) &= \Sigma \textsc{T}(q)
%\end{align*}

Scan costs are estimated based on the number of tuples, page size, and a unit-less I/O cost per page. 
We assume that I/O costs dominate CPU costs for relational operators. 
A database implementation for which this assumption is not true, e.g. an in-memory database, will need a different cost model.
Join costs are estimated as a function of the two relations joined, any blocking operations, and repeated fetching of tuples (if required).
Filtering, projecting, and aggregating never require repeated tuple fetches and have low computational overhead so we assume that they take negligible time.

We extend these heuristics to account for the new operators: \textit{Drop} and \textit{Impute}.
\textit{Drop} is a special case of a filter, so its time complexity is correspondingly negligible. 

\textbf{Constructing $T$}:
As with $\textsc{Penalty}$, the time computation for \textit{Impute} depends on the properties of the underlying algorithm via a function $T$.
If we assume that the \textit{Impute} routine is CPU-bound, then we adjust $T$ by some constant factor $f$, thereby scaling the result to be comparable with IO-bound routines.

The form of $T$ depends on the specific imputation method, but there are several considerations in choosing $T$.
In particular, $T$ should \emph{increase} as the number of tuples and complete and dirty attributes passed to the imputation function increases and it should take into account the complexity of the imputation algorithm.
Analysis of the time complexity of different imputation algorithms, especially with in-database execution, is beyond the scope of this paper.
As with $P$, it is the ordering between queries generated by $T$ that is more important than the precision of estimates.

We note here that for every imputation operator inserted into the query plan, a new
statistical model is instantiated and trained. Though it is tempting to try to pre-train
imputation models while the analyst begins to make queries, the exact rows of data, set
of complete attributes, and set of attributes to impute are unknown until runtime, making
this strategy infeasible. Thus, if applicable, $T$ should take into account both
training and application of the imputation model.

% In the implementation of our system,
% we use an iterative algorithm (see~\Cref{sec:imputation}), where the runtime cost
% is a function of the number of attributes with no missing data, number of attributes with
% missing data being imputed, number of tuples, and the number of iterations for the algorithm.

% In general, evaluating the time complexity for different \textit{Impute} operators is complicated, as the properties of different algorithms can vary significantly, the underlying I/O cost is not easily extracted (especially in the case that the entire set of tuples does not fit in memory), and the CPU cost dominates (in contrast to the other operators, which do not consider CPU cost explicitly).
% For example,~\cite{martin1995time} finds that the time complexity of the \textit{build tree} algorithm for one commonly-used class of decision trees is a function of the number of classes, several overhead constants, the parameterization of the partition and heuristic functions, and the \textit{arity} (the number of subsets considered for each split). 
% The \textit{build tree} phase often does not even dominate computation, as post-processing steps like pruning, which are vital in achieving good performance, can have cost exponential in the height of the tree.
% Nonetheless, in practice, we find that a simple parameterization can be acceptable (i.e.\ yields intelligent query plans).

\subsection{Imputation Strategies and their API}
\label{sec:imputation}

\ProjectName{} is designed so that any imputation strategy can be plugged in with minimal effort and without changes to the optimizer, by supplying penalty ($P$) and time ($T$) functions (\Cref{sec:cost-model}) pertinent to the algorithm.
These cost functions are then used by the optimizer during planning.
The flexibility of this approach is in the spirit of query optimization, which aims to reconcile performance with a declarative interface to data manipulation.
To motivate this API, we describe how a developer might specify these functions for a general-purpose imputation algorithm, as well as for two simpler imputation methods.

\subsubsection{Decision trees}

As a reference implementation,
\ProjectName{} uses a general-purpose imputation strategy based on
chained-equation decision trees.  Chained-equation imputation
methods~\cite{vanbuuren2011mice} (sometimes called \textit{iterative
regression}~\cite{gelman2006data}) impute multiple missing attributes by
iteratively fitting a predictive model of one missing attribute, conditional on
all complete attributes and all other missing attributes. Chained-equation
decision trees algorithms are effective~\cite{akande2015empirical}
and are widely used in epidemiological domains~\cite{burgette2010multiple}.

% Jose: I think this is a bit out of place. So trimmed for space
%These imputation algorithms are commonly used as part of a larger \textit{multiple imputation}
%process, in which multiple distinct copies of the complete data are generated and estimators
%are computed by averaging over the multiple datasets to form a single completed dataset~
%\cite{gelman2006data}. In \ProjectName{}, we consider only single imputation, though
%multiple imputation could be incorporated as another avenue of assessing imputation confidence.

The chained-equation algorithm proceeds by
iteratively fitting decision trees for the target attributes. 
In each iteration, the missing values of a single attribute are replaced with newly imputed values.  Values in a single column can be updated over multiple
iterations of the algorithm. With more iterations, the quality of the
imputation improves as values progressively reflect more accurate relationships
between attributes. The algorithm terminates after a fixed number of
iterations, or earlier if convergence is achieved, indicated by unchanged values in a 
column in subsequent iterations. For our experiments, we use Weka's \texttt{REPTree}
decision tree implementation~\cite{witten2016data}, a variant of the canonical \texttt{C4.5}
algorithm~\cite{quinlan1993}. Roughly, the time complexity of building one
decision tree is $\mathcal{O}(nm \text{ } log(n))$, where $n$ is the number of tuples
and $m$ is the number of dependent attributes~\cite{witten2016data}. Since we build
$k * |\textsc{Dirty}(q)|$ trees, we have

\begin{equation*}
    \begin{aligned}
    T(q) = k * &|\textsc{Dirty}(q)| * \textsc{Card}(q) * \\
               &(|\textsc{Clean}(q)| + |\textsc{Dirty}(q)| - 1) * log(\textsc{Card}(q))
    \end{aligned}
\end{equation*}

Separately, we specify a heuristic penalty that takes into account the
improvement in imputation quality from more tuples and attributes as well as
the diminishing returns from more data.

\[
    P(q) = \frac{|\textsc{Dirty}(q)|}{\sqrt{ \textsc{Attr}(q) * \textsc{Card}(q)}}
\]

\subsubsection{Mean}

At the other end of the complexity spectrum, mean value imputation is a common choice for exploratory data analysis.
Mean value imputation replaces missing values in a column with the mean of the available values in the column.

We can take advantage of the histograms maintained by \ProjectName{} to estimate the mean and construct a non-blocking impute operator.
Applying the heuristics from~\Cref{sec:quality} and~\Cref{sec:query-runtime}, we
have

\begin{equation*}
\begin{aligned}
    P(q) &= \sum_{c \in \textsc{Dirty}(q)} \frac{1+\widehat{Var}(c)}{\textsc{Card}(q)} \\
    T(q) &\approx 0
\end{aligned}
\end{equation*}

Note that $P$ increases with the variance of the columns and decreases with their cardinality, reflecting the intuition that the quality of the mean is lower with high variance and higher with more data.

\subsubsection{Hot deck}

Random hot deck imputation is a nonparametric method in which missing values in a column are
replaced with randomly selected complete values from the same column, preserving the
distribution of the complete values. We use the same $P$ as for mean value imputation, but
we modify $T$ to account for the fact that hot-deck is blocking in our implementation.  

\begin{equation*}
\begin{aligned}
    P(q) &= \sum_{c \in \textsc{Dirty}(q)} \frac{1+\widehat{Var}(c)}{\textsc{Card}(q)} \\
    T(q) &= \textsc{Card}(q)
\end{aligned}
\end{equation*}
}

\subsection{Query Planning}
\label{sec:planning}
The query planner must select a join ordering in addition to placing imputation operators as described in~\Cref{sec:placement}.

\subsubsection{Plan cache}
Before describing the operation of the query planner, we describe the semantics of a specialized data structure for holding sub-plans, called a \emph{plan cache}.
At a high level, a plan cache is a mapping from a set of tables to a set of dirty set-specific dominating plans over these tables.
We store multiple plans rather than a single plan per dirty set, because maintaining a Pareto frontier as a plan progresses through the optimizer's pipeline
allows us to obtain the final set of plans that best trade-off computation
cost and imputation quality. The plan-cache semantics are shown in~\Cref{fig:semantics-plan-cache}.

\begin{figure}
  \begin{align*}
    \llbracket Q[T] \rrbracket &= \begin{cases}
      P & (T, P) \in Q \\
      \emptyset & \text{otherwise} \\
    \end{cases} \\
    \llbracket Q[T] \lhd p \rrbracket &= \begin{aligned} Q \cup \{(T,\ P') \} \end{aligned}\\
    \text{ where }\ P' &= \left\{q ~\Bigg|~ \begin{aligned}
        q \in \llbracket Q[T] \rrbracket \cup \{p\}, \\
        \textsc{Dirty}(q) = \textsc{Dirty}(p),\\
        \forall q' \in \llbracket Q[T] \rrbracket.  \doms{q} {q'} 
      \end{aligned}
    \right\}
  \end{align*}
  \caption{Semantics of a plan cache $Q$, where $T$ is a set of tables and $p$ is a query plan}
  \label{fig:semantics-plan-cache}
\end{figure}

The plan cache uses the partial order of plans defined by  $\langle \textsc{Dirty}, \textsc{Penalty}, \textsc{Time} \rangle$ to
collect sound and complete Pareto frontiers. Plans with different $\textsc{Dirty}$ sets cannot be compared. The final
Pareto frontier produced by the optimizer is the set of plans with the best imputation
quality and runtime trade-off.

\subsubsection{Planning algorithm}
The planner (\Cref{algo:top-level-planner}) operates as follows.
First, it collects the set of attributes which are used by the operators in the plan or which are visible in the output of the plan (\Cref{lst:line:attr-start}--\Cref{lst:line:attr-end}).
This set will be used to determine which attributes are imputed.
Then, it constructs a plan cache. For each table used in the query, any available selections
are pushed to the leaves, and
a search over imputation operations produces various possible plans, which are added to the cache (\Cref{lst:line:sel}). If no selections are available, a 
simple scan is added to the plan cache (\Cref{lst:line:scan}). Join order is jointly optimized with imputation placements and a set of
plans encompassing all necessary tables is produced (\Cref{lst:line:join}). This set is now extended for any necessary grouping and aggregation
operations (\Cref{lst:line:group}). After a similar step has been taken for projections, the planner now contains the plan frontier (i.e.\ the
best possible set from which to pick our final plan). The final step in the planner is to find all plans that are
$\alpha$-bound and return the one that has the lowest runtime: the $\alpha$-bound optimal plan.

The join order and imputation optimization is based on the algorithm used in System R~\cite{blasgen1981system},
but rather than select the best plan at every point, we use our plan cache, which keeps track of the tables joined and the Pareto frontiers.
The output of the join optimizer is the set of plans which satisfy all join predicates and lie along their respective frontier.

\begin{algorithm}
\input{algorithms/plan-toplevel.tex}
\end{algorithm}

%\begin{algorithm}
%\input{algorithms/histogram-helpers.tex}
%\end{algorithm}

%\subsection{Optimality}\label{sec:optimal}
%We prove the optimality of the final plan selected by showing that our cost-model and algorithm
%construct a complete Pareto frontier.
%
%\begin{definition}
%$I: \mathcal{P}(D) \rightarrow \mathcal{P}(\{ \delta, \mu \})$, where
%$D$ is the dirty set for a relation, is an abstract imputation operator mapping a set of
%attributes with missing values to a set of imputation operations.
%
%For convenience we denote applying the set of imputation operations to a plan $p$ as $I(p)$ and understand that this
%is shorthand for $\{ op(p)   | op \in I(\textsc{Dirty}(p)) \}$.
%\end{definition}
%
%We can succinctly describe possible imputation transformations to a plan at a given point in the planning algorithm solely through $I$.
%
%\begin{theorem}\label{theorem:no-bad-sit}
%By computing dirty-set specific Pareto frontiers, it is never the case that we discard a plan $p_1$ s.t. $\doms{p_2}{p_1}$, but 
%$\exists p_1' \in I(p_1), p_2' \in I(p_2). \doms{p_1'}{p_2'} \land \textsc{Dirty}(p_1') = \textsc{Dirty}(p_2') \land p_1' \not \in I(p_2)$
%\end{theorem}
%
%\Cref{theorem:no-bad-sit} states that there is no \textit{bad situation} such as that depicted in~\Cref{fig:bad-sit-diagram}
%
%\begin{figure}
%\centering
%\begin{tikzpicture}
%% before I
%\node(p1) {$p_1$};
%\node[right=of p1](dom1){$\prec$};
%\node[right=of dom1](p2) {$p_2$};
%
%% after I
%\node[below=of p1](p1'){$p_1'$};
%\node[right=of p1'](dom2) {$\succ$};
%\node[right=of dom2](p2'){$p_2'$};
%
%% arrows
%\draw[->] (p1) -- node[right] {$\scriptstyle I$} (p1');
%\draw[->] (p2) -- node[right] {$\scriptstyle I$} (p2');
%\end{tikzpicture}
%\caption{A bad Pareto frontier selection would choose $p_2$ over $p_1$, as it is locally optimal, but with the available transformations ($I$) $p_2$  can only reach $p_2'$, which is dominated by the inaccessible $p_1'$.~\Cref{theorem:no-bad-sit} says such a situation cannot arise when constructing Pareto frontiers on a dirty-set basis.}
%\label{fig:bad-sit-diagram}
%\end{figure}
%
%\begin{proof}
%For contradiction, let $p_1$ and $p_2$ be plans such that $\doms{p_2}{p_1}$, $p_1$ is removed from the plan cache
%in favor of $p_2$, but there  $\exists p_1' \in I(p_1), p_2' \in I(p_2). \doms{p_1'}{p_2'} \land \textsc{Dirty}(p_1') = \textsc{Dirty}(p_2')$
%and $p_1'$ is not reachable from $p_2$ by $I$.
%
%There are two possible cases :
%
%\begin{case}
%$\textsc{Dirty}(p_1) = \textsc{Dirty}(p_2)$, in which case $I(p_1) = I(p_2)$, as $I$ uniquely determines the possible transformations. This makes $p_2$ and $p_1$ substitutable, so $p_1'$ is reachable from $p_2$. This is a contradiction.
%\end{case}
%
%\begin{case}
%$\textsc{Dirty}(p_1) \not= \textsc{Dirty}(p_2)$, in which case $p_1$ is not removed from the plan cache as the $\doms{p_2}{p_1}$ relation does not occur within the same dirty set. This is a contradiction
%\end{case}
%
%So it must be the case that no such situation arises.
%\end{proof}
%
%
%Given that no desirable plans are discarded by computing Pareto frontiers for each dirty set, the final Pareto frontier (for dirty set $\emptyset$) is optimal.

\subsection{Complexity}\label{sec:complexity}
Our optimization algorithm builds off the approach taken by 
a canonical cost-based optimizer~\cite{blasgen1981system}.
If imputation operators are ignored, we search the same plan space.
Therefore, our algorithm has a complexity of at least $O(2^J)$, where $J$ is the number of joins.

The addition of imputation operators increases the number of plans exponentially, as an imputation may be placed before any of the relational operators.
We restrict imputations to two classes: those that impute only attributes used in the operator and those that impute all the attributes that are needed downstream in the query.
By doing so we limit the number of imputations at any point to four: \textit{Drop} or \textit{Impute} over the attributes used in the operator or over all the downstream attributes.
This modification increases the complexity of our planning algorithm to $O(2^{4J})$.

To motivate the restriction of imputation types, we consider the implications of allowing arbitrary imputations.
If we allow any arbitrary subset of attributes to be imputed, then we would need to consider $O(2^{|D|+1})$ different imputation operators before each relational operator where $D$ is the set of dirty attributes in all tables.
This would increase the overall complexity of the algorithm dramatically.

Finally, we note that for the queries we have examined, this exponential blowup does not affect the practical performance of our optimizer.
Recall that the planner maintains different plans for different dirty sets, keeping only those plans that are not dominated by others.
So in many cases we can drop some of the intermediate plans generated at each operator.
The worst case complexity only occurs if the dirty sets tracked are distinct through the entire planning phase.

\review{
In order to support a large number of joins, with lower planning times, \ProjectName{} can maintain approximations of the Pareto sets.
A non-dominated plan can be approximated by an existing plan in the set, if the distance between them is smaller than some predefined bound.
Approximated plans are not added to the frontier, pruning the search space further. 
Planning times across both exact and approximate Pareto frontiers are discussed further in \Cref{sec:results}.
}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
