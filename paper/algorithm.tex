\section{Algorithm}

Our optimizer searches a restricted space (Sec.~\ref{sec:search-space}) of query plans for a plan that minimizes a metric (Sec.~\ref{sec:cost-model}) which combines the runtime cost of the query and the quality of the results.
This plan must not emit any tuples which contain null values, regardless of the state of the base tables.
Additionally, the traditional relational operators (selection $\sigma$, projection $\pi$, join $\bowtie$, and group-by/aggregate) must never observe a null value in any attribute that they directly operate on.

\subsection{Imputation operators}
We introduce two new relational operators to perform imputation: impute ($\mu$) and drop ($\delta$). Each operator takes arguments $(C, R)$ where $C$ is a set of attributes and $R$ is a relation. Impute uses a machine learning method to replace all null values with non-null values for attributes in $C$ in the relation $R$. Drop simply removes from $R$ all tuples which have a null value for some attribute in $C$. Both operators guarantee that the resulting relation will contain no null values for attributes in $C$. 

\subsection{Cost model}
\label{sec:cost-model}
We rank query plans $Q$ using a cost model $\text{Cost}(Q) = (1 - \alpha) \times \text{Time}(Q) + \alpha \times \text{Loss}(Q)$. $\text{Time}(Q)$ is an estimate of the runtime of the query which is derived from table statistics and selectivity estimation of the query predicates. $\text{Loss}(Q)$ is an estimate of the amount of error introduced by the imputation procedure. $\alpha$ is a parameter to the query optimizer that controls the emphasis on quality over performance. $\alpha = 1.0$ means that the query should be as accurate as possible (at the expense of performance), $\alpha=0.0$ means that the query should be as fast
as possible (at the expense of accuracy).

In order to correctly estimate $\text{Time}(Q)$ and $\text{Loss}(Q)$, the system must have cardinality estimates
for each sub-query in each query plan.
These cardinality estimates are impacted not just by filtering or joining, as in the traditional relational calculus, but also by the imputation operators.
For example, a drop operator will reduce the cardinality of the result while an impute operator will maintain the same cardinality as the input.
For simplicity, each of the logical nodes in a query plan points to a set of histograms.
When the optimizer creates a new query plan, it copies the histograms of the sub-plans and modifies them as necessary to account for new operation in the plan.
Algorithm~\ref{algo:histogram-transformation} describes the process of generating new histograms from sub-plans.

The computation of $\text{Time}(Q)$ is standard and is comparable to the cost model used in any modern RDBMS.\@ Our runtime cost model is perhaps less sophisticated than most, as it is identical to the cost model in SimpleDB.

\todo{Check loss calculation w/ Micah}
$\text{Loss}(Q)$ is computed as follows.
For each sub-query, in addition to a histogram, we keep track of the estimated number of null values in each column.
\begin{align*}
  \text{Loss}(Q) = \begin{cases}
    \sum_{c \in C} \text{Null}(c, Q') & Q = \delta_C(Q') \\
    \frac{1}{\sqrt{|Q'|}} \sum_{c \in C} \text{Null}(c, Q') & Q = \mu_C(Q') \\
    \text{Loss}(Q_1') + \text{Loss}(Q_2') & Q = Q_1' \bowtie_\psi Q_2' \\
    \text{Loss}(Q') & Q = \sigma_\phi(Q'), Q = \pi_C(Q') \\
  \end{cases}
\end{align*}
Dropping tuples which contain a null value incurs a loss penalty of 1 for each null field dropped.
Imputing null fields incurs a loss penalty $0 < p \leq 1$ which decreases as the number of tuples available to train the imputation algorithm increases.
The intuition behind this formula is that imputation performance should increase if there is more training data available, but there is a saturation point beyond which more data provides little additional benefit.

\subsection{Search space}
\label{sec:search-space}
To reduce the size of the query plan search space, only plans that fit the following template are considered. First, all filters are pushed to the leaves of the query tree, immediately after the scans. Joins are performed after filtering, and only left-deep plans are considered. Any group-by/aggregate will be performed after the final join. Finally, projections are placed at the root of the query tree. The space of query plans is similar to that considered by System R~\cite{blasgen1981system}, with the addition of imputation operators appearing before/after traditional operators.

\subsection{Imputation placement}
\label{sec:placement}
Imputation operators must be placed so that no relational operator receives a tuple containing null in an attribute that the operator examines, regardless of the state of the data in the base tables.

Imputation operators can be placed at any point in the query plan, but to meet the guarantee that no non-imputation operator sees a null value, there are cases where an imputation operator is required. To track these cases, each query plan is associated with a set of dirty attributes $D$. An attribute $c$ is \emph{dirty} in some relation if the values for $c$ may contain null. We compute a dirty set for each base table using the table statistics, which track the number of null values in each column. If we apply an imputation operator to a relation $R$ with a dirty set $D$, $\mu_C (R)$ or $\delta_C (R)$, the resulting query has a dirty set $D' = D \setminus C$.  Applying a projection $\pi_C(R)$ produces a dirty set $D' = D \cap C$. A join $R_1 \Join_\psi R_2$ with dirty sets $D_1$ and $D_2$  produces a dirty set $D' = D_1 \cup D_2$.  Filters do not change the dirty set.

That is, the dirty set over-approximates the set of attributes that contain null. For example, a filter might remove all tuples which contain null without changing the dirty set, forcing an unnecessary imputation. We choose to over-approximate the dirty set to avoid the possibility of dropping a tuple that contains a null value without explicitly imputing the value or applying a drop operator.

\subsection{Query planning}
The input to our query planner is a tuple $(T, \Phi, \Psi, P, G, A)$: a set of tables $T$, a set of filter predicates $\phi_t, t \in T$, a set of join predicates $\psi_(t_1, t_2), t_1, t_2 \in T$, a set of attributes $P$, and an optional set of attributes $G$ and aggregation function $A \in \{\text{Max}, \text{Min}, \text{Sum}, \text{Avg}, \text{Count}\}$.

The query planner must select a join ordering in addition to placing imputation operators as described in Section~\ref{sec:placement}.

To reduce the search space, we only consider the minimal imputation, the maximal imputation, and the minimal drop. The minimal imputation (resp. drop) only imputes (resp. drops) the columns required by the relational operator immediately following the imputation. The maximal imputation imputes all columns in the relation, regardless of which are required.  Algorithm~\ref{algo:plan-helpers} presents
a series of helper functions used by the top level planner, shown in Algorithm~\ref{algo:top-level-planner}.

\begin{algorithm}
\input{algorithms/plan-helpers.tex}
\end{algorithm}

\begin{algorithm}
\input{algorithms/plan-toplevel.tex}
\end{algorithm}

\begin{algorithm}
\input{algorithms/histogram-transform.tex}
\end{algorithm}

\subsection{Imputation Strategies}

We design our system such that any imputation strategy can be plugged in with minimal effort
and without changes to the optimizer, so the strategy can in principle be targeted to a
specific domain. The current version of the system uses a general-purpose imputation
strategy based on chained-equation classification and regression trees (CE-CART). Chained
equation imputation methods \cite{vanbuuren2011mice} (sometimes called \textit{iterative
regression} \cite{gelman2006data} and others) impute multiple missing attributes by
iteratively estimating predictive models of one missing attribute conditional on all
complete attributes and the other missing attributes. The individual predictive models can
be customized by the researcher to the problem at hand. Decision tree algorithms, like CART,
are found to be effective \cite{akande2015empirical} in empirical studies in general-purpose
routines and are widely used in epidemiological domains \cite{burgette2010multiple}.  

These imputation algorithms are rarely used in isolation --- rather, the ultimate goal is to
perform statistical analysis of the complete data. To this end, the technique of
\textit{multiple imputation} can be used, in which multiple distinct copies of the complete
data are generated and estimators are computed by averaging over the multiple datasets. In
ImputeDB, we cannot assume the user desires multiple copies of the query result, though we
note that in further work, multiple imputation could be used within ImputeDB to generate
lower-variance estimates of some aggregates, like averages.

Our algorithm proceeds by iteratively fitting regression trees to a subset of the data and
the target imputation columns. In each iteration, the missing values of a column are
replaced with newly imputed values. With each epoch, the quality of the imputation improves
as values progressively reflect more accurate relationships amongst attributes. The
algorithm terminates when convergence is achieved (the imputed values do not change across
epochs) or a fixed number of epochs is reached.  Algorithm~\ref{algo:imputation-strategy}
provides details on the implementation.  

It is worth noting that given the nature of the imputation strategy used here, imputation
operators become blocking in our system. An extension of ImputeDB could consider regression
algorithms that learn in an online nature; this would reduce the cost of the imputation at
the possible expense of quality.

\begin{algorithm}
    \input{algorithms/cart.tex}
\end{algorithm}

\subsection{Complexity}
Our optimization algorithm builds off the approach taken by System R\cite{blasgen1981system}, therefore our algorithm still operates in exponential time. Indeed, 
note that if we remove our restriction on types of imputation($\delta_{min}, \mu_{min}, \mu_{max}$), and allow any arbitrary subset of attributes to be imputed,
then every single operator in the query plan has a number of imputations exponential in the number of dirty columns. Our restriction, instead, increases the number
of plans (in the worst case) at each operator by a factor of 3. Of course, this implies that in the worst case (where all dirty sets tracked are distinct throughout the query plan),
we explore a number of plans that further scales the number of plans in the original algorithm by an exponential factor.

However, we note that in all cases we have considered, this exponential blowup does not affect the practical performance of our optimizer.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
