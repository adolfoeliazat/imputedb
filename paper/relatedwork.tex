\section{Related Work}

\subsection{Missing Values and Statistics}

Imputation of missing values is a widely studied field within the statistics and machine
learning communities. As highlighted in~\cite{gelman2006data}, missing data
can appear for a variety of reasons, including both random and conditioned on
existing values (observed and missing). Methods in the statistical community
focus on correctly modeling relationships between the attributes to factor in
varied forms of missingness. For example, Burgette and Reiter~\cite{burgette2010multiple} discuss the usage of sequential regression trees
for imputing missing data.

In~\cite{akande2015empirical}, Akande et al analyze the performance of various
multiple imputation techniques on the American Community Survey dataset. 
The computational difficulties of imputing on large base
tables are well known and can limit approaches. For example, Akande finds that
one approach (MI-GLM) is prohibitively expensive when attempting to impute on data that
includes variables with potentially large domains (ten categories in their case).
In contrast, \ProjectName{} allows users to specify a tradeoff between
information lost and runtime performance, allowing users to perform queries
directly on the entire dataset. Furthermore,
the query planner's imputation is guided by the requirements of each specific
query's operators, rather than requiring broad assumptions about query
workloads.  

\subsection{Missing Values and Databases}
There is a long history in the database community surrounding the
treatment of nulls. As early as 1973,~\cite{codd1973understanding}
provides a treatment of the semantics of null. Multiple
papers have described various (at times conflicting) treatments
of nulls~\cite{grant1977null}. \ProjectName's main design invariant - no relational operator
sees missing values for attributes it must operate on and users should never see
missing data - eliminates
the need to handle null value semantics, while guaranteeing soundness (modulo
imputation strategy) of the query evaluation.

Database system developers and others have worked on techniques to automatically
detect dirty values, whether missing or otherwise, and rectify the errors if
possible. A survey of methods and systems is provided in
\cite{hellerstein2008quantitative}.

BayesDB \cite{mansinghka2015bayesdb} provides users with a simple interface to 
leverage statistical inference techniques in a database. Non-experts
can use a simple declarative language (an extension of SQL), to specify models
which allow missing value imputation, amongst other broader functionality.
Experts can further customize strategies and express domain knowledge to
improve performance and accuracy.

While BayesDB can be used for value imputation, this step is not framed
within the context of query planning, but rather as an explicit statistical
inference step within the query language, using the \verb|INFER| operation. 

BayesDB provides a great alternative for bridging the gap between
traditional databases and sophisticated modeling software. \ProjectName{}, in
contrast, aims to remain squarely in the database realm, while allowing
users to directly express queries on a potentially larger subset of their data.

\ProjectName's cost-based query planner 
is partially based on the seminal work developed for System R's query planning\cite{blasgen1981system}.
However, in contrast to System R, \ProjectName{} performs additional histogram transformations to account
for the changing nature of missing values.

\subsection{Forecasting and Databases}
Parisi et al\cite{parisi2011embedding} introduce the idea of incorporating time-series forecast operators into
databases, along with the necessary relational algebra extensions. Their work explores the theoretical
properties of forecast operators and generalizes them into a family of operators, distinguished by
the type of predictions returned. They highlight the use of forecasting for replacing missing values.
In their follow on work\cite{parisi2013temporal}, they identify various equivalence and containment
relationships when using forecast operators, which could be used to perform query plan transformations that guarantee the same result. They
explore forecast-first and forecast-last plans, which perform forecasting operations before and after traditional
relational operators, respectively.

In contrast to this work, \ProjectName{} is targeted at generic value imputation, not necessarily tailored to 
time-series. The optimizer is not based on equivalence transformations, nor are there guarantees of equal
results under different conditions. Instead, the optimizer allows users to pick their tradeoff between
runtime cost and information lost. The search space considered by our optimizer is broader, not just
forecast-first/forecast-last plans, but rather imputation operators can be placed anywhere in the query plan
(with some restrictions). The novelty of our contribution lies in the successful incorporation of
imputation operations in non-trivial query plans with cost-based optimization.

\cite{duan2007processing} describes the Fa system and its declarative language for time-series forecasting. Their
system automatically searches the space of attribute combinations/transformations and statistical models
to produce forecasts within a given accuracy threshold. Accuracy estimates are determined using
standard techniques, such as k-fold cross-validation. 

Similarly to Fa, \ProjectName{} provides a declarative language, as
a subset of standard SQL. \ProjectName{}, however, is not searching the space of possible
imputation models, but rather the space of query plans that incorporate imputation operators. Another major point
of distinction between \ProjectName{} and Fa is that the latter doesn't consider tradeoffs between accuracy and computation time, but rather returns the most accurate forecast (with some stopping criterion).



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
