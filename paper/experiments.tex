\section{Experiments}\label{sec:experiments}
For our experiments we plan and execute queries for three separate survey-based data sets, showing that our system is well suited for early dataset exploration.
We show that \ProjectName{} performs an order of magnitude better than traditional
base-table imputation and produces results of comparable quality.

\subsection{Data sets}\label{subsec:datasets}
We collect three data sets for our experiments.
For all data sets, we selected a subset of the original attributes.
We also transform all data values to an integer representation by enumerating strings and transforming floating-point values into an appropriate range.

\subsubsection{CDC NHANES}
For our first set of experiments, we use survey data collected by the 
U.S. Centers for Disease Control and Prevention (CDC). We
experiment on a set of tables collected as part of the 2013--2014 National
Health and Nutrition Examination Survey (NHANES), a series of studies
conducted by the CDC on a national sample of several thousand individuals~\cite{cdc-data}.
The data consists on survey responses, physical examinations, and laboratory
results, amongst others.

There are 6 tables in the NHANES data set. We use three tables for our experiments:

\begin{itemize}
	\item \texttt{demo}: demographic information of subjects
	\item \texttt{exams}: physical exam results
	\item \texttt{labs}: laboratory exam results
\end{itemize}

The original tables have a large number of attributes, in some cases providing more granular tests results or alternative metrics.
We focus on a subset of the attributes for each table to simplify the presentation and exploration of queries.
\Cref{table:nhanes-description} shows the attributes selected, along with the percentage of \nullv{} values for each attribute.
For readability, we have replaced the NHANES attribute names with self-explanatory attribute names.

\begin{table*}
  \centering
  \begin{subtable}{0.3\textwidth}
    \centering
    \input{tables/cdc_demo}
    \caption{Demographics (\texttt{demo}). \demorows{} rows.}
  \end{subtable}\quad\begin{subtable}{0.3\textwidth}
    \centering
    \input{tables/cdc_labs}
    \caption{Laboratory Results (\texttt{labs}). \labexrows{} rows.}
  \end{subtable}\quad\begin{subtable}{0.3\textwidth}
    \centering
    \input{tables/cdc_exams}
    \caption{Physical Results (\texttt{exams}). \labexrows{} rows.}
  \end{subtable}
  \caption{Percentage of values missing in the CDC NHANES 2013--2014 data.}\label{table:nhanes-description} 
\end{table*}

\subsubsection{freeCodeCamp 2016 New Coder Survey}
For our second set of experiments, we use data collected by freeCodeCamp, an open-source
community for learning to code, as a part of a survey of new software
developers~\cite{fcc-data}.  The \textit{2016 New Coder Survey} consists of responses by
over 15,000 people to 48 different demographic and programming-related questions.  The
survey targeted users who were related to coding organizations.

We use a version of the data that has been pre-processed, but where missing values remain.
For example, 46.6\% of \textit{commutetime} responses are missing. However, it is worth
noting that some of the missing values are also expected, given the way the data has been
de-normalized. For example, \textit{bootcamploanyesno}, a binary attribute encoding whether
a respondent had a loan for a bootcamp, is expected to be \nullv{} for participants who did not
attend a bootcamp.

We choose a subset of 17 attributes, which are shown in~\Cref{table:fcc-description} along
with the percentage of missing values.

\begin{table}
  \centering
  \input{tables/fcc}
  \caption{Percentage of values missing in the freeCodeCamp Survey data (\texttt{fcc}).}\label{table:fcc-description} 
\end{table}

\subsubsection{American Community Survey}
For our final experiment, we run a simple aggregate query over data from the American
Community Survey (ACS), a comprehensive survey conducted by the U.S.
Census Bureau. We use a cleaned version of the 2012 Public Use Microdata Sample (PUMS)
data kindly provided by the authors of~\cite{akande2015empirical}. Given that the data had
been cleaned, we artificially dirtied it by replacing 40\% of the values uniformly at random with \nullv{} values.
The final dataset consists of 671,153 rows and 37 integer columns.

\subsection{Queries}
We collect a set of queries (\Cref{fig:queries}) that we think are interesting to plan.
We believe that they could reasonably be written by a user in the course of data analysis.

The queries consist not only of projections and selections, but also
interesting joins and aggregates. Our aim was to craft meaningful queries that would
provide performance figures relevant to practitioners using similar datasets.

Our first set of queries is on the CDC data (\Cref{fig:queries-cdc}).
\Cref{q1} calculates the average cuff size for individuals based on their income data, with a constraint on height.
\Cref{q2} compares creatine levels for individuals with low, medium, and high incomes and above a certain weight.
\Cref{q3} extracts the average blood lead levels for children under 6 years of age.
\Cref{q4} calculates the average systolic blood pressure, by gender, for subjects with a body mass index indicating obesity. 
\Cref{q5} calculates the average waist circumference for subjects above a certain height and weight.

Our second set of queries is on the freeCodeCamp data (\Cref{fig:queries-fcc}).
\Cref{q6} calculates the average income for higher-income survey participants, grouped by their bootcamp attendance.
\Cref{q7} estimates the average commute time of women from the United States who participated.
\Cref{q8} calculates the average amount of student debt based on school degree for survey participants who have student debt.
\Cref{q9} joins the freeCodeCamp data with a table provided by the World Bank which summarizes GDP per-capita across various countries~\cite{worldbank-data}.
The query calculates the average GDP per-capita by grouping 18+ year old participants who attended bootcamp versus those who did not.

\begin{table*}
  \centering
  \input{queries}
\end{table*}

%\begin{table*}
%  \centerfloat
%  \input{runtimes}
%    \caption{Base error, percent change in error and and running time for queries
%    with different imputation levels. Base error is the root-mean-square error (RMSE) between the query on clean
%    data and the query run on dirty data without imputation. Change in error is relative to the base error.}
%  \label{fig:experiments}
%\end{table*}

\subsection{Results}\label{sec:results}

For each query, we evaluate three different configurations: \ProjectName{} optimizing for
quality ($\alpha=0$), \ProjectName{} optimizing for performance ($\alpha=1$), and imputing
at the base tables followed by executing the query traditionally. 

\Cref{fig:runtimes} shows a summary of the performance results. The quality-optimized
queries are an order-of-magnitude faster than imputing on the base tables. We can get
another order-of-magnitude speedup when optimizing for performance. This performance
differential means it is feasible
to explore multiple imputations operations (including more expensive operators) when using
\ProjectName{}, in contrast to the traditional approach of base table imputation.

\begin{figure}
\includegraphics[width=\columnwidth]{figures/running_times_combined_bar.png}
\caption{Query runtimes with imputation on base tables, \ProjectName{}
    optimizing for quality ($\alpha=0$), and \ProjectName{} optimizing for
    performance ($\alpha=1$). Quality-optimized, on-the-fly imputation provides an order of
    magnitude speedup over imputing on base tables. Efficiency-optimized on-the-fly
    imputation can provide another order-of-magnitude speedup at the expense of quality.}
    
\label{fig:runtimes}
\end{figure}

%\Cref{fig:plantimes} provides a summary of the planning times for each of the queries.
%We exclude the planning time for queries that impute at base table, as that requires no
%planning. 

In the median across a variety of queries and choices of $\alpha$, planning took
\planningmediantime{} ms, 
constituting \planningruntimepercent{} percent of total runtime. In all cases, the optimizer
returned a query plan within \planningmaxtime{} ms, with times similar across values of $\alpha$.

All experiments were run on a single Amazon Web Services EC2 {\tt c4.xlarge} instance, with
four 2.9 GHz Intel Xeon E5--2666 v3 virtual CPUs and 7.5 GiB of main memory, on Debian Linux.
Each query and each value of $\alpha$ was run 200 times, after discarding a warm-up period.

% The one-standard-deviation
%intervals around the mean planning time often overlap, suggesting the planning component is
%constant in $\alpha$.

%\begin{figure}
%\includegraphics[width=\columnwidth]{figures/planning_times_imputedb.png}
%\caption{Planning times for each query}
%\label{fig:plantimes}
%\end{figure}
\subsubsection{Accuracy vs. Base-table Imputation}

\Cref{table:smape} shows the Symmetric-Mean-Absolute-Percentage-Error
(SMAPE)~\cite{Makridakis2000451} for \ProjectName{}'s query results when compared to
imputing on the base tables and executing the query on this cleaned base data.
This measure indicates how much error is introduced by on-the-fly imputation compared to
imputation on the base tables. This comparison is the relevant one in practice --- an
analyst would only be considering the trade-offs in imputation quality and time in the first
place for dirty input data, for which ground truth is unavailable.

In our case, the relevant comparison is against traditional
imputation on the base table, given the unavailability of ground truth.

For $\alpha=0$ and $\alpha=1$, we pair up tuples and compute SMAPE for all queries. That is,
we compute tuple-wise absolute percentage deviations within each iteration of a query, and
we report this value averaged over all iterations.  We
can see that optimizing for quality indeed reduces the SMAPE of query results.  In general, the SMAPE relative 
to the base-imputation approach is low---between \lowsmapealphazero{} and
\highsmapealphaoneexacs{} percent---indicating that on-the-fly imputation produces similar
results to imputation at the base tables. Notably, the results with largest error correspond
most closely to one existing approach of dropping all \nullv{}s.

We also calculate the number of tuples used to produce each aggregate output.
The count fraction columns in \Cref{table:smape} show the number of tuples in the aggregate for $\alpha = 0$ and $\alpha = 1$ as a fraction of the number of tuples
used when running the query on the imputed base table.
This shows that when optimizing for performance, not quality, many tuples are dropped.
Even in cases where the SMAPE reduction between $\alpha = 1$ and $\alpha = 0$ is small (\Cref{q2}, \Cref{q6}) the tuple count is significantly different.
In these cases, the aggregate value is not significantly impacted by the missing data. In particular, if values are missing uniformly at random,
the aggregate should not be affected.
However, if the missing data is biased then the aggregate will have a significant error.
This highlights a challenge for a user handling data imputation traditionally: it is unclear if the missing data will have a large or small negative impact on their analysis until they have paid the cost of running it.
By using \ProjectName{} this cost can be lowered significantly.

\Cref{fig:pareto-frontiers} shows the final Pareto frontiers produced for \Cref{q1} and \Cref{q6}. These are the resulting set of plans after pruning dominated plans throughout our algorithm.
The diversity of plans highlights the possible tradeoffs that the user can make in choosing a final plan for execution. The frontier is available at the end of the planning stage and
can be exposed to the user to help in guiding their workflow for exploring values of $\alpha$.

\begin{table}
\centering
\input{tables/perf_summary.tex}
\caption{Symmetric-Mean-Absolute-Percentage-Error for queries run under different $\alpha$
    parameterizations. To calculate SMAPE, \ProjectName{} results are compared to query results returned
    from executions with base table imputation. Queries optimized
    for quality ($\alpha=0$) generally achieve lower error than queries optimized for
    efficiency ($\alpha=1$). The count fraction column shows the number of tuples used in calculating each aggregate
     as a fraction of the number of tuples used when running the same query after imputing on the base table.
    A lower count share reflects more potential for errors.}
\label{table:smape}
\end{table}

\begin{figure}
\centering
\includegraphics[scale=0.4]{figures/pareto_frontiers_plot.png}
\caption{\ProjectName{}'s final Pareto frontier for \Cref{q1} and \Cref{q6}. The user can tradeoff between the plans by adjusting values of $\alpha$. Depending on the query
there may be small clusters of plans centered around a similar penalty or time value. This may occur as no plan actually dominates another at a given level of precision.
For clarity, we have removed from the plot plans that provide similar tradeoffs in either dimension. }
\label{fig:pareto-frontiers}
\end{figure}

\subsubsection{Runtime of Base-table Imputation}
In many real-world cases, applying the imputation step at the base table is prohibitively
expensive.
To illustrate the increasing difficulty of such an approach as datasets scale, we run the following query over the ACS dataset:
\begin{lstlisting}[breaklines]
SELECT AVG(c0) FROM acs_dirty;
\end{lstlisting}
Applying the imputation operation to the base table is extremely expensive, as it imputes over all attributes in all rows, in our case
completing in \acsbaseresultminutes{}. In contrast, \ProjectName{} executes a quality-optimized version
in \acsimputedbzeroresult{} and a runtime-optimized version in \acsimputedboneresult{}. This highlights the potential
benefit of using our system for early data exploration. Note that the performance differential relative to base table imputation
would likely widen further if selection predicates were added to the query.

For every imputation inserted into the query plan, a new statistical model is instantiated and trained before being used for imputation.
Although it is tempting to further optimize query execution in \ProjectName{} by pre-training imputation models while the analyst begins to make queries,
this is not desirable because the specific rows of data, set of complete attributes, and set of attributes to impute are unknown until runtime.


\subsubsection{Planning times}
\begin{table}
\centering
\input{tables/planning_summary_exact_pareto.tex}
\caption{Planning times are sub-second for up to 4 joins. For 5+  joins planning times rise. Many practical queries
can be formulated with a smaller number of joins, but to support more complex queries \ProjectName{} provides an
optimization that maintains approximate Pareto sets for queries with many joins. This reduces the 29 second
planning time shown for 7 joins by approximately 45\%, on average.}
\label{table:planning-times}
\end{table}

We explored the practical implications of our planning algorithm by constructing a series of queries with increasing number of joins.
We used the CDC tables and evaluated a combination of self-joins and joins across reference tables. Planning times for queries
involving 1 to 5 joins, which although limited is a practical value for real world exploratory queries, take less than a second.
For queries with 7 joins, planning time increases substantially to 29 seconds, on average. We introduced an optimization that reduces
the plan space by maintaining only approximations of the Pareto set for queries involving a large number of joins.
With this optimization, the same set of queries have an average planning time of 16 seconds (2 seconds SE).

%\todobox{Jose: I'm not a fan of this line, I would remove}{Furthermore, at that point, it may make more sense to specify a coherent model over the entire base table instead.}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
