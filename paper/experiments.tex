\section{Experiments}\label{sec:experiments}
For our experiments we plan and execute queries for three separate survey-based data sets, showing that our system is well suited for early dataset exploration.
We show that \ProjectName{} performs an order of magnitude better than traditional
base-table imputation and produces results of comparable quality.

\subsection{Data Sets}\label{subsec:datasets}
We collect three data sets for our experiments.
For all data sets, we selected a subset of the original attributes.
We also transform all data values to an integer representation by enumerating strings and transforming floating-point values into an appropriate range.

\subsubsection{CDC NHANES}
For our first set of experiments, we use survey data collected by the 
U.S. Centers for Disease Control and Prevention (CDC). We
experiment on a set of tables collected as part of the 2013--2014 National
Health and Nutrition Examination Survey (NHANES), a series of studies
conducted by the CDC on a national sample of several thousand individuals~\cite{cdc-data}.
The data consists on survey responses, physical examinations, and laboratory
results, amongst others.

There are 6 tables in the NHANES data set. We use three tables for our experiments:

\begin{itemize}
	\item \texttt{demo}: demographic information of subjects
	\item \texttt{exams}: physical exam results
	\item \texttt{labs}: laboratory exam results
\end{itemize}

The original tables have a large number of attributes, in some cases providing more granular tests results or alternative metrics.
We focus on a subset of the attributes for each table to simplify the presentation and exploration of queries.
\Cref{table:nhanes-description} shows the attributes selected, along with the percentage of \nullv{} values for each attribute.
For readability, we have replaced the NHANES attribute names with self-explanatory attribute names.

\begin{table*}
  \centering
  \begin{subtable}{0.3\textwidth}
    \centering
    \input{tables/cdc_demo}
    \caption{Demographics (\texttt{demo}). \demorows{} rows.}
  \end{subtable}\quad\begin{subtable}{0.3\textwidth}
    \centering
    \input{tables/cdc_labs}
    \caption{Laboratory Results (\texttt{labs}). \labexrows{} rows.}
  \end{subtable}\quad\begin{subtable}{0.3\textwidth}
    \centering
    \input{tables/cdc_exams}
    \caption{Physical Results (\texttt{exams}). \labexrows{} rows.}
  \end{subtable}
  \caption{Percentage of values missing in the CDC NHANES 2013--2014 data.}\label{table:nhanes-description} 
\end{table*}

\subsubsection{freeCodeCamp 2016 New Coder Survey}
For our second set of experiments, we use data collected by freeCodeCamp, an open-source
community for learning to code, as a part of a survey of new software
developers~\cite{fcc-data}.  The \textit{2016 New Coder Survey} consists of responses by
over 15,000 people to 48 different demographic and programming-related questions.  The
survey targeted users who were related to coding organizations.

We use a version of the data that has been pre-processed, but where missing values remain.
For example, 46.6\% of \textit{commutetime} responses are missing. However, it is worth
noting that some of the missing values are also expected, given the way the data has been
de-normalized. For example, \textit{bootcamploanyesno}, a binary attribute encoding whether
a respondent had a loan for a bootcamp, is expected to be \nullv{} for participants who did not
attend a bootcamp.

We choose a subset of 17 attributes, which are shown in~\Cref{table:fcc-description} along
with the percentage of missing values.

\begin{table}
  \centering
  \input{tables/fcc}
  \caption{Percentage of values missing in the freeCodeCamp Survey data (\texttt{fcc}).}\label{table:fcc-description} 
\end{table}

\subsubsection{American Community Survey}
For our final experiment, we run a simple aggregate query over data from the American
Community Survey (ACS), a comprehensive survey conducted by the U.S.
Census Bureau. We use a cleaned version of the 2012 Public Use Microdata Sample (PUMS)
data kindly provided by the authors of~\cite{akande2015empirical}. Given that the data had
been cleaned, we artificially dirtied it by replacing 40\% of the values uniformly at random with \nullv{} values.
The final dataset consists of 671,153 rows and 37 integer columns.

\subsection{Queries}
We collect a set of queries (\Cref{tbl:queries}) that we think are interesting to plan.
We believe that they could reasonably be written by a user in the course of data analysis.

The queries consist not only of projections and selections, but also
interesting joins and aggregates. Our aim was to craft meaningful queries that would
provide performance figures relevant to practitioners using similar datasets.

%Our first set of queries is on the CDC data (\Cref{fig:queries-cdc}).
%\Cref{q1} calculates the average cuff size for individuals based on their income data, with a constraint on height.
%\Cref{q2} compares creatine levels for individuals with low, medium, and high incomes and above a certain weight.
%\Cref{q3} extracts the average blood lead levels for children under 6 years of age.
%\Cref{q4} calculates the average systolic blood pressure, by gender, for subjects with a body mass index indicating obesity. 
%\Cref{q5} calculates the average waist circumference for subjects above a certain height and weight.
%
%Our second set of queries is on the freeCodeCamp data (\Cref{fig:queries-fcc}).
%\Cref{q6} calculates the average income for higher-income survey participants, grouped by their bootcamp attendance.
%\Cref{q7} estimates the average commute time of women from the United States who participated.
%\Cref{q8} calculates the average amount of student debt based on school degree for survey participants who have student debt.
%\Cref{q9} joins the freeCodeCamp data with a table provided by the World Bank which summarizes GDP per-capita across various countries~\cite{worldbank-data}.
%The query calculates the average GDP per-capita by grouping 18+ year old participants who attended bootcamp versus those who did not.

\begin{table*}
  \centering
  \input{queries}
  \vspace{1ex}
  \caption{Queries used in our experiments.}\label{tbl:queries}
\end{table*}

%\begin{table*}
%  \centerfloat
%  \input{runtimes}
%    \caption{Base error, percent change in error and and running time for queries
%    with different imputation levels. Base error is the root-mean-square error (RMSE) between the query on clean
%    data and the query run on dirty data without imputation. Change in error is relative to the base error.}
%  \label{fig:experiments}
%\end{table*}

\subsection{Results}\label{sec:results}

For each query, we evaluate \todo{update: three different configurations}: \ProjectName{} optimizing for quality ($\alpha=0$) and \ProjectName{} optimizing for performance ($\alpha=1$) and as a baseline, we fully impute the tables used by each query then run the query on the imputed tables, simulating the process that an analyst would follow to apply existing imputation techniques.

\Cref{fig:runtimes} shows a summary of the performance results. The quality-optimized
queries are an \todo{update/validate: order-of-magnitude faster than the baseline. We get another order-of-magnitude speedup when optimizing for performance}. This performance
differential means it is feasible
to explore multiple imputation operations (including more expensive operators) when using
\ProjectName{}, in contrast to the traditional approach of fully imputing the dataset.

\begin{figure}
\includegraphics[width=\columnwidth]{figures/running_times_combined_bar.png}
\caption{Runtimes for the queries in~\Cref{tbl:queries} using reference
    chained-equation decision trees algorithm. $\alpha=0$ (resp. $\alpha=1$) is
    \ProjectName{} run at the highest (resp. lowest) quality setting. Baseline
    is full table imputation followed by running the query. Each query and each
    value of $\alpha$ was run 200 times.}
\label{fig:runtimes}
\end{figure}

%\Cref{fig:plantimes} provides a summary of the planning times for each of the queries.
%We exclude the planning time for queries that impute at base table, as that requires no
%planning. 

% The one-standard-deviation
%intervals around the mean planning time often overlap, suggesting the planning component is
%constant in $\alpha$.

%\begin{figure}
%\includegraphics[width=\columnwidth]{figures/planning_times_imputedb.png}
%\caption{Planning times for each query}
%\label{fig:plantimes}
%\end{figure}
\subsubsection{Accuracy vs. Base-table Imputation}

\Cref{table:smape} shows the Symmetric-Mean-Absolute-Percentage-Error (SMAPE)~\cite{Makridakis2000451} for \ProjectName{}'s query results compared to the baseline.
This measures the error introduced by on-the-fly imputation as compared to full imputation on the base tables.
This comparison is the relevant one in practice --- an analyst would only be considering the trade-offs in imputation quality and time in the first place for dirty input data, for which ground truth is unavailable.

\todo{Distinguish between underlying imputation methods}
For $\alpha=0$ and $\alpha=1$, we pair up tuples and compute SMAPE for all queries. That is,
we compute tuple-wise absolute percentage deviations within each iteration of a query, and
we report this value averaged over all iterations.  We
can see that optimizing for quality indeed reduces the SMAPE of query results.  In general, the SMAPE relative 
to the baseline is low---\todo{update: between \lowsmapealphazero{} and
\highsmapealphaoneexacs{} percent}---indicating that on-the-fly imputation produces results similar to the baseline. Notably, the results with largest error correspond
most closely to one existing approach of dropping all \nullv{}s.

We also calculate the number of tuples used to produce each aggregate output.
The count fraction columns in \Cref{table:smape} show the number of tuples in the aggregate for $\alpha = 0$ and $\alpha = 1$ as a fraction of the number of tuples used when running the query on the imputed base table.
This shows that when optimizing for performance, not quality, many tuples are dropped.
Even in cases where the SMAPE reduction between $\alpha = 1$ and $\alpha = 0$ is small (\Cref{q2}, \Cref{q6}) the tuple count is significantly different.
In these cases, the aggregate value is not significantly impacted by the missing data. In particular, if values are missing uniformly at random,
the aggregate should not be affected.
However, if the missing data is biased then the aggregate will have a significant error.
This highlights a challenge for a user handling data imputation traditionally: it is unclear if the missing data will have a large or small negative impact on their analysis until they have paid the cost of running it.
By using \ProjectName{} this cost can be lowered significantly.

\review{
\Cref{fig:pareto-frontiers} shows the final Pareto frontiers produced by the planner for \Cref{q1} and \Cref{q6}. These are the resulting set of plans after pruning dominated plans throughout our algorithm.
and encapsulate the possible tradeoffs that the user can make in choosing a final plan for execution. The frontier is available at the end of the planning stage and
can be exposed to the user to help in guiding their workflow.
}

\begin{table}
\centering
\input{tables/perf_summary.tex}
\caption{Symmetric-Mean-Absolute-Percentage-Error for queries run under different $\alpha$ parameterizations, as compared to the baseline.
Queries optimized
    for quality ($\alpha=0$) generally achieve lower error than queries optimized for
    efficiency ($\alpha=1$). The count fraction column shows the number of tuples used in calculating each aggregate
     as a fraction of the number of tuples used when running the same query after imputing on the base table.
    A lower count share reflects more potential for errors.}
\label{table:smape}
\end{table}

\begin{figure}
\centering
\includegraphics[scale=0.4]{figures/pareto_frontiers_plot.png}
\caption{\review{
    \ProjectName{}'s final planner Pareto frontier for \Cref{q1} and \Cref{q6}. 
    For clarity, we have removed from the plot plans that provide nearly indistinguishable tradeoffs in either dimension.
}}
\label{fig:pareto-frontiers}
\end{figure}

\subsubsection{Baseline Runtime}
In many real-world cases, applying imputation to an entire dataset is prohibitively expensive.
For example, if we run chained-equation decision trees imputation on the full ACS dataset, it completes in \acsbaseresultminutes{}.

In contrast, \ProjectName{} executes the query \lstinline{SELECT AVG(c0) FROM acs} over the ACS dataset (also using chained-equation decision trees imputation) in \acsimputedbzeroresult{} when optimizing for quality and \acsimputedboneresult{} when optimizing for runtime.
The performance difference comes from the fact that \ProjectName{} only needs to perform imputation on the single column required by the query.
Adding selection predicates would further reduce the query runtime by reducing the amount of required imputation.
An analyst could do the same, but tracking the required imputations would get increasingly complicated as the queries became more complex.
This highlights the benefit of using our system for early data exploration.

\todo{Should this be here?}
For every imputation operator inserted into the query plan, a new statistical model is instantiated and trained before being used for imputation.
Although it is tempting to further optimize query execution in \ProjectName{} by pre-training imputation models while the analyst begins to make queries,
this is not desirable because the specific rows of data, set of complete attributes, and set of attributes to impute are unknown until runtime.

%\subsubsection{Planning Times}
%\begin{figure}
%\centering
%\includegraphics[scale=0.4]{figures/planning_times_plot.png}
%\caption{Planning times are sub-second for up to 5 joins. For more joins, we implement an approximation to Pareto sets. Users can increase the level of
%approximation for more aggressive planning reductions.
%}
%\label{fig:planning-times}
%\end{figure}

\todo{Update: Planning times for our benchmark queries are reasonable, with mean planning
times across the benchmark queries ranging from \planninglowtime{} ms to
\planninghightime{} ms, constituting \planninglowtimepct{} to \planninghightimepct{} of
total runtime, respectively.

In all cases, the optimizer returned a query
plan within \planningmaxtime{} ms.} The choice of $\alpha$ has no impact on planning times,
because query selection is done after collecting the Pareto frontier.

\review{
However, \ProjectName{}'s planning algorithm is still exponential in the number of joins in the plan, and the use of Pareto sets exacerbates this exponential growth.
We explored the limits of our planning algorithm by constructing a series of queries with increasing number of joins over the CDC tables.
Planning times (~\Cref{fig:planning-times}) for queries involving 1--5 joins---a practical value for  real world exploratory queries---are less than 1 second.
We extended \ProjectName{} to support an approximation of Pareto sets to improve planning times for queries with 6 or more joins. Approximate Pareto sets
displayed a reduction in planning time linear with respect to the reduction in size of the final frontier. For queries with 6--8 joins, approximate sets achieved a reduction
in planning time between 14\% and 23\%, on average. Users can increase the level of approximation to reduce planning times further.
}
%\todobox{Jose: I'm not a fan of this line, I would remove}{Furthermore, at that point, it may make more sense to specify a coherent model over the entire base table instead.}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
