\section{Experiments}

To evaluate the performance of our system, we generated plans for queries on two data sets: the American Community Survey and a synthetic dataset.
\subsection{Data sets} \label{subsec:datasets}

The American Community Survey (ACS) provides a number of public data sets collected by the U.S. Census Bureau.
We used a cleaned version of the 2012 Public Use Microdata Sample (PUMS) data kindly provided by the authors of~\cite{akande2015empirical}.
The cleaning procedure is described in detail in~\cite{akande2015empirical}, but to summarize, the following data were removed:
\begin{itemize}
\item Rows corresponding to vacant houses or single occupant households.
\item Identification variables (e.g. area codes, serial numbers).
\item Flag variables.
\item Continuous variables.
\end{itemize}
The final data set consists of a single table with 671,153 rows and 37 columns, where all column entries are integers.

In order to evaluate our implementation, we created a dirtied variant of the ACS data by randomly eliminating
10\% of the fields across the table.

In order to evaluate additional queries, we constructed a synthetic table, consisting of 10 columns and 10,000 rows, with
all values drawn from a uniform distribution between 0 and 100. To create a dirtied variant of the synthetic data, we randomly deleted 30\% of the fields
across the table.

\subsection{Queries}

We collected a set of queries (Figure~\ref{fig:queries}) that we think are both representative, in that they could reasonably be written by a user in the course of data analysis, and interesting to plan.

The queries on the ACS data all consist only of selections, filters, and aggregations.
The ACS data is contained in a single table, so there are relatively few join queries that are interesting on this data set and remain relevant for analysts, rather
than contrived solely for the purpose of experimentation.
However, even with select, filter, and aggregate, there are interesting choices to make with imputation placement. In order to explore joins, we leverage the synthetic
dataset and write various ad-hoc queries.

\begin{table}
  \centerfloat
  \input{queries}
  \caption{Queries used in our experiments.}
  \label{fig:queries}
\end{table}

\begin{table}
  \centerfloat
  \input{runtimes}
<<<<<<< HEAD
  \caption{Running time and error for queries with different imputation levels. Base error is the RMSE between the query run on clean data and the query run on dirty data without imputation.}
  \label{fig:experiments}
\end{table}

Figure~\ref{fig:experiments} provides a summary of our experimental results.
We run each query in four configurations: without imputation on clean data, without imputation on dirty data, and with imputation on dirty data for $\alpha = 0$ and $\alpha = 1$.
When a query returns a relation of the same ``shape'' (same number of tuples and same schema) on both clean and dirty data, we can compute an error value.
For our experiments, we report root-mean-square error (RMSE).
For imputations, we run queries using the two extreme values of $\alpha$: 0 and 1, representing a focus on runtime and on information loss, respectively.
For each query, the table displays the error between the query run on clean data and the query run on dirty data with imputation and the running time in seconds.

Planning times are not shown, as they are fractions of a second for all of the queries that we tested.
The overall low planning times highlight the practicality of our algorithm, despite the exponential complexity.
Running times clearly vary based on $\alpha$, allowing users to tune their query performance as desired.
=======
  \caption{Running time and errors (SSE) for queries with different imputation levels. base error corresponds to the
error with traditional SQL null semantics. The errors for \ProjectName{} plans are labeled with $\epsilon$. The two
$\alpha$ levels reflect a relative focus on information loss. $t_p$ and $t_r$ represent planning and run time, respectively, in seconds.}
  \label{fig:experiments}
\end{table}

Figure~\ref{fig:experiments} provides a summary of our experimental results. For purposes of simplicity, we present only the results associated with
queries that return a result size matching our base case. This addresses the fact that deriving a reasonable error metric for set-queries 
is inherently task specific (e.g. consider tuning precision vs. recall). For all other queries, we use sum of squared errors (SSE) as a metric.
The overall low planning times highlight the practicality of our algorithm, despite the exponential complexity. Runtimes clearly vary based on $\alpha$, allowing
users to tune their query performance as desired.
>>>>>>> 594b06596d402300a8cf6bbaaf50b762b48aa5f3

We can group our queries into five categories: sum/count queries, average queries, min/max queries, selection queries, and join queries on synthetic data.

\begin{itemize}
\item For sum and count queries (\ref{q1}, \ref{q2}, \ref{q4}), imputation does a good job of reducing the error.
For each of these queries, we can reduce total error by several orders of magnitude.
Imputation performs very well in these cases, even for data which is missing uniformly at random, because each missing value directly increases the error of the result.

\item For average queries (\ref{q3}, \ref{q5}, \ref{q6}), imputation does not reduce the error, mainly because running these queries on dirty data incurs only a small penalty.
This is because, in the case of average, removing data uniformly at random does not change the mean.
If we were to remove data in a biased way, imputation would perform better because there would be more error for it to correct.

\item For min and max queries (\ref{q7}, \ref{q8}, \ref{q12}), it is unlikely that imputation will ever help, because by definition these queries are looking for extreme values.
These values are unlikely to be the result of imputation and are also unlikely to be randomly removed from the dataset.

\item We do not compute error for selection queries (\ref{q9}, \ref{q10}, \ref{q11}), because the output of the query on clean data is not directly comparable to the output on dirty data.

  \item Imputation performs poorly on the join queries on synthetic data (\ref{q13}, \ref{q14}) because the data is synthetic, so there are no correlations for imputation to find. We included these queries because we wanted to test the performance of our algorithm on queries that include joins, and we were pleased to find that simple joins cause no perceptible performance problems.
\end{itemize}

Finally, we performed CE-CART imputation on the entirety of the ACS table, in order to compare our system to the traditional, pre-processing approach to imputation.
We were unable to complete the full imputation due to resource constraints, but the process ran for more than one hour without terminating.
This time compares favorably to our maximum of \todo{add time} for a query with imputation.
Given the varying needs of database users, \ProjectName{} clearly allows flexibility without a significant performance hit.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
