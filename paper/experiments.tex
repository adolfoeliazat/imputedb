\section{Experiments}

To evaluate the performance of our system, we generate plans for queries on two data sets: the American Community Survey and a synthetic dataset.
\subsection{Data sets} \label{subsec:datasets}

The American Community Survey (ACS) provides a number of public data sets collected by the U.S. Census Bureau.
We used a cleaned version of the 2012 Public Use Microdata Sample (PUMS) data kindly provided by the authors of~\cite{akande2015empirical}.
The cleaning procedure is described in detail in~\cite{akande2015empirical}, but to summarize, the following data were removed:
\begin{itemize}
\item Rows corresponding to vacant houses or single occupant households.
\item Identification variables (e.g. area codes, serial numbers).
\item Flag variables.
\item Continuous variables.
\end{itemize}
The final data set consists of a single table with 671,153 rows and 37 columns, where all column entries are integers.

In order to evaluate our implementation, we created a dirtied variant of the ACS data by randomly eliminating
10\% of the fields across the table.

In order to evaluate additional queries, we constructed a synthetic table, consisting of 10,000 rows and 10 columns, with
all values drawn from a uniform distribution between 0 and 100. To create a dirtied variant of the synthetic data, we randomly deleted 30\% of the fields
across the table.

\subsection{Queries}

We collected a set of queries (Table~\ref{fig:queries}) that we think are both representative, in that they could reasonably be written by a user in the course of data analysis, and interesting to plan.

The queries on the ACS data all consist only of projections, selections, and aggregations.
The ACS data is contained in a single table, so there are relatively few join queries that are interesting on this data set and remain relevant for analysts, rather
than contrived solely for the purpose of experimentation.
However, even with select, filter, and aggregate, there are interesting choices to make with imputation placement. In order to explore joins, we leverage the synthetic
dataset and write various ad-hoc queries.

\begin{table}
  \centerfloat
  \input{queries}
  \caption{Queries used in our experiments.}
  \label{fig:queries}
\end{table}

\begin{table}
  \centerfloat
  \input{runtimes}
    \caption{Base error, percent change in error and and running time for queries
    with different imputation levels. Base error is the root-mean-square error (RMSE) between the query run on clean
    data and the query run on dirty data without imputation. Change in error is relative to the base error.}
  \label{fig:experiments}
\end{table}

Table~\ref{fig:experiments} provides a summary of our experimental results.
We run each query in four configurations: without imputation on clean data, without imputation on dirty data, and with imputation on dirty data for $\alpha = 0$ and $\alpha = 1$.
When a query returns a relation of the same ``shape'' (same number of tuples and same schema) on both clean and dirty data, we can compute an error value.
For our experiments, we report root-mean-square error (RMSE) for the base error (i.e. the error in the dirty data without imputation with respect to the clean data) and the change
in RMSE for the imputed plans.
For imputations, we run queries using the two extreme values of $\alpha$: 0 and 1, representing a focus on runtime and on information loss, respectively.
For each query, the table displays the base error, the change in error in both imputation scenarios, and the running time in seconds.

Planning times are not shown, as they are fractions of a second for all of the queries that we tested.
The overall low planning times highlight the practicality of our algorithm, despite the exponential complexity.
Running times clearly vary based on $\alpha$, allowing users to tune their query performance as desired.

We can group our queries into five categories: sum/count queries, average queries, min/max queries, selection queries, and join queries on synthetic data.

\begin{itemize}
\item For sum and count queries (\ref{q2}, \ref{q3}, \ref{q5}), imputation does a good job of reducing the error.
For each of these queries, we can reduce total error by several orders of magnitude.
Imputation performs very well in these cases, even for data which is missing uniformly at random, because each missing value directly increases the error of the result.

\item For average queries (\ref{q4}, \ref{q6}, \ref{q7}), imputation does not reduce the error, mainly because running these queries on dirty data incurs only a small penalty.
This is because, in the case of average, removing data uniformly at random does not change the mean.
If we were to remove data in a biased way, imputation would likely perform better because there would be more error for it to correct.

\item For a min query (\ref{q8}), it is unlikely that imputation will ever help, because by definition these queries are looking for extreme values.
These values are unlikely to be the result of imputation and are also unlikely to be randomly removed from the dataset.

\item We do not compute error for selection queries (\ref{q1}, \ref{q9}, \ref{q10}, \ref{q12}), because the output of the query on clean data is not directly comparable to the output on dirty data.

  \item Imputation performs poorly on the join queries on synthetic data (\ref{q11}, \ref{q13}) because the data is synthetic, so there are no correlations for imputation to find. We included these queries because we wanted to test the performance of our algorithm on queries that include joins, and we were pleased to find that simple joins cause no perceptible performance problems.
\end{itemize}

Finally, we performed CE-CART imputation on the entirety of the ACS table, in order to compare our system to the traditional, pre-processing approach to imputation.
We were unable to complete the full imputation due to resource constraints, but the process ran for more than one hour without terminating.
This time compares favorably to our maximum of 72 seconds for a query with imputation on the ACS data.
Given the varying needs of database users, \ProjectName{} clearly allows flexibility without a significant performance hit.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
