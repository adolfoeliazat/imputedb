\section{Experiments}
\todo{we of course need to rewrite this whole section}
To evaluate the performance of our system, we generate plans for queries
for two separate survey-based data sets, showing that our system
is ideal for early dataset exploration.

\subsection{Data sets} \label{subsec:datasets}
\subsubsection{Center for Disease Control: Surveys}
For our first set of experiments, we use survey data collected by the 
Center for Disease Control (CDC) in the United States. In particular, we
experiment on a set of tables collected as part of the National
Health and Nutrition Examination Survey (NHANES), a series of studies
conducted by the CDC on a national sample of several thousand individuals\cite{cdc-data}.
The data consists on survey responses, physical examinations, and laboratory
results, amongst others. Our dataset corresponds to the 2013-2014 NHANES
and is available for direct download on the CDC's organization site on 
kaggle.com, a popular data science website.

There are a total of 6 tables associated with the NHANES data set. For purposes
of our experiments, we use three tables:

\begin{itemize}
	\item Demographics: outlines demographic information of subjects
	\item Examinations: physical exam results
	\item Laboratory: laboratory exam results
\end{itemize}

The original tables have a large number of attributes, in some cases providing more granular
tests results or alternative metrics. In order to simplify the presentation and
exploration of queries, we focused on a subset of the attributes for each table.
Table~\ref{table:nhanes-description} shows the attributes selected, along with the
percentage of nulls. It is worth noting that for readability, we have replaced the
NHANES variable names with self-explanatory attribute names.

\begin{table}
    \centering
    \begin{subtable}{0.5\textwidth}
        \centering
       \input{tables/cdc_demo}
        \caption{Demographics}
    \end{subtable}
    \begin{subtable}{0.5\textwidth}
        \centering
        \input{tables/cdc_labs}
        \caption{Laboratory Results}
    \end{subtable}
       \begin{subtable}{0.5\textwidth}
        \centering
        \input{tables/cdc_exams}
        \caption{Physical Results}
    \end{subtable}
    \caption{Missing values in CDC NHANES 2013-2014 data}
    \label{table:nhanes-description} 
\end{table}

All tables used consists of 10 attributes. The laboratory and examinations data,
referred to as \textit{labs} and \textit{exams} in our queries, consists of 9813 rows,
while the demographics data (\textit{demo}) has 10175 rows.

\subsubsection{freeCodeCamp: 2016 New Coder Survey}
For our second set of experiments, we chose to use data collected
by freeCodeCamp as a part of a survey of new software developers
(both professional and amateur). freeCodeCamp is an open-source
community that intends to help users learn how to program. In addition,
to providing training material for users, it also pairs users with
nonprofits to develop software solutions. Their \textit{2016 New Coder Survey} consists of responses by over 15,000 people to 48 different
questions. The survey targeted users who were in one way or another
related to coding organizations.

For our purposes, we use a version of the data that has been 
pre-processed, but where plenty of missing values remain. It is
worth noting that given the table design (where many attributes
are related), many of these missing values are to be
expected. The dataset we started with had 15,620 rows and 113
columns. We chose 17 attributes, which are outlined in Table~\ref{table:fcc-description}, along with the percentage of missing values.

\begin{table}
   \centering
    \input{tables/fcc}
    \caption{Missing values in FCC Survey Data}
   \label{table:fcc-description} 
\end{table}

\subsection{Queries}
We collected a set of queries (Table~\ref{fig:queries}) that we think are both representative, in that they could reasonably be written by a user in the course of data analysis, and interesting to plan.

The queries on the CDC NHANES data consists not only of projections and selections,
but also interesting joins and aggregations. Our aim was to craft meaningful queries
that would provide performance figures relevant to practitioners who may use
similar datasets.

Query~\ref{q1} calculates the average weight for users based on their income data.
To do so, it joins demographics and physical examination data. Query~\ref{q2}
compares cholesterol levels for individuals who are both ends of the income
spectrum and who satisfy a minimum weight requirement. Query~\ref{q3}
extracts the maximum blood lead levels for subjects who satisfy a particular
waist circumference requirement.

In Query~\ref{q4}, we calculate the average income for survey participants,
based on their bootcamp attendance. Query~\ref{q5} estimates the average
age of women from the Uniter States who participated. Query~\ref{q6} 
calculates the average amount of money survey participants with student debt destined to
learning based on their school degree. Finally, in Query~\ref{q7}, we join the FCC data with a reference table provided
by the World Bank which summarizes GDP per Capita across various
countries\cite{worldbank-data}. The query calculates the average GDP-per-capita for participants based on whether or not they
attended a bootcamp. 

Note that for all queries we have enumerated strings and encoded them with
an appropriate integer value.

\begin{table}
\centering
 \begin{subtable}{\linewidth}
  \input{queries-cdc}
  \caption{Queries on CDC data}
  \label{fig:queries-cdc}
 \end{subtable}
 ~
 \begin{subtable}{\linewidth}
 \input{queries-fcc}
 \caption{Queries on FCC data}
 \label{fig:queries-fcc}
 \end{subtable}
  \caption{Queries used in our experiments.}
  \label{fig:queries}
\end{table}

\begin{table}
  \centerfloat
  \input{runtimes}
    \caption{Base error, percent change in error and and running time for queries
    with different imputation levels. Base error is the root-mean-square error (RMSE) between the query run on clean
    data and the query run on dirty data without imputation. Change in error is relative to the base error.}
  \label{fig:experiments}
\end{table}

Table~\ref{fig:experiments} provides a summary of our experimental results.
We run each query in four configurations: without imputation on clean data, without imputation on dirty data, and with imputation on dirty data for $\alpha = 0$ and $\alpha = 1$.
When a query returns a relation of the same ``shape'' (same number of tuples and same schema) on both clean and dirty data, we can compute an error value.
For our experiments, we report root-mean-square error (RMSE) for the base error (i.e. the error in the dirty data without imputation with respect to the clean data) and the change
in RMSE for the imputed plans.
For imputations, we run queries using the two extreme values of $\alpha$: 0 and 1, representing a focus on runtime and on information loss, respectively.
For each query, the table displays the base error, the change in error in both imputation scenarios, and the running time in seconds.

Planning times are not shown, as they are fractions of a second for all of the queries that we tested.
The overall low planning times highlight the practicality of our algorithm, despite the exponential complexity.
Running times clearly vary based on $\alpha$, allowing users to tune their query performance as desired.

We can group our queries into five categories: sum/count queries, average queries, min/max queries, selection queries, and join queries on synthetic data.

\begin{itemize}
\item For sum and count queries (\ref{q2}, \ref{q3}, \ref{q5}), imputation does a good job of reducing the error.
For each of these queries, we can reduce total error by several orders of magnitude.
Imputation performs very well in these cases, even for data which is missing uniformly at random, because each missing value directly increases the error of the result.

\item For average queries (\ref{q4}, \ref{q6}, \ref{q7}), imputation does not reduce the error, mainly because running these queries on dirty data incurs only a small penalty.
This is because, in the case of average, removing data uniformly at random does not change the mean.
If we were to remove data in a biased way, imputation would likely perform better because there would be more error for it to correct.

\item For a min query (\ref{q8}), it is unlikely that imputation will ever help, because by definition these queries are looking for extreme values.
These values are unlikely to be the result of imputation and are also unlikely to be randomly removed from the dataset.

\item We do not compute error for selection queries (\ref{q1}, \ref{q9}, \ref{q10}, \ref{q12}), because the output of the query on clean data is not directly comparable to the output on dirty data.

  \item Imputation performs poorly on the join queries on synthetic data (\ref{q11}, \ref{q13}) because the data is synthetic, so there are no correlations for imputation to find. We included these queries because we wanted to test the performance of our algorithm on queries that include joins, and we were pleased to find that simple joins cause no perceptible performance problems.
\end{itemize}

Finally, we performed CE-CART imputation on the entirety of the ACS table, in order to compare our system to the traditional, pre-processing approach to imputation.
We were unable to complete the full imputation due to resource constraints, but the process ran for more than one hour without terminating.
This time compares favorably to our maximum of 72 seconds for a query with imputation on the ACS data.
Given the varying needs of database users, \ProjectName{} clearly allows flexibility without a significant performance hit.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
