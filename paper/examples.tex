\section{Motivating Example}
Consider an epidemiologist tasked with an initial
exploratory analysis of data collected for individuals across
a battery of exams. In particular, she is interested in exploring
the relationship between income and the immune system,
controlling for factors such as weight and gender.

While careful modeling techniques will be necessary before putting
results into the final research paper, the epidemiologist is anxious
to get a quick and accurate view into the data. However,
the data has been collected through CDC surveys (see Section~\ref{subsec:datasets} for details),
and there is a significant amount of missing data across all
relevant attributes. The researcher must develop a strategy
for handling the missing values. She could drop any records with
missing values and continue her analysis with the remaining data,
or she could fit a model on the base tables which can take advantage of existing correlations between attributes to fill in the missing values.
If she drops the records with missing data, she will throw away the majority of her data set, as many records have missing fields.
Fitting a model on the dataset will take more programming effort and may take a long time to run.

To complicate matters further, the epidemiologist realizes that
some of the steps she is interested in, including filtering and grouping,
are impacted differently by missing data, and that these step may change
repeatedly, as she wants to run various queries during the exploration phase.
The imputation strategy itself may change through out time. For example, as she is getting an initial
set of distribution statistics (such as mean and standard deviation), she may be willing to tolerate
a simpler imputation approach, as long as she gets runtime performance in exchange. Later on,
as more precise estimates are needed for comparing treatments or policy changes, the researcher
may want to focus on the quality of the imputation.

Rather than work through
the implications of each case, she submits a query, as shown in Figure~\ref{fig:example-query}, to ImputeDB.\@ The system, parameterized by a value indicating
a tradeoff between information loss and runtime cost, then finds
the optimal query plan. Figure~\ref{fig:quality-plan} and Figure~\ref{fig:fast-plan} show what this plan would be
when optimizing for quality and performance, respectively. These two plans are found within a broader search space
of XXXX plans. \todo{add a small comment about the number of plans in the plancache
to highlight size of search space}.

The differences between the two plans highlight the subtleties of optimizing queries which contain imputation operators.
Note that imputation operators are placed in different points of the plan, operating over different columns, and indeed
even influencing the order of joins. The quality-optimized plan (Figure~\ref{fig:quality-plan}) uses the impute operation, rather
than dropping tuples with missing values. It waits to impute \verb|demo.income| until after the final join has taken place, but
other imputations take place earlier on in the plan, some before filtering and join operations. 
Meanwhile, the performance-optimized plan (Figure~\ref{fig:fast-plan}) performs a single impute operation (\verb|exams.weight|), and applies drops to tuples with missing values before aggregating.


\begin{figure}
\begin{lstlisting}[language=SQL]
SELECT income, AVG(white_blood_cell_ct)
FROM demo, exams, labs
WHERE gender = 2 AND 
      weight >= 120 AND
      demo.id = exams.id AND 
      exams.id = labs.id
GROUP BY demo.income
\end{lstlisting}
\caption{A typical public health query on CDC's NHANES data}
\label{fig:example-query}
\end{figure}

\begin{figure}
  \Tree
  [.$\pi_{\text{income, AVG(white\_blood\_cell\_ct)}}$
    [.$g_{\text{income, AVG(white\_blood\_cell\_ct)}}$
      [.$\mu_{\text{demo.income}}$
        [.$\bowtie_{\text{exams.id} = \text{demo.id}}$
          [.$\mu_{\text{labs.white\_blood\_cell\_ct}}$
            [.$\bowtie_{\text{exams.id} = \text{labs.id}}$
              [.$\sigma_{\text{exams.weight} \geq 120}$ 
                [.$\mu_{\text{exams.weight}}$ exams ] 
              ] 
              labs 
            ]
          ]
        [.$\sigma_{\text{demo.gender} = 2}$ demo ]
      ] 
    ] 
  ] 
]
\caption{A plan for the query in Fig.~\ref{fig:example-query}, optimized for quality.}
\label{fig:quality-plan}
\end{figure}

\begin{figure}
  \Tree
  [.$\pi_{\text{income, AVG(white\_blood\_cell\_ct)}}$
    [.$g_{\text{income, AVG(white\_blood\_cell\_ct)}}$
      [.$\delta_{\text{demo.income, labs.white\_blood\_cell\_ct}}$
        [.$\bowtie_{\text{exams.id} = \text{labs.id}}$
          [.$\bowtie_{\text{demo.id} = \text{exams.id}}$
            [.$\sigma_{\text{demo.gender} = 2}$ demo ]
            [.$\sigma_{\text{exams.weight} \geq 120}$ [.$\mu_{\text{exams.weight}}$ exams ] ] ] labs ] ] ] ]
\caption{A plan for the query in Fig.~\ref{fig:example-query}, optimized for performance.}
\label{fig:fast-plan}
\end{figure}


Tens of queries later, the epidemiologist has the holistic view of the
data that they required before carefully crafting their own tailored
imputation model. Knowing that there may be a need to explore
this data set further, they can easily incorporate their imputation model
into ImputeDB for future use.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
