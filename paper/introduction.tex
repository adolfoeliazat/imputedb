\section{Introduction}

Many databases have large numbers of missing or NULL values;  these can arise for a variety of reasons, including missing source data (e.g., unknown facts),
missing columns during data integration, de-normalized databases, or as a result of outlier detection and cleaning where NULL values are substituted for
bad values~\cite{kim2003}.  Such NULL values can lead to incorrect or ill-defined query results~\cite{rubin1976}, and as such removing these values from data
before processing is often desirable.

% Solution
One common approach is to manually replace missing values using some sort of statistical or predictive model based on other values in the table and record.
This process is called \emph{imputation}.
In this paper, we present a new system, called \ProjectName{}, which is the designed to selectively apply imputation to a subset of records, on-the-fly, during query execution.  \emph{The key insight behind \ProjectName{} is that imputation
only needs to be performed on the data relevant to a particular query and
that this subset is generally much smaller than the entire database.}  
% Background
While traditional imputation methods work over the entire data set and replace all missing values, running a sophisticated imputation algorithm over a large data set can be very expensive---our experiments show that even a relatively simple regression tree algorithm takes \acsbaseresulthours{} to train and run
on a 600K row database; prior work~\cite{akande2015empirical} reports times of 45 minutes for multiple-imputation of missing values on a 1\% sample of the same database on a desktop computer.\todo{Explain the discrepancy between 45 * 100 and 355? They use L=10 datasets and unknown hardware. Or drop this comparison entirely?}

A simpler approach might drop all rows with missing values, but not only can this introduce bias into the results, but this may result in discarding all of the data.
In contrast to existing systems~\cite{burgette2010multiple,akande2015empirical}, \ProjectName{} avoids imputing over the entire data set, as done with traditional methods. Specifically, \ProjectName{}
carefully plans the placement  imputation / row drop operations inside the query plan, resulting in a 
significant speedup in query execution while minimizing the number of dropped rows.  Also unlike previous work, the focus on of our work is not on the imputation algorithms themselves (we can employ almost any such algorithm), but rather on the  query planning involved in placement of imputation operations
in an optimal way into query plans.   Specifically, our optimization algorithms generate the Pareto-optimal frontier of plans that trade imputation cost for result quality.

% \todobox{find appropriate location for this}
% {In a previous project, she
% worked with the American Community Survey data (671,153 rows), and performing imputation on the whole
% dataset took around 75 minutes\todo{Here, we can reference the number from our own experiment (done), or the number from Akande et al: ``performing imputation on just a 1.5 percent random sample took 45 minutes''}.}

% % Prior work
% There is relevant prior work in three broad areas: statistics, databases, and time-series forecasting.
% Prior work in statistics\todo{add more specific subarea qualifier} has focused on imputation quality~\cite{burgette2010multiple} and runtime cost~\cite{akande2015empirical}.
% Prior work in databases has established simple semantics for handling tuples with missing values~\cite{codd1973understanding,grant1977null}.
% Prior work in time-series forecasting has incorporated forecasting extensions into domain-specific databases and characterized their behavior~\cite{parisi2011embedding,parisi2013temporal,duan2007processing}.
% \ProjectName{} contributes the first optimizer to plan data imputation on a per query basis, allowing users to use standard SQL on real-world data sets with missing values
% and express preferences in quality of data imputation versus running time. The guiding design principle behind \ProjectName{} is that the user should never see missing data or have to modify their queries to account for it.

% Implications
Our approach enables an exploratory analysis workflow in which the analyst can issue standard SQL queries over a data set, even if that data has missing values, and get
queries that execute between \lowxalphazero{} and \highxalphaoneexacs{} times as fast as it takes to impute all missing values and then run queries (the traditional approach). Furthermore, the results obtained with this approach
are similar in quality to those obtained with the complete imputation approach, as indicated by low Symmetric-Mean-Absolute-Percentage-Errors --- between \lowsmapealphazero{} and \highsmapealphaoneexacs{} percent --- in our empirical results
over real-world data sets (see~\Cref{sec:experiments} for details).

% Scope
\ProjectName{} is designed to enable early data exploration, by allowing analysts to run their queries without explicitly decide with when to perform imputation.
% SAM -- cut the following -- could consider moving later but feels out of place here
%However, we optimize query plans with respect to a heuristic measure of imputation quality, and provide no hard guarantees about the accuracy of the results.

\subsection{Contributions}
\ProjectName{} leverages a number of contributions to minimize imputation time while producing comparable results to traditional approaches,
increasing productivity for users who frequently encounter missing values in their data. These contributions include:
\begin{itemize}
\item \textbf{Relational algebra extended with imputation:}
  We extend the standard relational algebra with two new operators to represent imputation operations: impute ($\mu$) and drop ($\delta$).
  Impute operation fills in missing data values using an iterative regression model (CE-CART)~\cite{burgette2010multiple}.
  Drop operation simply drops tuples which contain null values.
\item \textbf{Model of imputation quality and cost:}
  We extend the traditional cost model for query plans to incorporate a measure of the quality of the imputations performed.
  We use the cost model to abstract over the imputation algorithm used.
  To add an additional imputation technique, it is sufficient to characterize it with two functions: one to describe its running time and one to describe the quality of its results.
\item \textbf{Query planning with imputation:}
  We present the first query planning algorithm to jointly optimize for running time and the quality of the imputed results.
  It does so by maintaining multiple sets of Pareto optimal plans according to the cost model.
  By deferring selection of the final plan, we make it possible for the user to trade off running time and result quality.
\item \textbf{Proof of optimality:}
 We prove that our cost-model and planning algorithm produce a sound and complete final Pareto frontier, making the final plan choice optimal under our search space and the user's preferences.
\end{itemize}


% Summary of related work

% Although this manual imputation solves the problem of missing data, in the age of big data it may be very expensive to run an imputation algorithm on an entire dataset.
% Additionally, it may not be necessary to completely clean the data to make it usable.
% Some users may be willing to run queries on dirty data, simply ignoring any missing values, as long as they do not have to pay the cost of imputation.
% Others may want to run queries on a subset of the data, and so do not need to impute
% every field in every record. Yet others may want to customize the 
% imputation algorithm for the tradeoffs and demands of a particular domain.

% In this paper, we present \ProjectName{}, a database system which is designed to interact with a dirty dataset as though it were clean.
% To achieve this goal, we perform imputation on the fly, during query execution.
% Performing imputation at query time allows our system to impute only the data necessary to run the query, and it allows users to flexibly trade imputation quality for computation time.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
