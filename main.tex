\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}

\title{6.830 Project Proposal \\
  \large Data Imputation as a Query Optimization}
\author{Jose Cambronero \& John Feser \& Micah Smith}

\begin{document}
\maketitle

\section{Problem Statement}
Data imputation is a well studied problem in the machine learning literature, but some aspects related to the implementation of imputation in database systems have not been sufficiently examined. Traditionally studies have viewed imputation as a pre-processing step on a table, rather than as an active step in each query. Particularly, existing work has not incorporated imputation into query optimization, nor has it investigated the relative impact of early versus late stage imputation on the quality of query results.

\section{Approach}
In order to study the placement of an imputation step, we propose to create a logical imputation operator (along with respective physical instances) and incorporate its placement as part of the query plan optimization process in SimpleDB.  We plan to introduce measures for uncertainty and completeness of query results as well as the cost of imputation, which outline the main tradeoffs in the imputation placement. We introduce these measures into our cost estimation, allowing us to intelligently place the data imputation step during query planning. Finally, we plan to produce empirical results that show the tradeoffs between efficiency and accuracy in the context of simple data imputation models in queries that perform aggregate operations or return a significant number of tuples (in contrast to simple point-queries) and compare to existing approaches.  These types of queries are more likely to provide reasonable cases for imputation of missing values and reflecting an increasingly large portion of workloads in modern OLAP warehouses.

\section{Example}

\section{Evaluation}
We are considering various possible data sources to experiment with. Existing literature has used the American Community Survey, so this may be a natural choice in our case as well. The ACS data is a good test dataset because of its scale and relative simplicity. Additionally, we may be able to compare to existing work that used ACS data.

In order to measure accuracy of our imputation algorithm, we plan to compute results with and without missing values where possible, and use standard error measures to compare to our results. The best-case scenario for data imputation is missing data where the data is removed uniformly at random. It should be easy to construct a test dataset from an existing dataset simply by removing data from it; artificially degrading an existing dataset also means that we will have ground truth to compare our results with.

In contrast to pre-processing-based imputation, we plan to make a case for imputation at the level of intermediate results. There is an analogy to be made between this approach and views, as neither alter the original table and provide logical independence. Many modern database servers have a variety of clients, with different needs, making one imputation strategy unlikely to satisfy everyone. Consider a financial data warehouse, with clients such as business strategy and actuaries. It is unlikely that the queries issued by these two groups will require or benefit from the same imputation strategy. If a salary value is missing for a client, the business strategy query may not require a costly regression model estimate and a simple heuristic could be enough to return a reasonable answer. 

Furthermore, we plan to show that there is an opportunity for more efficient imputation on (smaller) intermediate results. In very large databases it may not be feasible to perform imputation on the base tables, particularly if the imputation method of choice is expensive. However, queries often return results that are smaller than the base tables, so it may be more reasonable to perform imputation on these intermediate results. The intermediate results could then be stored as a materialized view if they are expected to be reused or recomputed as necessary.

\section{Project Milestones}

We outline a series of milestones to be completed:
\begin{enumerate}
\item Initial data collection and query workload for evaluation.
\item Consolidate set of imputation strategies.
\item Formalize a measure of uncertainty, completeness and cost in imputation as related to 2.
\item Formalize the possible query rewrites when using an imputation operator.
\item Incorporate imputation-related measures into cost estimation for SimpleDB.
\item Incorporate imputation-related rewrites for SimpleDB.
\item Evaluate performance.
\end{enumerate}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
