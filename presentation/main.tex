\documentclass{beamer}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[activate={true,nocompatibility},final,kerning=true,spacing=true,factor=1100,stretch=10,shrink=10]{microtype}
\usepackage{amsfonts,amssymb,amsmath}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[style=ieee]{biblatex}
\usepackage{booktabs}
\usepackage{hyperref} % load after other packages
\usepackage{todonotes}
\presetkeys{todonotes}{inline}{}
\usepackage[style=ieee]{biblatex}
\usepackage{siunitx}

\usepackage{listings} % code samples with syntax hl
% listings settings
\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}; should come as last argument
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{red},    % comment style
  frame=single,	                   % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},       % keyword style
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{gray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mauve},     % string literal style
  tabsize=2,	                   % sets default tabsize to 2 spaces
}




\addbibresource{references.bib}

\makeatletter
\newcommand*{\centerfloat}{%
  \parindent \z@
  \leftskip \z@ \@plus 1fil \@minus \textwidth
  \rightskip\leftskip
  \parfillskip \z@skip}
\makeatother

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\MakeRobust{\Call}

\renewcommand{\emptyset}{\varnothing}

\newcommand{\ProjectName}{ImputeDB}



\title{\ProjectName{}: A Database for Missing Data}
\author{Jose Cambronero \and Micah Smith \and John K. Feser}
\institute{MIT}
\date{\today{}}

\begin{document}
\frame{\titlepage}

\begin{frame}{Introduction and Motivation}
\begin{itemize}
	\item Handling incorrect or dirty data is a complex and challenging problem for data scientists
	\item $\text{missing data} \subset \text{dirty data}$ and requires imputation/dropping.
	\item Cost of running imputation on entire dataset is a high barrier for analysis.
	\item Lack of flexibility inherent in existing pre-processing approaches.
	\item User should never see missing data or have to modify query. Want a DBMS that 
		allow us to "pretend" data is perfect.
	\item \ProjectName{} allows users to interact with dataset as though it were clean, planning
	around necessary imputation/drops.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Query Planning: Overview}
\begin{itemize}
	\item $\mu_{min}, \mu_{max}, \delta$: new relational algebra operators
	\item Cost of computation vs. information loss: key tradeoff controlled by $\alpha$ parameter
	\item Placement of operators and dirty sets generated considered in planning.
	\item Histogram transformations used to provide up-to-date cardinality and missing value estimates during planning.
	\item Dynamic programming approach, extends System R optimization\cite{blasgen1981system}
	\item Exponential complexity, but tractable in practice with low planning times.
	\item Imputation operator is a state-of-the-art regression-tree based method.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Experiments: Queries}
\begin{figure}
  \centerfloat
  \tiny
  \input{../paper/queries}
  \caption{The queries used in our experiments.}
  \label{fig:queries}
\end{figure}
\end{frame}

\begin{frame}[fragile]{Experiments: Results}
\begin{figure}
  \tiny
  \centerfloat
  \input{../paper/runtimes}
  \caption{Running time and error for queries with different imputation levels.}
  \label{fig:experiments}
\end{figure}
\end{frame}

\begin{frame}
	\center
	Extra slides with details for those interested
\end{frame}

\begin{frame}[fragile]{Motivating Example}
\begin{itemize}
	\item An analyst wants to explore relationships between polling data and survey data on household landline ownership
	\item There is an issue with non-responses in survey data
	\item They issue an ImputeDB query and get an effective response
\end{itemize}

\begin{figure}
\begin{lstlisting}[language=SQL]
SELECT polling.ST, AVG(acs.TEL)
FROM polling, acs
WHERE polling.ST = acs.ST
  AND polling.ERROR > 50         -- 5 percentage points
GROUP BY polling.ST;
\end{lstlisting}
\caption{A typical analyst query on ACS data}
\label{fig:example-query}
\end{figure}
\end{frame}

\begin{frame}[fragile]{Motivating Example: Plan}
\begin{itemize}
	\item ImputeDB adds an impute as necessary for the aggregation
	\item It does not need to impute before then as \verb|acs.ST| is not affected by non-responses and thus has no missing values
\end{itemize}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.4\textwidth]{../paper/figures/example.png}
    \caption{Query plan generated by ImputeDB prioritizing imputation quality.}
    \label{fig:query-plan}
\end{figure}
\end{frame}

\begin{frame}[fragile]{Query Planning: Top-Level Algorithm}

\begin{algorithm}[H]
\scriptsize
\input{../paper/algorithms/plan-toplevel.tex}
\end{algorithm}

\end{frame}

\begin{frame}[fragile]{Imputation Algorithm}
\begin{itemize}
	\item Iterative algorithm using regression trees: \textit{Chained Equations Regression Trees} (CE-CERT)\cite{burgette2010multiple}
	\item Significant cost to perform on entire dataset
	\item \ProjectName{} allows flexible use
\end{itemize}
\scriptsize

\begin{algorithm}[H]
\scriptsize
\input{../paper/algorithms/cart.tex}
\end{algorithm}
\end{frame}

\begin{frame}[fragile]{Experiments: Data}
\begin{itemize}
	\item Real dataset: American Community Survey (U.S. Census Bureau)
		\begin{itemize}
			\item Preprocessed version courtesy of authors of~\cite{akande2015empirical}.
			\item 671,153 rows and 37 columns, all integers.
			\item Randomly deleted 10\% of fields to create dirtied variant
			\item Can write typical data analyst queries involving filters and aggregates
		\end{itemize}
	\item Synthetic dataset: drawn from uniform distribution $[0,100)$
		\begin{itemize}
			\item10,000 rows and 10 columns
			\item Randomly deleted 30\% ti create dirtied variant
			\item Crafted ad-hoc queries using joins
		\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Related work}
\begin{itemize}
	\item Long history of imputation in statistical learning community.
		\begin{itemize}
			\item Akande et al (\cite{akande2015empirical}) explored ACS data to compare imputation strategies.
			\item Burgette and Reiter (\cite{burgette2010multiple}) introduce sequences of regression trees.
		\end{itemize}
	\item Long history of null value semantics in database community.
		\begin{itemize}
			\item Treatment of nulls described as early as  \cite{codd1973understanding}
			\item Statistical databases, such as BayesDB (\cite{mansinghka2015bayesdb}) combine statistical inference techniques and traditional querying.
		\end{itemize}
	\item \ProjectName{} first to integrate imputation into query planning with cost-based optimizer.
	\item \ProjectName{} treatment of nulls completely abstracted from user perspective: SQL logical independence.
\end{itemize}
\end{frame}


\begin{frame}[fragile]{Conclusion and Future work}
\begin{itemize}
	\item Missing values and their imputation can successfully be integrated into the relational calculus
	\item By taking a dynamic programming approach, we can consider a variety of operator placements
and input columns, while keeping planning tractable in real-world examples.
	\item Considered series of real-world and fabricated queries, showing viability of approach.
	\item Future work:
		\begin{itemize}
			\item Extend imputation operators with global information (\textit{impute by need})
			\item Introduce multiple imputation algorithms into optimizer for consideration in plans \
			\item Implement in a production-quality database.
		\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[t,allowframebreaks]{References}
  \nocite{*}
  \printbibliography
 \end{frame}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
