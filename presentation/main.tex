\documentclass{beamer}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[activate={true,nocompatibility},final,kerning=true,spacing=true,factor=1100,stretch=10,shrink=10]{microtype}
\usepackage{amsfonts,amssymb,amsmath}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[style=ieee]{biblatex}
\usepackage{booktabs}
\usepackage{hyperref} % load after other packages
\usepackage{todonotes}
\presetkeys{todonotes}{inline}{}


\addbibresource{references.bib}

\makeatletter
\newcommand*{\centerfloat}{%
  \parindent \z@
  \leftskip \z@ \@plus 1fil \@minus \textwidth
  \rightskip\leftskip
  \parfillskip \z@skip}
\makeatother

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\MakeRobust{\Call}

\renewcommand{\emptyset}{\varnothing}

\newcommand{\ProjectName}{ImputeDB}



\title{\ProjectName{}: A Database for Missing Data}
\author{Jose Cambronero \and Micah Smith \and John K. Feser}
\institute{MIT}
\date{\today{}}

\begin{document}
\frame{\titlepage}

\begin{frame}{Introduction and Motivation}
\begin{itemize}
	\item Handling incorrect or dirty data is a complex and challenging problem for data scientists
	\item $\text{missing data} \subset \text{dirty data}$ and requires imputation/dropping.
	\item Cost of running imputation on entire dataset is a high barrier for analysis.
	\item Lack of flexibility inherent in existing pre-processing approaches.
	\item User should never see missing data or have to modify query. Want a DBMS that 
		allow us to "pretend" data is perfect.
	\item \ProjectName{} allows users to interact with dataset as though it were clean, planning
	around necessary imputation/drops.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Query Planning: Overview}
\begin{itemize}
	\item $\mu_{min}, \mu_{max}, \delta$: new relational algebra operators
	\item Cost of computation vs. information loss: key tradeoff controlled by $\alpha$ parameter
	\item Placement of operators and dirty sets generated considered in planning.
	\item Histogram transformations used to provide up-to-date cardinality and missing value estimates during planning.
	\item Dynamic programming approach, extends System R optimization\cite{blasgen1981system}
	\item Exponential complexity, but tractable in practice with low planning times.
	\item Imputation operator is a state-of-the-art regression-tree based method.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Experiments: Queries}
\begin{figure}
  \centerfloat
  \tiny
  \input{../paper/queries}
  \caption{The queries used in our experiments.}
  \label{fig:queries}
\end{figure}
\end{frame}

\begin{frame}[fragile]{Experiments: Results}
\begin{figure}
  \tiny
  \centerfloat
  \input{../paper/runtimes}
  \caption{Running time and error for queries with different imputation levels.}
  \label{fig:experiments}
\end{figure}
\end{frame}

\begin{frame}
	\center
	Extra slides with details for those interested
\end{frame}

\begin{frame}[fragile]{Motivating Example}
\todo{motivating example}
\end{frame}


\begin{frame}[fragile]{Query Planning: Top-Level Algorithm}

\begin{algorithm}[H]
\scriptsize
\input{../paper/algorithms/plan-toplevel.tex}
\end{algorithm}

\end{frame}

\begin{frame}[fragile]{Imputation Algorithm}
\begin{itemize}
	\item Iterative algorithm using regression trees: \textit{Chained Equations Regression Trees} (CERT)\cite{burgette2010multiple}
	\item Significant cost to perform on entire dataset
	\item \ProjectName{} allows flexible use
\end{itemize}
\scriptsize

\begin{algorithm}[H]
\scriptsize
\input{../paper/algorithms/cert.tex}
\end{algorithm}
\end{frame}

\begin{frame}[fragile]{Experiments: Data}
\begin{itemize}
	\item Real dataset: American Community Survey (U.S. Census Bureau)
		\begin{itemize}
			\item Preprocessed version courtesy of authors of~\cite{akande2015empirical}.
			\item 671,153 rows and 37 columns, all integers.
			\item Randomly deleted 10\% of fields to create dirtied variant
			\item Can write typical data analyst queries involving filters and aggregates
		\end{itemize}
	\item Synthetic dataset: drawn from uniform distribution $[0,100)$
		\begin{itemize}
			\item10,000 rows and 10 columns
			\item Randomly deleted 30\% ti create dirtied variant
			\item Crafted ad-hoc queries using joins
		\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Related work}
\begin{itemize}
	\item Long history of imputation in statistical learning community.
		\begin{itemize}
			\item Akande et al (\cite{akande2015empirical}) explored ACS data to compare imputation strategies.
			\item Burgette and Reiter (\cite{burgette2010multiple}) introduce sequences of regression trees.
		\end{itemize}
	\item Long history of null value semantics in database community.
		\begin{itemize}
			\item Treatment of nulls described as early as  \cite{codd1973understanding}
			\item Statistical databases, such as BayesDB (\cite{mansinghka2015bayesdb}) combine statistical inference techniques and traditional querying.
		\end{itemize}
	\item \ProjectName{} first to integrate imputation into query planning with cost-based optimizer.
	\item \ProjectName{} treatment of nulls completely abstracted from user perspective: SQL logical independence.
\end{itemize}
\end{frame}


\begin{frame}[fragile]{Conclusion and Future work}
\begin{itemize}
	\item Missing values and their imputation can successfully be integrated into the relational calculus
	\item By taking a dynamic programming approach, we can consider a variety of operator placements
and input columns, while keeping planning tractable in real-world examples.
	\item Considered series of real-world and fabricated queries, showing viability of approach.
	\item Future work:
		\begin{itemize}
			\item Extend imputation operators with global information (\textit{impute by need})
			\item Introduce multiple imputation algorithms into optimizer for consideration in plans \
			\item Implement in a production-quality database.
		\end{itemize}
\end{itemize}
\end{frame}

\end{document}
